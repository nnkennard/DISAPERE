,sentence,task,label,encoded,output,target
0,"This paper gives a perspective of GANs from control theory, where it is well established in which cases a dynamical system (which can be described analytically as a function of time) is stable or not (by looking at the roots of the denominator of the so-called transfer function in control theory).",pol,none,0.0,0,0
1,It is interesting that the analysis using this framework on simple examples is in line with known results in the GAN literature (Dirac GAN).,pol,pol_positive,3.0,3,3
2,"Although I personally enjoyed reading the results that from control theory perspective are inline with GAN literature, the paper does not provide novel surprising results.",pol,pol_negative,1.0,1,1
3,"For e.g. the results on the oscillating behavior of Dirac-GAN are described in related works (e.g. Mescheder et al. 2018), and in practice, WGAN with no regularization is not used (as well as GAN with momentum, as normally beta1=0 in practice).",pol,pol_negative,1.0,1,1
4,"In my understanding, the authors present these results to justify the validity of the approach.",pol,none,0.0,0,0
5,"However, this limits the novelty of the results relative to existing literature.",pol,pol_negative,1.0,1,1
6,"The authors do not focus (in the main paper) on GAN variants used currently, and it is not clear if the proposed approach provides improvement relative to the current state of the art implementations (see next paragraph)",pol,pol_negative,1.0,1,1
7,.,pol,none,0.0,0,0
8,"Moreover, if I understand correctly the WGAN analysis does not take into account that G and D are non-linear, and it is unclear if these can be done.",pol,pol_negative,1.0,1,1
9,I am also wondering if the comparison with the baselines is fair.,pol,pol_negative,1.0,1,1
10,"In other words, although in the present results, the proposed NF-SGAN/WGAN outperforms the baseline, the reported performance of the baselines is worse than in related works on CIFAR10.",pol,pol_negative,1.0,1,1
11,"In particular, FID of ~30 on CIFAR10 for the baselines is notably higher then current reported results on this dataset (e.g. Miyato et al. 2018; Chavdarova et al. 2019).",pol,none,0.0,0,0
12,"In my opinion, the authors could start from the existing state of the art implementations on this dataset, and report if negative feedback (NF) improves upon.",pol,pol_neutral,2.0,1,2
13,"As the approach uses NF is derived specifically for unstable dynamics, it is not clear to me how adding it would affect the training if the dynamics *is stable*. In this context, I consider that for example, the work by Balduzzi et al. 2018 may be relevant as it describes that the dynamics of games (the Jacobian) has two components, one of which describes the oscillating behavior (Hamiltonian game); whereas most games are a mix of oscillating and non-oscillating dynamics.",pol,pol_negative,1.0,1,1
14,It is not clear to me if NF would improve stability/performances in general games.,pol,pol_negative,1.0,1,1
15,"As the authors’ main claim is improved stability I am curious to see more detailed analysis on real-world datasets (e.g. multiple seed runs, 2nd-moment estimates over iterations as in Chavdarova et al. 2019).",pol,pol_neutral,2.0,2,2
16,"In summary, although the proposed perspective seems promising given the presented results and it is interesting, in my opinion, it does not provide novel insights nor obtains current state-of-the-art results on CIFAR10, or guarantee that if pursuing it would allow for solving current issues of GAN training.",pol,pol_positive,3.0,3,3
17,--- Minor ---,pol,none,0.0,0,0
18,- Abstract: ‘converge better’ it is not clear to me in what sense (faster/better final performances),pol,pol_negative,1.0,1,1
19,"- Page 3, Eq. 5: as D and G are functions of time here, eq 5 should maybe be written in more detail to include t",pol,pol_negative,1.0,2,1
20,"-  Page 3, Sec. 3: I think citing works that also focus on Dirac-GAN would motivate better why you focus on Dirac-GAN in this paper (e.g. writing `as in Mescheder et al. 2018`)",pol,pol_neutral,2.0,1,2
21,- Page 4: infinity - infinite,pol,pol_neutral,2.0,1,2
22,- Page 5: can also.. explains ->  explain,pol,pol_neutral,2.0,2,2
23,"This paper proposes, analyzes, and empirically evaluates PowerSGD (and a version with momentum), a simple adjustment to standard SGD algorithms that alleviates issues caused by poorly scaled gradients in SGD.",pol,none,0.0,0,0
24,"The rates in the theoretical analysis are competitive with those for standard SGD, and the empirical results argue that PowerSGD algorithms are competitive with widely used adaptive methods such as Adam and RMSProp, suggesting that PowerSGD may be a useful addition to the armory of adaptive SGD algorithms.",pol,none,0.0,0,0
25,"Overall I recommend acceptance of this paper, although I think there may be a couple of places where the authors overclaim a bit on the theoretical side.",pol,none,0.0,0,0
26,Specifically:,pol,none,0.0,0,0
27,•,pol,none,0.0,0,0
28,"The convergence analysis assumes a batch size equal to T, the number of steps of PowerSGD.",pol,none,0.0,0,0
29,"This implies that the amount of work (in FLOPs) done by the algorithm (at least the version being analyzed) is quadratic in T, which makes the convergence rates a bit misleading.",pol,pol_negative,1.0,1,1
30,"If one reframes the convergence rate in terms of FLOPs U=T^2 instead of iterations, then the convergence rate drops from 1/T to 1/sqrt(U), which undermines the claim in remark 3.4 that this analysis is superior to that of Yan et al. (2018).",pol,pol_negative,1.0,1,1
31,•,pol,none,0.0,0,0
32,"In Remark 3.4.3, the authors claim that another point of difference between their results and Yan et al.'s (2018) is that Yan et al. assume bounded gradients, an assumption that is not satisfied for e.g., mean squared error (MSE).",pol,none,0.0,0,0
33,"But a very similar assumption is hidden in the bounded-gradient-variance assumption Assumption 3.2; for example, Assumption 3.2 is clearly not satisfied by the least-squares regression problem",pol,pol_negative,1.0,0,1
34,min_β (1/N)Σ_n (y_n – x_n • β)^2,pol,none,0.0,0,0
35,with the minibatch gradient estimator computed over randomly chosen minibatches B:,pol,none,0.0,0,0
36,\hat g = (1/|B|) Σ_{n \in B} x_n (y_n – x_n • β).,pol,none,0.0,0,0
37,"As the norm of β goes to infinity, so does the expected norm of the error of \hat g. I'm not saying this is a particularly big",pol,pol_negative,1.0,1,1
38,"deal, just that it's not an improvement over Yan et al.'s result.",pol,none,0.0,1,0
39,"That aside, this seems like good work that could have a significant impact on practice.",pol,pol_positive,3.0,3,3
40,A couple of other minor points:,pol,none,0.0,0,0
41,•,pol,none,0.0,0,0
42,It looks like neither the experiments nor Theorem 3.2 show any benefit to PowerSGDM over PowerSGD.,pol,pol_negative,1.0,1,1
43,It would be nice to see some discussion (or at least speculation) on why that is.,pol,pol_negative,1.0,2,1
44,• Not all of the arrows in Figure 1 are pointing to the right lines.,pol,pol_negative,1.0,1,1
45,"• In the abstract, it might be good to clarify that the exponentiation is elementwise.",pol,pol_negative,1.0,2,1
46,"This paper introduces policy gradient methods for RL where the policy must choose a direction (a.k.a., the navigation problem).",pol,none,0.0,0,0
47,"Mapping techniques from ""non-directional"" problems (where the action space is not a direction) and then projeting on the sphere is sub-optimal (the variance is too big).",pol,pol_negative,1.0,0,1
48,"The authors propose to sample directly on the sphere, using the fact that the likelyhood of an angular Gaussian r.v. has *almost* a closed form and its gradient can almost be computed, up to some normalization term (the integral which is constant in the standard Gaussian case).",pol,none,0.0,0,0
49,This can be seen as a variance reduction techniques.,pol,none,0.0,0,0
50,"The proofs are not too intricate, for someone used to variance reduction (yet computations must be made quite carefully).",pol,pol_positive,3.0,0,3
51,"The result is coherent, interesting from a theoretical point of view and the experiment are somehow convincing.",pol,pol_positive,3.0,3,3
52,The main drawback would be the rather incrementality of that paper (basically sample before projecting is a bit better than projecting after sampling) and that this directional setting is quite limited...,pol,pol_negative,1.0,1,1
53,"In this paper, the authors consider CNN models from the lens of kernel methods.",pol,none,0.0,0,0
54,"They build upon past work that showed that such models can be seen to lie in appropriate RKHS, and derive upper and lower bounds for the kernel norm.",pol,none,0.0,0,0
55,"These bounds can be used as regularizers that help train more robust neural networks, especially in the context of euclidean perturbations of the inputs, and training GANs.",pol,none,0.0,0,0
56,They show that the bounds can also be used to recover existing special cases such as spectral norm penalizations and gradient regularization.,pol,none,0.0,0,0
57,"They derive generalization bounds from the point of view of adversarial learning, and report experiments to buttress their claims.",pol,none,0.0,0,0
58,"Overall, the paper is a little confusing.",pol,pol_negative,1.0,1,1
59,"A lot of the times, the result seem to be a derivative of the work by Bietti and Mairal, and looks like the main results in this paper are intertwined with stuff B+M already showed in their paper.",pol,pol_negative,1.0,1,1
60,"It's hard to ascertain what exactly the contributions are, and how they might not be a straightforward consequence of prior work (for example, combining results from Bietti and Mairal; and generalization bounds for linear models).",pol,pol_negative,1.0,1,1
61,"It might be nice to carefully delineate the authors' work from the former, and present their contributions.",pol,pol_negative,1.0,2,1
62,"Page 4: Other Connections with Lower bounds: The first line "" ""we may also consider ... "". This line is vague. How will you ensure the amount of deformation is such that the set \bar{U} is contained in U ?",pol,pol_negative,1.0,1,1
63,"Page 4 last paragraph: ""One advantage ... complex architectures in practice"" : True, but the tightness of the bounds *do* depend on ""f"" (specifically the RKHS norm).",pol,pol_negative,1.0,1,1
64,"It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?",pol,pol_negative,1.0,1,1
65,eqn (8): use something else to denote the function 'U'.,pol,pol_negative,1.0,1,1
66,You used 'U' before to denote the set.,pol,pol_negative,1.0,0,1
67,eqn (12): does \tilde{O} hide polylog factors? please clarify.,pol,pol_neutral,2.0,2,2
68,Summary,pol,none,0.0,0,0
69,"Authors present a decentralized policy, centralized value function approach (MAAC) to multi-agent learning.",pol,none,0.0,0,0
70,They used an attention mechanism over agent policies as an input to a central value function.,pol,none,0.0,0,0
71,Authors compare their approach with COMA (discrete actions and counterfactual (semi-centralized) baseline) and MADDPG (also uses centralized value function and continuous actions),pol,none,0.0,0,0
72,"MAAC is evaluated on two 2d cooperative environments, Treasure Collection and Rover Tower.",pol,none,0.0,0,0
73,"MAAC outperforms baselines on TC, but not on RT.",pol,none,0.0,1,0
74,"Furthermore, the different baselines perform differently: there is no method that consistently performs well.",pol,none,0.0,1,0
75,Pro,pol,none,0.0,0,0
76,- MAAC is a simple combination of attention and a centralized value function approach.,pol,pol_positive,3.0,0,3
77,Con,pol,none,0.0,0,0
78,"- MAAC still requires all observations and actions of all other agents as an input to the value function, which makes this approach not scalable to settings with many agents.",pol,pol_negative,1.0,1,1
79,"- The centralized nature is also semantically improbable, as the observations might be high-dimensional in nature, so exchanging these between agents becomes impractical with complex problems.",pol,pol_negative,1.0,1,1
80,"- MAAC does not consistently outperform baselines, and it is not clear how the stated explanations about the difference in performance apply to other problems.",pol,pol_negative,1.0,1,1
81,"- Authors do not visualize the attention (as is common in previous work involving attention in e.g., NLP).",pol,pol_negative,1.0,1,1
82,It is unclear how the model actually operates and uses attention during execution.,pol,pol_negative,1.0,1,1
83,Reproducibility,pol,none,0.0,0,0
84,"- It seems straightforward to implement this method, but I encourage open-sourcing the authors' implementation.",pol,pol_positive,3.0,1,3
85,Summary:,pol,none,0.0,0,0
86,"The authors take two tasks,sentiment analysis and natural language inference, and identify datasets for them which they counterfactually augment it by asking people over the Amazon Mechanical Turk Platform to change either the sentiment (in the case of sentiment analysis) or the nature of relationship in the NLI task by making minimal changes to the text that produce the targeted changes.",pol,none,0.0,0,0
87,Authors find that popular models trained on either fail on the other dataset while the models trained on both actually generalize much better.,pol,none,0.0,0,0
88,"This is because the original sample and its counterfactual pair the label changed , has the difference in the text that matters to the change and this pair could reduce spurious correlations that models might find in the data distribution.",pol,none,0.0,0,0
89,Pros:,pol,none,0.0,0,0
90,This is a very interesting experiment and certainly the dataset that will be released would be extremely valuable to the community.,pol,pol_positive,3.0,3,3
91,The one part (I dont have much NLP background but I do have a causality background) that I like most is that the new text generated are counterfactual in some real sense with respect to a real world generating process - that is people modifying text with changed targets.,pol,pol_positive,3.0,0,3
92,A lot of existing work that claim to do counterfactual changes do not specify assumptions about the generating mechanism.,pol,none,0.0,1,0
93,For counterfactuals to be valid they have to be intervention on the actual generating mechanism (or an assumed one) acting on a given unit (latent) that produced the current sample.,pol,none,0.0,0,0
94,"The paper in that respect (even if it does not explicitly specify relationship between counterfactuals and generating mechanisms) tries to be faithful to a ""strict causal notion"" by actually asking people to modify the text.",pol,pol_positive,3.0,0,3
95,Cons:,pol,none,0.0,0,0
96,"- I think the authors want to make an explicit connection to counterfactuals as understood in the causality community. Then they shy away from it saying they are inspired by it. May be a formal exposition in the supplement about counterfactuals and generating mechanisms could help readers from other communities (NLP) even it means repeating standard/synthetic examples. Its good to say what exactly in a counterfactual generation process, the ""people"" in amazon turk were substituting.",pol,pol_negative,1.0,2,1
97,-  Is the romantic/ horror flips and their absence the only spurious thing in Figure 4 ?,pol,pol_negative,1.0,2,1
98,"-  In figure 6, it appears that BERT is sensitive to the domain - does it mean that it is bad ? - Authors indicate that ideally it must not be so. Because Table 3 results seem to indicate that BERT performs the best in almost all the cases .",pol,pol_neutral,2.0,1,2
99,-  Can the authors highlight the best performances in each case in the Tables by a bold face.,pol,pol_neutral,2.0,2,2
100,It helps easily eye ball the best performing model.,pol,none,0.0,3,0
101,"This paper proposes a multi-agent hierarchical reinforcement learning algorithm so that multiple humanoid robots can navigate in multi-agent settings (e.g. avoid collisions, collaboration, chase and escape) in a physically simulated environment.",pol,none,0.0,0,0
102,The key difference of this paper with the prior work on MARL is that it used an accurate physics simulation of humanoid robots.,pol,none,0.0,1,0
103,This is the main reason of using the hierarchical RL.,pol,none,0.0,0,0
104,"In general, I like this paper.",pol,pol_positive,3.0,3,3
105,It is an important step towards multi-agent learning in complex physical environments.,pol,pol_positive,3.0,0,3
106,"The results look appealing, too.",pol,pol_positive,3.0,3,3
107,"However, I voted for ""Weak Reject"" for two reasons.",pol,none,0.0,0,0
108,"First, the technical contribution is lean. Neither the multi-agent learning or the hierarchical learning of the algorithm is novel.",pol,pol_negative,1.0,1,1
109,The combination of these two methods seems straightforward.,pol,pol_negative,1.0,3,1
110,"Once a low-level walking controller is trained, the high-level multi-agent navigation control is not much different from simple environments, e.g. point mass control, used in the previous works.",pol,pol_negative,1.0,1,1
111,"I do not understand the ""deep integration of MARL and HRL"" that is claimed in the Introduction.",pol,pol_negative,1.0,1,1
112,"I also do not agree with another claim that ""We consider the simulation and training environment to be another novel contribution... few simulator support more than one agent, at most 2"".",pol,pol_negative,1.0,1,1
113,"In most of the simulators that I am familiar with, such as Mujoco, Bullet, DART, it is straightforward to add multiple simulated robots.",pol,none,0.0,0,0
114,"Second, the writing can be greatly improved.",pol,pol_negative,1.0,2,1
115,"Almost half of the technical details are buried in ""8. Supplementary material"".",pol,pol_negative,1.0,1,1
116,"Since it is not fair to use ""Supplementary material"" as a way to extend the page limit, I will make my judgement of the paper solely based on the contents up to Section 7.",pol,pol_negative,1.0,0,1
117,"In the main text (up to Section 7), there is no mentioning of how the low-level controllers are learned, and how to combine PPO in a MARL partial parameter sharing setting.",pol,pol_negative,1.0,1,1
118,I think that these are important details and may also be the contributions of this paper.,pol,none,0.0,0,0
119,Most of these should be moved to the main text.,pol,pol_negative,1.0,1,1
120,Here are some more suggestions on writing:,pol,none,0.0,0,0
121,"1) Certain paragraphs in the main text can be significantly shortened, such as the reward shaping in Section 5.2.",pol,pol_negative,1.0,1,1
122,"2) It would be great if the paper can clearly define the experiments: ""waypoint"", ""oncoming"", ""mall"", and ""bottleneck"".",pol,pol_negative,1.0,2,1
123,"3) The paper needs a thorough proof-reading. There are many grammar mistakes, typos, missing citations. For example,",pol,pol_negative,1.0,1,1
124,promiss->promise,pol,none,0.0,2,0
125,week signal->weak signal,pol,none,0.0,2,0
126,missing citation [?] in page 3,pol,none,0.0,1,0
127,reuse the same symbol v_{com} for agent's velocity and desired speed in eq(3),pol,none,0.0,0,0
128,This paper proposes an extension of NeuralLP that is able to learn a very restricted (in terms of expressiveness) set of logic rules involving numeric properties.,pol,none,0.0,0,0
129,"The basic idea behind NeuralLP is quite simple: traversing relationships in a knowledge graph can be done by multiplicating adjacency matrices, and which rules hold and which ones don't can be discovered by learning an attention distribution over rules from data.",pol,none,0.0,0,0
130,"The idea is quite clever: relationships between numeric data properties of entities, such as age and heigh, can also be linked by relationships such as \leq and \geq, and those relations can be treated in the same way as standard knowledge graph relationship by the NeuralLP framework.",pol,none,0.0,3,0
131,"A major drawback in applying this idea is that the corresponding relational matrix is expensive to both materialise, and use within the NeuralLP framework (where matrices are mostly sparse).",pol,none,0.0,1,0
132,"To this end, authors make this process tractable by using dynamic programming and by defining such a matrix as a dynamic computation graph by means of the cumsum operator.",pol,none,0.0,0,0
133,"Furthermore, authors also introduce negated operators, also by defining the corresponding adjacency matrices by means of computation graphs.",pol,none,0.0,0,0
134,Authors evaluate on several datasets - two real world and two synthetic - often showing more accurate results than the considered baselines.,pol,pol_positive,3.0,0,3
135,"One thing that puts me off is that, in Table 2, AnyBurl (the single one baseline authors considered other than the original NeuralLP) yields better Hits@10 values than Neural-LP-N, but the corresponding bold in the results is conveniently omitted.",pol,pol_negative,1.0,1,1
136,"Another concern I have is that the expressiveness of the learned rules can be somehow limited, but this paper seems like a good star towards learning interpretable rules involving multiple modalities.",pol,pol_positive,3.0,3,3
137,Missing references - authors may want to consider citing https://arxiv.org/abs/1906.06187 as well in Sec. 2 - it seems very related to this work.,pol,pol_negative,1.0,1,1
138,MEASURING DENSITY AND SIMILARITY OF TASK RELEVANT INFORMATION IN NEURAL REPRESENTATIONS,pol,none,0.0,0,0
139,Summary:,pol,none,0.0,0,0
140,This work attempts to define two kinds of metrics (metrics for information density and for information similarity) for the sake of automatically detecting similarity between tasks so that transfer learning can be done more efficiently.,pol,none,0.0,0,0
141,"The concepts are clearly explained, and the metric for information density seems to match up with intuitions coming out of forward selections approaches.",pol,pol_positive,3.0,3,3
142,The metric for information transfer seems to be the commonplace metric that other works default to when they show that pre-trained representations are effective on downstream tasks.,pol,none,0.0,0,0
143,"It is not clear that the notion of similarity through classifier weights makes sense, but see below for clarification questions.",pol,pol_negative,1.0,1,1
144,"The problem addressed (automatic similarity scoring of tasks) is important for transfer learning, and thus the results have potential to be very impactful if they generalize to other kinds of tasks; as is, they seem to apply only to classification tasks, but that is a good step.",pol,pol_positive,3.0,0,3
145,Pros:,pol,none,0.0,0,0
146,Clearly written; experiments on the datasets chosen do seem to suggest that the proposed methods have potential.,pol,pol_positive,3.0,3,3
147,Brings in nice intuition from forward feature selection.,pol,pol_positive,3.0,3,3
148,An important problem with potential for high impact.,pol,pol_positive,3.0,0,3
149,Cons:,pol,none,0.0,0,0
150,It is not clear to me that the classifier difference metric is well-defined.,pol,pol_negative,1.0,1,1
151,Is there a constraint on the CFS and classifiers that ensure the difference between the weights really captures what is suggested?,pol,pol_negative,1.0,2,1
152,"Is it not the case that classifier weights could come out quite different despite the tasks being quite similar if the linear classifiers learned to capitalize on dissimilar, yet equally fruitful patterns in the input features?",pol,pol_neutral,2.0,2,2
153,Do you have thoughts on how this could be applied outside the context of sentence representations and further outside the context of classification?,pol,pol_neutral,2.0,2,2
154,"Those seem to be quite limiting features of these methods, which is not to say that they are not useful in that realm, but only to clarify my understanding of their possible scope of application.",pol,pol_negative,1.0,1,1
155,"These classification datasets are often so close, that I do wonder whether even simpler methods would work just as well.",pol,pol_negative,1.0,1,1
156,"For example, clustering on bags-of-words might also show that SST, SST-fine, and IMDb are close/similar/transferable.",pol,pol_negative,1.0,0,1
157,The same could be said for SICK and SNLI.,pol,pol_negative,1.0,1,1
158,It would be nice to see a comparison to such baselines in order to get a sense of how the proposed methods give insights that other unsupervised or supervised methods might give just as well.,pol,pol_positive,3.0,2,3
159,"Otherwise, it is hard to tell how significant these correlations are. Since the end goal is to determine transferability of tasks and not the methods, it does seem like there are simpler baselines that you could compare against.",pol,pol_negative,1.0,1,1
160,[Edit] I changed my rating from 4 to 5 based on the author responses.,pol,none,0.0,0,0
161,=======,pol,none,0.0,0,0
162,This paper proposed a GAN that learns a disentangled factors of variations in unsupervised (or weakly-supervised) manner.,pol,none,0.0,0,0
163,"To this end, the proposed method incorporates a contrastive loss together with Siamese network, which encourages the generator to output smaller variations in samples if they are drawn by varying the same latent factors.",pol,none,0.0,0,0
164,"The proposed idea is evaluated on simple datasets such as MNIST and centered faces, and show that it is able to learn disentangled latent codes by incorporating some heuristics.",pol,none,0.0,0,0
165,"Although the paper presents an interesting and reasonable idea, I think the paper is incomplete and in the proof-of-concept stage.",pol,pol_positive,3.0,3,3
166,"In terms of method, the guidance for learning Siamese networks are designed heuristically (e.g. edges, colors, etc.) which limits its applicability over various datasets; I think that designing more principled approach to build such guidances from data should be one of the key contributions of the paper.",pol,pol_negative,1.0,1,1
167,"In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing.",pol,pol_negative,1.0,1,1
168,"In conclusion, I suggest a reject of this paper due to the lacks of comprehensive study and evaluation.",pol,pol_negative,1.0,1,1
169,*CAVEAT*,pol,none,0.0,0,0
170,"I must caveat that this paper is out of my comfort zone in terms of topic, so my review below should only be taken lightly.",pol,none,0.0,1,0
171,It also explaina the brevity of my review.,pol,none,0.0,0,0
172,My apologies to the authors and other reviewers.,pol,none,0.0,0,0
173,*Paper summary*,pol,none,0.0,0,0
174,"The authors design a set architecture, which is equivariant to permutations on the input.",pol,none,0.0,0,0
175,"They show the simplest such set architecture, which preserves equivariance, while being a universal approximator.",pol,none,0.0,0,0
176,"Nicely this architecture relies on a correction to PointNet, called PointNetST, which they show is not equivariant universal.",pol,none,0.0,0,0
177,"Furthermore, they run experiments on a few toy examples demonstrating that their system performs well.",pol,pol_positive,3.0,0,3
178,*Paper decision*,pol,none,0.0,0,0
179,"I have decided to give this paper a weak accept, since it contains both theory and nice experiments.",pol,pol_positive,3.0,3,3
180,"To change to a firm accept, I think the paper needs some changes in written style mainly, to make it friendlier to newcomers to the area, which can easily be implemented in the camera ready stage of paper preparation.",pol,pol_negative,1.0,1,1
181,"For instance, the omission of a results discussion section or a conclusion are clearly not reader friendly.",pol,pol_negative,1.0,1,1
182,*Supporting arguments*,pol,none,0.0,0,0
183,"- The paper is written clearly. This said, it requires a great deal of effort to follow the maths if you are not already fluent in a lot of the ideas used in the paper (this includes myself).",pol,pol_positive,3.0,3,3
184,- I think the structure of the paper is fine for this sort of work. Perhaps at the beginning it would be more useful to spend more time on a roadmap of the results presented in the paper and to explain the exact significance of why the reader should want to continue reading.,pol,pol_neutral,2.0,2,2
185,"- I think the selection of experiments is nice, containing both regression and classification. What would have been nicer would be to perform some sort of ablation study, where the authors studied how the representational capacity of the network changed as a result of them introducing the universal linear transmission layer.",pol,pol_neutral,2.0,3,2
186,- A direct theoretical and experimental comparison between PointNet and PointNetST would have been useful for me to understand the impact of the change that the authors introduce.,pol,pol_neutral,2.0,2,2
187,*,pol,none,0.0,0,0
188,Questions/notes for the authors*,pol,none,0.0,0,0
189,- Please answer my concerns in the support arguments,pol,none,0.0,0,0
190,- Where is the conclusion section?,pol,none,0.0,2,0
191,This paper provides exact bounds on the risk when training a two-layer neural network in an asymptotic regime.,pol,none,0.0,0,0
192,"Namely, the paper considers training under the square-loss objective, a two-layer neural network with $h$ hidden units on inputs of dimension $d$ and training on $n$ samples.",pol,none,0.0,0,0
193,"The asymptotic regime is considered by making all of $d$, $h$, $n$ go to $\infty$, in a way that the ratio $d/n$ approaches $\gamma_1$ and the ratio $h/n$ approaches $\gamma_2$.",pol,none,0.0,0,0
194,"This paper considers the following scenarios of training described below, where the data is generated from a linear model on Gaussian inputs and with a zero-mean noise.",pol,none,0.0,0,0
195,"The emphasis of the results is on understanding when a ""double descent"" type phenomenon occurs (""Double descent"" is a recently coined phenomenon in literature where the risk, as a function of the ""complexity of the model"", initially has a classical U-shape behavior, but eventually decreases again once the complexity of the model exceeds the number of training points.)",pol,none,0.0,0,0
196,1. Training only the second layer: The risk is first decomposed into a bias and a variance term.,pol,none,0.0,1,0
197,An exact bound on the variance term of the risk is obtained.,pol,none,0.0,0,0
198,"While the exact nature of the bound is rather complex to parse, the takeaway is that a double descent phenomenon is observed in terms of $\gamma_2$, namely, the risk blows up when $h \approx n$, but decreases as $h$ is increased beyond $n$.",pol,none,0.0,0,0
199,"2. Training only the first layer: Two different regimes are considered here, depending on the scale of initialization, called ""vanishing"" and ""non-vanishing"" initializations.",pol,none,0.0,0,0
200,"In both regimes, the risk is independent of $\gamma_2$, that is, the risk does not depend on number of hidden units (although the risk bounds are different and there is an additional assumption in the case of non-vanishing initialization to ensure that the initialized network computes the zero function).",pol,none,0.0,0,0
201,"In other words, a ""double descent"" phenomenon is not observed in this setting.",pol,none,0.0,0,0
202,Recommendation:,pol,none,0.0,0,0
203,"I recommend ""weak acceptance"".",pol,none,0.0,0,0
204,The paper extends prior works that obtain asymptotic risk bounds on linear models to the setting of two-layer neural networks (where only one layer is trained).,pol,none,0.0,0,0
205,"However, I am unable to assess the technical novelty of this work as it seems to heavily rely on prior work which in turn use techniques from random matrix theory.",pol,pol_negative,1.0,1,1
206,Technical Comments:,pol,none,0.0,0,0
207,"- I felt that while it is valuable to have exact bounds on the risk, the form of the bounds are quite complex and hard to parse (especially in Thm 4, case of training only the second layer).",pol,pol_negative,1.0,1,1
208,"Moreover, these bounds are just in the case where the teacher model is linear and while it is claimed that this could be relaxed to a more general class of functions, the specific bounds might change drastically.",pol,pol_negative,1.0,1,1
209,"So any insights on the nature of these bounds will be valuable, especially with some comments on how these bounds change if the teacher model is itself realized as a 2-layer neural network.",pol,pol_negative,1.0,2,1
210,"- The parameter count of a 2-layer network with $h$ hidden units and input dimension $d$ is $O(dh)$. So perhaps it makes sense to study an asymptotic regime where $dh/n$ approaches $\gamma$, instead of both d and h growing linearly in n. While this issue is hinted at in the discussion section, I don't understand the statement ""the mechanism that provably gives rise to double descent from previous works Hastie et al. (2019); Belkin et al. (2019) might not translate to optimizing two-layer neural networks.""",pol,pol_negative,1.0,1,1
211,- Another future direction that could be included in discussions is the setting where both layers are trained simultaneously.,pol,pol_neutral,2.0,2,2
212,This paper introduces the study of the problem of frequency estimation algorithms with machine learning advice.,pol,none,0.0,0,0
213,"The problem considered is the standard frequency estimation problem in data streams where the goal is to estimate the frequency of the i-th item up to an additive error, i.e. the |\tilde f_i - f_i| should be minimized where \tilde f_i is the estimate of the true frequency f_i.",pol,none,0.0,0,0
214,Pros:,pol,none,0.0,0,0
215,-- Interesting topic of using machine learned advice to speed up frequency estimation is considered,pol,pol_positive,3.0,3,3
216,-- New rigorous bounds are given on the complexity of frequency estimation under Zipfian distribution using machine learned advice,pol,pol_positive,3.0,3,3
217,-- Experiments are given to justify claimed improvements in performance,pol,pol_positive,3.0,3,3
218,Cons:,pol,none,0.0,0,0
219,"-- While the overall claim of the paper in the introduction seems to be to speed up frequency estimation using machine learned advice, results are only given for the Zipfian distribution.",pol,pol_negative,1.0,1,1
220,"-- The overall error model in this paper, which is borrowed from Roy et al. is quite restrictive as at it assumes that the queries to the frequency estimation data structure are coming from the same distribution as that given by f_i’s themselves.",pol,pol_negative,1.0,1,1
221,"While in some applications this might be natural, this is certainly very restrictive in situations where f_i’s are updated not just by +/-1 increments but through arbitrary +/-Delta updates, as in this case it might be more natural to assume that the distribution of the queries might be proportional to the frequency that the corresponding coordinate is being updated, for example.",pol,none,0.0,1,0
222,-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.,pol,pol_negative,1.0,3,1
223,-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher’18.,pol,pol_negative,1.0,1,1
224,-- The analysis is relatively straightforward and boils down to bucketing the error and integration over the buckets.,pol,pol_negative,1.0,1,1
225,Other comments:,pol,none,0.0,0,0
226,"-- The machine learned advice is assumed to be flawless at identifying the Heavy Hitters, authors might want to consider incorporating errors in the analysis.",pol,pol_negative,1.0,2,1
227,Summary,pol,none,0.0,0,0
228,The paper presents a novel approach for learning a generative model where different factors of variations can be independently manipulated.,pol,none,0.0,0,0
229,The method is build upon  the GAN framework where the latent variables are divided into different subsets (chunks) which are expected to encode information about high-level factors of variation.,pol,none,0.0,0,0
230,"To this end, a Siamese Network for each chunk is trained with a contrastive loss minimizing the distance between generated images sharing the same factor (the latent variables in the chunk are equal), and maximizing the distance between pairs where the latent variables differ.",pol,none,0.0,0,0
231,"Given that the proposed model fails in this fully-unsupervised setting, the authors propose to add weak-supervision into the model by forcing the Siamese networks to  focus only on particular aspects of generated images (e.g, color, edges, etc..).",pol,none,0.0,0,0
232,This is achieved by applying  a basic transformation  over the input images in order to remove specific information.,pol,none,0.0,0,0
233,The evaluation of the  proposed model is carried out using the MS-Celeb dataset where the authors provide qualitative results.,pol,none,0.0,0,0
234,Methodology,pol,none,0.0,0,0
235,*Disentangling generative factors without explicit labels is a challenging and interesting problem.,pol,pol_positive,3.0,3,3
236,The idea of dividing the latent representation in different subsets and using a proxy task involving triplets of images has been already explored in [3].,pol,none,0.0,0,0
237,"However, the use of Siamese networks in this context is novel and sound.",pol,pol_positive,3.0,3,3
238,"*As shown in the reported results, the proposed method fails to learn meaningful factors in the unsupervised setting.",pol,none,0.0,1,0
239,"However, the authors do not provide an in-depth discussion of this phenomena.",pol,pol_negative,1.0,1,1
240,"Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios.",pol,pol_negative,1.0,1,1
241,*The strategy proposed to introduce weak-supervision is too ad-hoc.,pol,pol_negative,1.0,1,1
242,I agree that using cues such as the average color of an image can be useful if we want to model basic factors of variation.,pol,pol_positive,3.0,0,3
243,"However, it is unclear how a similar strategy could be applied if we are interested in learning variables with higher-level semantics such as the expression of a face or its pose.",pol,pol_negative,1.0,1,1
244,"*As far as I understand, the transformations applied to the input images (e.g, edge detection) must be differentiable (given that it is necessary to backpropagate the gradient of the contrastive loss through the generator network).",pol,none,0.0,1,0
245,"If this is the case, this should be properly discussed in the paper.",pol,pol_negative,1.0,1,1
246,"Moreover, given that the amount of differentiable transformations is reduced, this also limits the application of the proposed method for more interesting scenarios.",pol,none,0.0,1,0
247,*It is not clear why the latent variables modelling the generative factors are defined using a Gaussian prior.,pol,pol_negative,1.0,1,1
248,How the case where two images have a very similar latent factor is avoided while generating pairs of images for the Siamese network?,pol,pol_negative,1.0,2,1
249,Have the authors considered to use categorical or binary variables?,pol,pol_negative,1.0,2,1
250,The use of the contrastive loss sounds more appropriate in this case.,pol,none,0.0,1,0
251,Experimental results,pol,none,0.0,0,0
252,*The experimental section is too limited.,pol,pol_negative,1.0,1,1
253,"First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion.",pol,pol_negative,1.0,1,1
254,"For example, when the edge extractor is used, what kind of information is modeled by the latent variables? Is it consistent across different samples?",pol,pol_neutral,2.0,2,2
255,"Moreover, it is not clear why the authors have limited the evaluation to the case where only two “chunks” are used.",pol,pol_negative,1.0,1,1
256,"In principle, the method could be applied with many more subsets of latent variables and then manually inspect them to check it they are semantically meaningful (see [2])",pol,none,0.0,0,0
257,"*As previously mentioned, there are many recent works addressing the same problem from a fully-unsupervised perspective [1,2,3].",pol,none,0.0,0,0
258,"All these works provide quantitative results evaluating the learned representations by using them to predict real labels (e.g, attributes in the CelebA data-set).",pol,none,0.0,0,0
259,The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation.,pol,pol_negative,1.0,1,1
260,This could clarify the advantages of the weakly-supervised strategy compared to unsupervised approaches.,pol,none,0.0,0,0
261,Review summary,pol,none,0.0,0,0
262,+The addressed problem (learning disentangled representations without explicit labeling) is challenging and interesting.,pol,pol_positive,3.0,3,3
263,+The idea of using a proxy task (contrastive loss with triplets of generated images) is somewhat novel and promising.,pol,pol_positive,3.0,3,3
264,- The authors report only negative results for the fully-unsupervised version of UD-GAN The paper lacks and in-depth discussion about why this negative result is interesting.,pol,pol_negative,1.0,1,1
265,-The strategy proposed to provide weak-supervision to the model is too ad-hoc and it is not clear how to apply it in general applications,pol,pol_negative,1.0,1,1
266,-The experimental section do not clarify the benefits of the proposed approach.,pol,pol_negative,1.0,1,1
267,"In particular, the qualitative results are too limited and no quantitative evaluations is provided.",pol,pol_negative,1.0,1,1
268,"[1] Variational Inference of Disentangled Latent Concepts from Unlabelled Observations (Kumar et al, ICLR 2018)",pol,none,0.0,0,0
269,"[2] Beta-vae: Learning basic visual concepts with a constrained variational framework. (Higgins et. al, ICLR 2017)",pol,none,0.0,0,0
270,[3] Disentangling Factors of Variation by Mixing Them.,pol,none,0.0,0,0
271,"(Hu et. al, CVPR  2018)",pol,none,0.0,0,0
272,This paper proposed several extensions to the Neural LP work.,pol,none,0.0,0,0
273,"Specifically, this paper addresses several limitations, including numerical variables, negations, etc.",pol,none,0.0,1,0
274,"To efficiently compute these in the original Neural LP framework, this paper proposed several computation tricks to accelerate, as well as to save memory.",pol,none,0.0,0,0
275,"Experiments on benchmark datasets show significant improvements over previous methods, especially in the case where numerical variables are required.",pol,none,0.0,3,0
276,"I think overall the paper is written clearly, with good summarization of existing works. Also I like the simple but effective tricks for saving the computation and memory.",pol,pol_positive,3.0,3,3
277,"One main concern is, how general this approach would be? As it is a good extension for Neural LP, it is not clear that the framework of Neural LP is flexible or powerful enough in general.",pol,pol_negative,1.0,1,1
278,"For example, if rules contain quantifiers, how would this be extended?",pol,pol_negative,1.0,2,1
279,Minor comments:,pol,none,0.0,0,0
280,"1) 4.1,  “O(n^2/2)” -- just put O(n^2) or simply write as n^2/2.",pol,pol_negative,1.0,1,1
281,"2) How are the rules from in Eq (2)? i.e., how is \beta_i selected for each i?",pol,pol_negative,1.0,2,1
282,In the extreme case it would be all the permutations.,pol,none,0.0,1,0
283,"3) I would suggest a different name other than Neural-LP-N, as it is somewhat underselling this work. Also it makes Table 2 not that easy to read.",pol,pol_negative,1.0,1,1
284,"This is a paper of the verification of neural networks, i.e. check their robustness,",pol,none,0.0,0,0
285,and the main contribution here is to tackle it as a statistical problem adressed with,pol,none,0.0,0,0
286,multi-level splitting Monte Carlo approach.,pol,none,0.0,0,0
287,"I found the paper well motivated and original,",pol,pol_positive,3.0,3,3
288,resulting in a publishable piece of research up to a few necessary adjustments.,pol,none,0.0,0,0
289,These,pol,pol_negative,1.0,0,1
290,concern principally notation issues and some potential improvements in the writing.,pol,none,0.0,0,0
291,"Let me list below some main remarks along the text, including also some typos.",pol,none,0.0,0,0
292,"* In the introduction, ""the classical approach"" is mentioned but to be the latter is",pol,pol_negative,1.0,1,1
293,insufficiently covered. Some more detail would be welcome.,pol,none,0.0,2,0
294,"* page 2, ""predict the probability"": rather employ ""estimate"" in such context?",pol,pol_neutral,2.0,2,2
295,"* ""linear piecewise"": ""piecewise linear""?",pol,pol_neutral,2.0,2,2
296,"* what is ""an exact upper bound""?",pol,pol_neutral,2.0,2,2
297,"* In related work, no reference to previous work on ""statistical"" approaches to NN",pol,pol_negative,1.0,1,1
298,verification. Is it actually the case that this angle has never been explored so far?,pol,none,0.0,2,0
299,"* I am not an expert but to me ""the density of adversarial examples"" calls for further",pol,pol_neutral,2.0,1,2
300,explanation.,pol,none,0.0,0,0
301,* From page 3 onwards: I was truly confused by the use of [x] throughought the text,pol,pol_negative,1.0,1,1
302,(e.g. in Equation (4)),pol,none,0.0,0,0
303,.,pol,none,0.0,0,0
304,"x is already present within the indicator, no need to add yet",pol,pol_negative,1.0,1,1
305,another instance of it. Here and later I suffered from what seems to be like an awkward,pol,none,0.0,1,0
306,attempts to stress dependency on variables that already appear or should otherwise,pol,none,0.0,0,0
307,appear in a less convoluted way.,pol,none,0.0,0,0
308,"* In Section 4, it took me some time to understand that the considered metrics do not",pol,pol_negative,1.0,1,1
309,require actual observations but rather concern coherence properties of the NN per se.,pol,none,0.0,1,0
310,"While this follows from the current framework, the paper might benefit from some more",pol,pol_neutral,2.0,1,2
311,explanation in words regarding this important aspect.,pol,none,0.0,0,0
312,"* In page 6, what is meant by ""more perceptually similar to the datapoint""?",pol,none,0.0,2,0
313,"* In the discussion: is it really ""a new measure"" that is introduced here?",pol,pol_neutral,2.0,2,2
314,*,pol,none,0.0,0,0
315,"In the appendix: the MH acronym should better be introduced, as should the notation",pol,pol_neutral,2.0,1,2
316,"g(x,|x')",pol,none,0.0,0,0
317,if not done elsewhere (in which case a cross-reference would be welcome).,pol,none,0.0,1,0
318,"Besides this, writing ""the last samples"" requires disambiguation (using ""respective""?).",pol,pol_neutral,2.0,0,2
319,"The paper presents a new attack, called the shadow attack, that can maintain the imperceptibility of adversarial samples when out of the certified radius.",pol,none,0.0,0,0
320,This work not only aims to target the classifier label but also the certificate by adding large perturbations to the image.,pol,none,0.0,0,0
321,"The attacks produce a 'spoofed' certificate, so though these certified systems are meant to be secure, can be attacked.",pol,none,0.0,0,0
322,Theirs seem to be the first work focusing on manipulating certificates to attack strongly certified networks.,pol,none,0.0,0,0
323,"The paper presents shadow attack, that is a generalization of the PGD attack.",pol,none,0.0,0,0
324,"It involves creation of adversarial examples, and addition of few constraints that forces these perturbations to be small, smooth and not many color variations.",pol,none,0.0,0,0
325,For certificate spoofing the authors explore different spoofing losses for l-2(attacks on randomized smoothing) and l-inf(attacks on crown-ibp) norm bounded attacks.,pol,none,0.0,0,0
326,Strengths: The paper is well written and well motivated.,pol,pol_positive,3.0,3,3
327,"The work is novel since most of the current work focus on the imperceptibility and misclassification aspects of the classifier, but this work addresses attacking the strongly certified networks.",pol,pol_positive,3.0,0,3
328,Weakness: It would be good to see some comparison to the state of the art,pol,pol_negative,1.0,2,1
329,"The contributions of this paper are in the domain of policy search, where the authors combine evolutionary and gradient-based methods.",pol,none,0.0,0,0
330,"Particularly, they propose a combination approach based on cross-entropy method (CEM) and TD3 as an alternative to existing combinations using either a standard evolutionary algorithm or a goal exploration process in tandem with the DDPG algorithm.",pol,none,0.0,0,0
331,"Then, they show that CEM-RL has several advantages compared to its competitors and provides a satisfactory trade-off between performance and sample efficiency.",pol,none,0.0,0,0
332,"The authors evaluate the resulting algorithm, CEM-RL, using a set of benchmarks well established in deep RL, and they show that CEM-RL benefits from several advantages over its competitors and offers a satisfactory trade-off between performance and sample efficiency.",pol,none,0.0,0,0
333,"It is a pity to see that the authors provide acronyms without explicitly explaining them such as DDPG and TD3, and this right from the abstract.",pol,pol_negative,1.0,1,1
334,"The parer is  in general interesting, however the clarity of the paper is hindered",pol,pol_negative,1.0,1,1
335,"by the existence of several typos, and the writing in certain passages can be improved. Example of typos include  “an surrogate gradient”, “""an hybrid algorithm”,  “most fit individuals are used ” and so on…",pol,pol_negative,1.0,0,1
336,In the related work the authors present the connection between their work and contribution to the state of the art in a detailed manner.,pol,pol_positive,3.0,0,3
337,"Similarly, in section 3 the authors provide an extensive background allowing to understand their proposed method.",pol,pol_positive,3.0,3,3
338,"In equation 1, 2 the updates of  \mu_new and \sigma_new uses \lambda_i, however the authors provide common choices for \lambda without any justification or references.",pol,pol_negative,1.0,1,1
339,The proposed method is clearly explained and seems convincing.,pol,pol_positive,3.0,3,3
340,However the theoretical contribution is poor. And the experiment uses a very classical benchmark providing simulated data.,pol,pol_negative,1.0,1,1
341,"1. In the experimental study, the authors present the value of their tuning parameters (learning rate, target rate, discount rate…) at the initialisation phase without any justifications. And the experiments are limited to simulated data obtained from MUJOCO physics engine - a very classical benchmark.",pol,pol_negative,1.0,1,1
342,2. Although the experiments are detailed and interesting they support poor theoretical developments and use a very classical benchmark,pol,pol_negative,1.0,3,1
343,The rebuttal provided by the authors is convincing.,pol,none,0.0,3,0
344,The reinforcement learning tasks with sparse rewards are very important and challenging.,pol,none,0.0,3,0
345,The main idea of this work is to encourage intra-life novelty.,pol,none,0.0,0,0
346,The authors introduce the curiosity grid and the intrinsic reward term so that the agent can explore toward unvisited states at every episode.,pol,none,0.0,0,0
347,"However, the results are not enough to be accepted to ICLR having a very high standard.",pol,pol_negative,1.0,1,1
348,"In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C.",pol,pol_negative,1.0,0,1
349,There are some RL algorithms reported to be better than A2C.,pol,none,0.0,0,0
350,"For instance, I would like to see the comparison between DeepCS and SmartHash by Tang et al 2017.",pol,pol_negative,1.0,2,1
351,=================================================================================================,pol,none,0.0,0,0
352,I've read the rebuttal. I updated my score but still not vote for accept.,pol,none,0.0,1,0
353,This paper is not my main research area.,pol,none,0.0,1,0
354,"Very unfortunately, this paper was assigned to me.",pol,none,0.0,0,0
355,The main issue of this paper is the fair comparisons with other works.,pol,pol_negative,1.0,1,1
356,"However, I don't have enough knowledges to judge this point.",pol,none,0.0,1,0
357,So please assess this paper with other reviewers comments.,pol,none,0.0,0,0
358,"This paper seeks to separate ""causal"" features from ones with spurious correlations in the context of natural language machine learning tasks.",pol,none,0.0,0,0
359,The proposed approach is to ask human annotators to alter examples in a minimal way that changes the label.,pol,none,0.0,0,0
360,Thereby the humans separate out the causal features (those changed) from the spurious or irrelevant features (those left unchanged).,pol,none,0.0,0,0
361,"Experiments show that classifiers trained on the original data perform poorly on the altered data and vice versa, but (unsurprisingly) training on the union of the two datasets results in a classifier that performs well in both cases.",pol,pol_positive,3.0,0,3
362,"Furthermore, training an SVM on the original results in irrelevant attributes (such as movie genre) being weighted, whereas these weights are largely removed when training on the union of the datasets.",pol,pol_positive,3.0,0,3
363,"This suggests that the augmented training data results in weighting the ""right"" features more.",pol,pol_positive,3.0,0,3
364,"Overall, I think this paper should be accepted because it makes several interesting contributions: It proposes an interesting approach, shows intriguing experimental results, and produces an interesting dataset (size ~2k) that may be useful for future testing.",pol,pol_positive,3.0,3,3
365,The main limitation of the paper is that the evidence is largely circumstantial.,pol,pol_negative,1.0,1,1
366,"The method has intuitive appeal and the experimental results are suggestive, but the experiments do not conclusively show that the method achieves something that ordinary machine learning does not.",pol,pol_negative,1.0,1,1
367,"My suggestion for a further experiment would be to apply the movie review classifiers to, say, book reviews -- something where the task is fundamentally the same but the context is different. If the classifier trained on the union of the original and altered datasets performs better than a classifier trained on only on dataset, then that is strong evidence that this approach yields better extrapolation.",pol,pol_positive,3.0,1,3
368,This paper gives a theoretical analysis of an interesting statistical physics technique known as replica exchange.,pol,none,0.0,0,0
369,"The basic idea is that Langevin dynamics at low temperature is slow to converge, and that one could potentially boost the convergence by alternating between low and high temperature.",pol,none,0.0,0,0
370,"At the extreme one could imagine running in parallel a random search and a gradient descent, and ``teleporting"" the gradient descent algorithm whenever the random search algorithm finds a point with better value.",pol,none,0.0,0,0
371,This makes a lot of sense and it is nice to see a theoretical analysis of this.,pol,pol_positive,3.0,3,3
372,"The mathematics are sound, but I do not know whether it is an appropriate submission for ICLR.",pol,pol_positive,3.0,1,3
373,"One comment from the math side: it would be interesting (albeit probably difficult) to study kappa in (3.10) as a function of a. In particular at face value it looks like one only benefits from taking a larger, so why not study the limiting behavior of a->infty? What is the limiting value of kappa? Can you perform those calculations in the convex case at least?",pol,pol_neutral,2.0,2,2
374,GANs (generative adversarial network) represent a recently introduced min-max generative modelling scheme with several successful applications.,pol,none,0.0,0,0
375,"Unfortunately, GANs often show unstable behaviour during the training phase.",pol,none,0.0,0,0
376,"The authors of the submission propose a functional-gradient type entropy-promoting approach to tackle this problem, as estimating entropy is computationally difficult.",pol,none,0.0,0,0
377,"While the idea of the submission might be useful in some applications, the work is rather vaguely written, it is in draft phase:",pol,pol_negative,1.0,1,1
378,"1. Abbreviations, notations are not defined: GAN, WGAN-GP, DNN, FID (the complete name only shows up in Section 4), softplus, sigmoid, D_{\theta_{old}}, ...",pol,pol_negative,1.0,1,1
379,"2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.",pol,pol_negative,1.0,1,1
380,"3. Estimating entropies is a standard practice in statistics and machine learning, with an arsenal of estimators; the motivation of the submission is questionable.",pol,pol_negative,1.0,1,1
381,"4. Differentiation w.r.t. functions (or more generally elements in normed spaces) is a well-defined concept in mathematics, including the notions of Gateaux, Frechet and Hadamard differentiability.",pol,none,0.0,0,0
382,"It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.",pol,pol_negative,1.0,1,1
383,"While the idea of the work might be useful in practice, the current submission requires significant revision and work before publication.",pol,pol_negative,1.0,1,1
384,---,pol,none,0.0,0,0
385,After paper revisions:,pol,none,0.0,0,0
386,Thank you for the updates.,pol,none,0.0,0,0
387,The submission definitely improved. I have changed my score to '6: Marginally above acceptance threshold'; the suggested regularization can be a useful heuristic for the GAN community.,pol,none,0.0,3,0
388,This paper proposes an interesting extension to the Neural LP framework for learning numerical rules in knowledge graphs.,pol,none,0.0,0,0
389,The proposed method can handle predicates involving the comparison of the numerical attribute values.,pol,none,0.0,0,0
390,The authors demonstrate its effectiveness on both synthetic knowledge graphs and the parts of existing knowledge graphs which consider numerical values.,pol,none,0.0,0,0
391,I recommend the paper to be rejected in its current form for the following 3 reasons:,pol,none,0.0,0,0
392,"(1) Although the idea of making numerical rules differentiable is interesting, the current proposed method can only deal with one form of numerical predicate, which is numerical comparison.",pol,none,0.0,1,0
393,The limitation to such a special case makes the paper somewhat incremental.,pol,pol_negative,1.0,1,1
394,"(2) The paper does not do a great job of convincing the reader that the problem it is trying to solve is an important matter, or the proposed method is indeed effective in some applications.",pol,pol_negative,1.0,1,1
395,"Although the proposed method does a good job in synthetic experiments, outperforming existing methods by a large margin, its performance on the numerical variants of Freebase/DBPedia dataset does not show consistent significant improvement.",pol,pol_negative,1.0,1,1
396,The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.,pol,pol_negative,1.0,2,1
397,(3) The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks.,pol,pol_negative,1.0,1,1
398,A good place to start with is to visualize(print out) the learned numerical rules and see if they make any sense.,pol,pol_negative,1.0,1,1
399,"The experiment section needs significant improvement, especially when there is space left.",pol,pol_negative,1.0,1,1
400,The authors can consider improving the paper based on the above drawbacks.,pol,none,0.0,1,0
401,I encourage the authors to re-submit the paper once it's improved.,pol,none,0.0,2,0
402,"This article presents experiments on medium- and large-scale language modeling when the ideas of adaptive softmax (Grave et al., 2017) are extended to input representations.",pol,none,0.0,0,0
403,"The article is well written and I find the contribution simple, but interesting.",pol,pol_positive,3.0,3,3
404,It is a reasonable and well supported increment from adaptive softmax of Grave et al. (2017).,pol,pol_positive,3.0,3,3
405,My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space.,pol,pol_negative,1.0,1,1
406,"I understand that for two matrices A and B we have rank(AB) <= min(rank(A), rank(B)), and we are not making the small-sized embeddings richer when backprojecting to R^d, but",pol,pol_neutral,2.0,0,2
407,have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?,pol,pol_negative,1.0,2,1
408,References,pol,none,0.0,0,0
409,"Joulin, A., Cissé, M., Grangier, D. and Jégou, H., 2017, July. Efficient softmax approximation for GPUs. In International Conference on Machine Learning (pp. 1302-1310).",pol,none,0.0,0,0
410,Summary,pol,none,0.0,0,0
411,The authors introduce kaleidoscope matrices (K-matrices) and propose to use them as a substitute for structured matrices arising in ML applications (e.g. circulant matrix used for the convolution operation).,pol,none,0.0,0,0
412,The authors prove that K-matrices are expressive enough to capture any structured matrix with near-optimal space and matvec time complexity.,pol,none,0.0,0,0
413,"The authors demonstrate that learnable K-matrices achieve similar metrics compared to hand-crafted features on speech processing and computer vision tasks, can learn from permuted images, achieve performance close to a CNN trained on unpermuted images and demonstrate the improvement of inference speed of a transformer-based architecture for a machine translation task.",pol,none,0.0,0,0
414,Review,pol,none,0.0,0,0
415,The overall quality of the paper is high.,pol,pol_positive,3.0,0,3
416,The main contribution of the paper is the introduction of a family of matrices called kaleidoscope matrices (or K-matrices) which can be represented as a product of block-diagonal matrices of a special structure.,pol,pol_positive,3.0,0,3
417,"Because of the special structure, the family allows near-optimal time matvec operations with near-optimal space complexity for structured matrices which are commonly used in deep architectures.",pol,none,0.0,0,0
418,The proposed approach is novel.,pol,pol_positive,3.0,3,3
419,It gives a new characterization of sparse matrices with optimal space complexity up to a logarithmic term.,pol,none,0.0,0,0
420,"Moreover, the proposed characterization is able to learn any structured matrix and matvec time complexity of the K-matrix representation is near-optimal matvec time complexity of the structured matrix.",pol,pol_positive,3.0,0,3
421,"Even though in the worst-case complexity is not optimal, the authors argue that for matrices that are commonly used in machine learning architectures (e.g. circulant matrix in a convolution layer) the characterization is optimal.",pol,pol_positive,3.0,0,3
422,This results in a new differentiable layer based on a K-matrix that can be trained with the rest of an architecture using standard stochastic gradient methods.,pol,pol_positive,3.0,0,3
423,"However, it is worth noting that the reviewer is not an expert in the field, and it is hard for him to compare the proposed approach with previous work.",pol,none,0.0,1,0
424,The paper is generally easy to follow.,pol,pol_positive,3.0,3,3
425,"Even though the introduction of K-matrices requires a lot of definitions, they are presented clearly and Figure 1 helps to understand the concept of K-matrices.",pol,pol_positive,3.0,3,3
426,The experimental pipeline is also clear.,pol,pol_positive,3.0,3,3
427,"Given the special structure of the family, the reviewer might guess that having K-matrices can slow down the training, i.e. it might require more epochs to achieve the reported results compared to baselines.",pol,pol_neutral,2.0,1,2
428,Providing training plots might increase the quality of the paper.,pol,pol_negative,1.0,2,1
429,The experimental results are convincing.,pol,pol_positive,3.0,3,3
430,"First, the authors show that K-matrices can be used instead of a handcrafted MFSC featurization in an LSTM-based architecture on the TIMIT speech recognition benchmark with only a 0.4% loss of phoneme error rate.",pol,none,0.0,0,0
431,"Then, the authors evaluate K-matrices on ImageNet dataset.",pol,none,0.0,0,0
432,"In order to do so, they compare a lightweight ShuffleNet architecture which uses a handcrafted permutation layer to the same architecture but with a learnable K-matrix instead of the permutation layer.",pol,none,0.0,0,0
433,The authors demonstrate the 5% improvement of accuracy over the ShuffleNet with 0.46M parameters with only 0.05M additional parameters of the K-matrix and the 1.2% improvement of accuracy over the ShuffleNet with 2.5M parameters with only 0.2M additional parameters of the K-matrix.,pol,none,0.0,0,0
434,"Next, the authors show that K-matrices can be used to train permutations in image classification domains.",pol,none,0.0,0,0
435,"In order to demonstrate so, they take the Permuted CIFAR-10 dataset and ResNet-18 architecture, insert a trainable K-matrix at the beginning of the architecture and compare against ResNet-18 with an inserted FC-layer (attempting to learn the permutation as well) and ResNet-18 trained on the original, unpermuted CIFAR-10 dataset.",pol,none,0.0,0,0
436,"With K-matrix, the authors achieve a 7.9% accuracy improvement over FC+ResNet-18 and only a 2.4% accuracy drop compared to ResNet-18 trained on the original CIFAR-10.",pol,none,0.0,0,0
437,"Finally, the authors demonstrate that K-matrices can be used instead of the decoder’s linear layers in a Transformer-based architecture on the IWSLT-14 German-English translation benchmark which allows obtaining 30% speedup of the inference using a model with 25% fewer parameters with 1.0 drop of BLEU score.",pol,none,0.0,0,0
438,"Overall, the analysis and the empirical evaluations suggest that K-matrices can be a practical tool in modern deep architectures with a variety of potential benefits and tradeoffs between a number of parameters, inference speed and accuracy, and ability to learn complex structures (e.g. permutations).",pol,pol_positive,3.0,0,3
439,Improvements,pol,none,0.0,0,0
440,"1. Even though K-matrices are aimed at structured matrices, it would be curious either to empirically compare K-matrices to linear transformations in fully-connected networks (i.e. dense matrices) or to provide some theoretical analysis.",pol,pol_negative,1.0,1,1
441,"2. Section 3.3 argues that K-matrices allow to obtain an improvement of inference speed, however, providing the results of convergence speed (e.g. training plots with a number of epochs) will allow a better understanding of the proposed approach and will improve the quality of the paper.",pol,pol_negative,1.0,0,1
442,The paper proposes a method to disentangle latent variables for certain factors of interest in an image by considering the original input image and a transformation of the image where information about the factors of interest is removed.,pol,none,0.0,0,0
443,The generative process is then modeled by having two latent variables --  the first responsible for generating the transformed image whereas both latent variables are responsible for generating the original input image.,pol,none,0.0,0,0
444,"This inductive bias naturally enforces that the second latent variable will not model the information which the first needs to reconstruct the transformed image, due to the VAE objective penalizing redundancy in information present in the latents.",pol,none,0.0,0,0
445,"The paper demonstrates this in one setting where the transformation is random shuffling of image patches, which should remove the global information of the original input image.",pol,none,0.0,0,0
446,The methodology of the paper was concise and easy to follow.,pol,pol_positive,3.0,3,3
447,The simple inductive bias presented in the paper for disentangling local and global information is very interesting.,pol,pol_positive,3.0,3,3
448,"It is not obvious that shuffling image patches at a particular scale would lead to complete loss of global information, but the paper does show results on SVHN and CIFAR10 for which global information is sufficiently disentangled.",pol,pol_negative,1.0,0,1
449,The results for digit identity clustering were great for showing the correlation between their learnt global information and label information.,pol,pol_positive,3.0,3,3
450,"The paper introduced their model as a general purpose strategy for placing desired information in latent variables using auxiliary tasks, but focus was directed to the global vs local line of analysis.",pol,none,0.0,0,0
451,"While giving examples for what kind of information can be removed, the authors mentioned that color to gray-scale might be one possibility.",pol,none,0.0,0,0
452,It would have been interesting to see this and other possibilities explored in the paper.,pol,pol_neutral,2.0,2,2
453,I feel that the idea deserves a broader analysis beyond just a single choice of disentanglement.,pol,pol_negative,1.0,0,1
454,It is mentioned in the paper that having a single inference network for the posterior as opposed to the factorized one is conceivable.,pol,none,0.0,0,0
455,I would be curious to see an analysis of how that works out as compared to the separate encoders case.,pol,pol_neutral,2.0,2,2
456,"Overall, the paper has a novel idea which is well motivated and executed in terms of experiments.",pol,pol_positive,3.0,3,3
457,"The paper proposes an end-to-end joint model for named entity recognition (NER) and relation extraction (RE), using pre-trained language models.",pol,none,0.0,0,0
458,"The model is very simple, with the key is to use BERT and take NER output as input to RE.",pol,none,0.0,0,0
459,"The experimental results show the model, without the need for handcrafted features, get state-of-the-art results on five datasets.",pol,none,0.0,0,0
460,"Although the paper is well written and shows good results, I would reject the paper because:",pol,none,0.0,3,0
461,- the idea is trivial and simple. I don't think there's significant novelty here: all the components are existing and combining them seems very trivial to me.,pol,pol_negative,1.0,1,1
462,- the good performance seems to be from BERT rather than the model's structure (table 2 suggests that). I thus think the contribution of the paper is pretty not significant.,pol,pol_negative,1.0,1,1
463,I think the paper does not fit this conference. It is better to be presented in a Demonstration section at a *ACL conference.,pol,pol_negative,1.0,1,1
464,This paper has problems with clarity/polish and experimental design that are sufficiently severe,pol,pol_negative,1.0,1,1
465,to merit rejection by themselves.,pol,none,0.0,0,0
466,Regarding clarity/polish:,pol,none,0.0,0,0
467,"I am generally not super picky about these things, but there does have to be some standard.",pol,pol_negative,1.0,1,1
468,"This paper looks very hastily put together, especially pages 7 and 8.",pol,pol_negative,1.0,1,1
469,There are many typos and unclear statements.,pol,pol_negative,1.0,1,1
470,Just a few examples:,pol,none,0.0,0,0
471,> Generative Adversarial Networks (GANs) are powerful framework for (in the abstract),pol,none,0.0,0,0
472,> be a good metric to evolution the difference (in the abstract),pol,none,0.0,2,0
473,"> In the past few years, Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are impactful because it has shown lots of great results for many AI tasks, (first sentence)",pol,none,0.0,0,0
474,> It means that there is no an unanimous metric to represent the difference between the true data distribution and the generated distribution,pol,none,0.0,1,0
475,What does this mean? People have mostly settled on using FID for this.,pol,pol_negative,1.0,1,1
476,"> It is also difficult to know whether the generated distribution is close to the true distribution, and this is often observed by human eyes.",pol,none,0.0,1,0
477,Isn't this just restating the point made in the first sentence?,pol,pol_negative,1.0,2,1
478,"Regardless, nobody really uses human evaluation anymore - so this is just not correct.",pol,pol_negative,1.0,1,1
479,>,pol,none,0.0,0,0
480,"It means that if the original generator and discriminator are random, it is difficult to confirm that the generator and discriminator can converge to the ideal conclusion by training with given data.",pol,none,0.0,1,0
481,"But this paper doesn't propose a way to solve that problem, so it's strange to mention this here in this way.",pol,pol_negative,1.0,1,1
482,These issues would maybe be excusable if not for the totally inadequate experimental validation.,pol,pol_negative,1.0,1,1
483,A non-exhaustive list of methodological problems with the (single) experiment:,pol,none,0.0,0,0
484,"1. The experiment uses a single run each of the baseline and DG-GAN, when it's well-known that GAN training runs",pol,pol_negative,1.0,1,1
485,have inter-run variance larger than the difference in score reported in Fig 1 and 2.,pol,none,0.0,0,0
486,2. The models have not been trained for long enough.,pol,pol_negative,1.0,1,1
487,"3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which",pol,pol_negative,1.0,1,1
488,probably leads to:,pol,none,0.0,0,0
489,"4. The scores achieved by the baseline are very far from state of the art, making the comparison mostly useless,",pol,pol_negative,1.0,1,1
490,"and rendering the third claim from the introduction (""We propose an new algorithm with the new metric which demonstrates better results than state-of-the-art algorithms."") completely untrue.",pol,pol_negative,1.0,1,1
491,"In light of these other issues, I haven't checked the proofs.",pol,none,0.0,1,0
492,This paper seems to be an exposition on the primary performance affecting aspects of generative adversarial networks (GANs).,pol,none,0.0,0,0
493,"This can possibly affect our understanding of GANs, helping practitioners get the most in their applications, and perhaps leading to innovations that positively affect GAN performance.",pol,none,0.0,0,0
494,"Normally, expositions such as this I find difficult to recommend for publication.",pol,none,0.0,1,0
495,"In these times, one can find ""best practices"" with a reasonable amount of rigor on data science blogs and such.",pol,none,0.0,0,0
496,"An exposition that I would recommend for publication, would need to exhibit a high sense of depth and rigor for me to deem it publication worthy.",pol,none,0.0,1,0
497,"This paper, for me, achieves this level of quality.",pol,pol_positive,3.0,3,3
498,"The authors start off by giving a precise, constrained list of hyperparameters and architectural components that they would explore.",pol,none,0.0,0,0
499,This is listed in the title and explained in detail in the beginning of the paper.,pol,none,0.0,0,0
500,The authors are right in explaining that they could not cover all hyperparameters and chose what I feel are quite salient ones.,pol,none,0.0,3,0
501,My one ask would have been a survey of how activations might affect performance.,pol,pol_neutral,2.0,1,2
502,"I sense that everyone has settled upon LeakyReLUs for internal layers, but a survey of that work and experimentation within the authors' framework would have been nice.",pol,pol_neutral,2.0,2,2
503,The authors then explain the metrics for evaluation and datasets.,pol,none,0.0,0,0
504,The datasets offered a healthy variety for typical image recognition tasks.,pol,none,0.0,3,0
505,It would be interesting to see what these metrics would reveal when applied to other types of data (e.g. scientific images).,pol,pol_neutral,2.0,2,2
506,"The  authors explain, with graphs, the results of the loss, normalization, and architectures.",pol,none,0.0,0,0
507,"I feel the discussion on loss was rushed, and I gained no insight on what the authors thought was a prominent difference between the three losses studied.",pol,pol_negative,1.0,1,1
508,"Perhaps the authors had no salient observations for loss, but explicitly stating such would be useful to the reader.",pol,pol_negative,1.0,1,1
509,The only observation I gained as far as this is that non-saturating loss would possibly be stable across various datasets.,pol,none,0.0,1,0
510,"Regularization and normalization are discussed in much more detail, and I think the authors made helpful and interesting observations, such as the benefits of spectral normalization and the fact that batch normalization in the discriminator might be a harmful thing.",pol,pol_positive,3.0,3,3
511,These are good takeaways that could be useful to a vast number of GANs researchers.,pol,pol_positive,3.0,3,3
512,"For architectures to be a main pillar of the paper, I feel that this area could have been explored in greater detail.",pol,pol_negative,1.0,1,1
513,"I feel that this discussion devolved into a discussion, again, about normalization rather than the architectural differences in performance.",pol,pol_negative,1.0,0,1
514,"Unless I am misunderstanding something, it seems that the authors simply tested one more architecture, for the express purpose of testing whether their observations about normalization would hold.",pol,pol_negative,1.0,1,1
515,"As a bonus, the authors bring up some problems they had in making comparisons and reproducing results.",pol,none,0.0,0,0
516,"I think this is an extremely important discussion to have, and I am glad that the authors detailed the obstacles in their journey. Hopefully this will inspire other researchers to avoid adding to the complications in this field.",pol,pol_positive,3.0,3,3
517,The graphs were difficult to parse.,pol,pol_negative,1.0,1,1
518,"I was able to make them out, but perhaps separating the top row (FID and diversity graphs) into separate figures, separate lines, or something would have reduced some confusion.",pol,pol_negative,1.0,1,1
519,"In addition, different charts presenting only one loss function, with their spectral normalization and gradient penalty variants, would have made the effects of the normalization more obvious on the FID distribution graphs.",pol,pol_negative,1.0,1,1
520,"If this can be changed before publication, I would strongly suggest it.",pol,pol_negative,1.0,2,1
521,I appreciate that the authors provided source code via GitHub.,pol,pol_positive,3.0,3,3
522,"However, in the future, the authors should be careful to provide an anonymous repository for review purposes.",pol,none,0.0,1,0
523,"I had to be careful not to allow myself to focus on the author names which are prominent in the repository readme, and one of whom has his/her name in the GitHub URL itself.",pol,none,0.0,1,0
524,I didn't immediately recognize the names and thus it was easy for me not to retain them or focus on them.,pol,none,0.0,1,0
525,"However, if it had been otherwise, it might have risked biasing the review.",pol,none,0.0,1,0
526,"In all, I think this is a good and useful paper from which I have learned and to which I will refer in the future as I continue my research into GANs and VAEs.",pol,pol_positive,3.0,3,3
527,I would suggest changing the title to be more appropriate and accurate (the researchers are primarily focused on showing the positive and negative effects of normalization across various loss functions and architectures).,pol,pol_neutral,2.0,2,2
528,"But altogether, I believe this is a paper worth publishing at ICLR.",pol,pol_positive,3.0,0,3
529,This paper proposes a novel view for stabilising GANs from the perspective of control theory.,pol,none,0.0,0,0
530,This view provides new insights into GAN training and may inspire future research along this direction.,pol,none,0.0,0,0
531,"This paper is overall well written, with a smooth introduction of background material that might be less familiar for machine learning researchers.",pol,pol_positive,3.0,3,3
532,"There are places that need further clarification, but I think the proposed direction is promising.",pol,pol_positive,3.0,3,3
533,Questions about the method:,pol,none,0.0,0,0
534,"- Since the proposed method starts from Laplace transform, it would be helpful to further discuss the connection between other methods that regularises the eigenvalues of the Jacobian (such as spectral-normalisation), which work in the frequency domain from a different perspective.",pol,pol_negative,1.0,2,1
535,"For example, could the proposed regulariser be interpreted as imposing certain constraint on the spectrum of Jacobian?",pol,none,0.0,2,0
536,- Does section 2.2 depend on the assumption of linear dynamics?,pol,pol_neutral,2.0,2,2
537,- Does the E in eq.7 come from eq. 4?,pol,pol_neutral,2.0,2,2
538,"- Could you give some intuition for the paragraph above section 3.4, about the different form of inputs when treating D and G as dynamics?",pol,pol_neutral,2.0,2,2
539,"For consistency, it is perhaps better to keep the dependency of p_D and p_G on x explicit (same for eq. 10), unless this is intended?",pol,pol_neutral,2.0,1,2
540,"- My main concern about the analysis is that it shows why several methods (e.g., momentum, multiple update steps) are *not* helpful for stabilising GANs, but does not tell why training with these methods, as well as others such as gradient penalty, *do converge* in practice with properly chosen hyper-parameters?",pol,pol_negative,1.0,1,1
541,Experiments:,pol,none,0.0,0,0
542,- About setup: the paper reports using ResNets for natural images as in Mescheder et al. (2018).,pol,pol_negative,1.0,1,1
543,"However, Mescheder et al. (2018) uses DCGAN for CIFAR10, which raises further questions about the scores on this dataset:",pol,none,0.0,0,0
544,"- The baseline scores of Reg-SGAN and Reg-WGAN seem to be worse than those reported in Mescheder et al. (2018), which have inception scores above 6 according to Figure 6 of their paper.",pol,pol_negative,1.0,1,1
545,"In Figure 5 of this paper, they are clearly below 6.",pol,none,0.0,1,0
546,What’s the reason for this discrepancy?,pol,pol_neutral,2.0,2,2
547,"In this paper, the authors claim that they are able to update the generator better to avoid generator mode collapse and also increase the stability of GANs training by indirectly increasing the entropy of the generator until it matches the entropy of the original data distribution using functional gradient methods.",pol,none,0.0,0,0
548,The paper is interesting and well written.,pol,pol_positive,3.0,3,3
549,"However, there is a lot of work coming out in the field of GANs currently, so I am not able to comment on the novelty of this regularization approach, and I am interested to know how this method performs when compared to other techniques to avoid mode collapse such as feature matching and mini-batch discrimination, etc.",pol,pol_neutral,2.0,1,2
550,"The paper deals with further development of RAND-WALK model of Arora et al. There are stable idioms, adjective-noun pairs and etc that are not covered by RAND-WALK, because sometimes words from seemingly different contexts can join to form a stable idiom.",pol,none,0.0,0,0
551,"So, the idea of paper is to introduce a tensor T and a stable idiom (a,b) is embedded into v_{ab}=v_a+v_b+T(v_a, v_b,.) and is emitted with some probability p_sym (proportional to exp(v_{ab} times context)).",pol,none,0.0,0,0
552,"The latter model is similar to RAND-WALK, so it is not surprising that statistical functions there are similarly concentrated.",pol,pol_negative,1.0,1,1
553,"Finally, there exists an expression, PMI3(u,v,w), that shows the correlation between 3 words, and that can be estimated from the data directly.",pol,none,0.0,0,0
554,"It is proved that Tucker decomposition of that tensor gives us all words embeddings together with tensor T. Thus, from the latter we will obtain a tool for finding embeddings of idioms (i.e. v_a+v_b+T(v_a, v_b,.)).",pol,none,0.0,0,0
555,"Theoretical analysis seems correct (I have not checked all the statements thoroughly, but I would expect formulations to be true).",pol,pol_positive,3.0,3,3
556,The only problem I see is that phrase similarity part is not convincing.,pol,pol_negative,1.0,1,1
557,"I cannot understand from that part whether T(v_a, v_b,.) addition to v_a+v_b gives any improvement or not.",pol,pol_negative,1.0,1,1
558,Summary: The paper studies the problem of training deep neural networks in the distributes setting while ensuring privacy.,pol,none,0.0,0,0
559,"Each data sample is held by one individual (e.g., on a cell phone), and a central algorithm trains a learning model on top of this data.",pol,none,0.0,0,0
560,"In order to protect the privacy of the individuals, the paper proposes the use of multi-layer encoders (E) over the raw data, and then send them across the server.",pol,none,0.0,0,0
561,"The privacy is ensured by exemplifying the inability to reconstruct the original data from the encoded features, via running a reverse deep model (X).",pol,none,0.0,0,0
562,"The notion of privacy is quantified by the Euclidian distance between the reconstructed vector via the best X and the original feature vector, maximized over E. The overall framework resembles a GAN, and the paper calls it RAN (Reconstructive Adversarial Network).",pol,none,0.0,0,0
563,Positive aspects: The problem of training privacy preserving deep models over distributed data has been a significant and important challenge.,pol,pol_positive,3.0,3,3
564,"The current solutions that adhere to differential privacy based approaches are not yet practical. In my view, it is a very important research question.",pol,pol_positive,3.0,1,3
565,Negative aspects: One major concern I have with the paper is the notion of privacy considered.,pol,pol_negative,1.0,1,1
566,The notion of privacy considered in the paper makes two assumptions which I am not comfortable with: i) The protection that the notion assures is against reconstruction attacks.,pol,pol_negative,1.0,1,1
567,"There has been a large body of work which shows that weaker attacks like membership attacks can be equally damaging, ii) Privacy is a worst-case guarantee.",pol,pol_negative,1.0,0,1
568,"I do not see the GAN style approach taken by the paper, ensures this.",pol,pol_negative,1.0,1,1
569,The aim of this paper is to solve SAT instances using a CNN architecture.,pol,none,0.0,0,0
570,SAT instances are represented using an efficient encoding of boolean matrices.,pol,none,0.0,0,0
571,"The overall idea is to decompose an input SAT instance into simpler ones, and to train the neural model on simpler instances using an existing solver for labeling these instances.",pol,none,0.0,0,0
572,"Based on satisfaction probabilities induced from simpler formulas, the architecture predicts a partial assignment which is fed to the existing solver for deriving the satisfiability result.",pol,none,0.0,0,0
573,"Arguably, the topic of “learning to solve SAT instances” is very interesting, by coupling results from neural networks and SAT solvers.",pol,pol_positive,3.0,3,3
574,"This work is inspired from the landmark paper on NeuroSAT, and the experimental results look promising.",pol,pol_positive,3.0,3,3
575,"However, since the framework is focused on solving random SAT problems (especially random 3-SAT instances), the paper is missing a detailed description of this active research topic in AI and the SAT community (see e.g. [1,2]).",pol,pol_negative,1.0,1,1
576,"Notably, the problem of generating realistic random k-SAT instances has long been considered as one of the most important challenges in SAT research [3].",pol,none,0.0,0,0
577,"Importantly, modern random k-SAT instances are not only characterized by their number of variables, and their ratio  #clauses / #variables, but with an additional “structure” which mimics real-world, industrial instances (see e.g. [4]).",pol,none,0.0,0,0
578,"Furthermore, I had some trouble understanding how a SAT instance is solved using algorithm 1.",pol,pol_negative,1.0,1,1
579,Specifically the text in Section 3.3 that explains Algorithm 1 is a bit confusing.,pol,pol_negative,1.0,1,1
580,How do “we choose a specific number of assignments based on prediction probabilities”?,pol,pol_negative,1.0,1,1
581,"Unless I missed something, the output of the CNN architecture is a probability value that the input formula is SAT, so I don’t really see how this can be related to prediction probabilities of assignments.",pol,pol_negative,1.0,1,1
582,"This should be explained in detail since Line 15 is the main output of the algorithm, which is fed (Line 16) to an existing solver for completing the assignment.",pol,pol_negative,1.0,1,1
583,"The example at the end of section 3.3 is not very helpful: namely, the CNF formula $(x_2) \land (\neg x_2)$ is clearly unsatisfiable, so how can the model predict that it is satisfiable with 80% probability? And, if we try here $x_2 = 1$, we immediately get $\bot$ (the unsat CNF), but not $x_1$ (which was already assigned to $0$).",pol,pol_negative,1.0,1,1
584,"Finally, the CNN architecture should be compared with modern SAT solvers which have been participating to SAT competitions.",pol,pol_negative,1.0,2,1
585,"The Z3 solver is mainly focused on solving SMT instances [5], not random k-SAT instances which, by the way, is a common track in annual SAT competitions (see e.g. [6]).",pol,none,0.0,0,0
586,"To this point, generic SAT solvers such as MiniSAT [7] and Glucose [8] are able to solve in few seconds some random 3-SAT instances with thousands of variables and tens of thousands of clauses (see e.g. [4]).",pol,none,0.0,0,0
587,"So, the motivating assertion “[...] state-of-the-art solvers do not yet scale to large, difficult formulas, such as ones with hundreds of variables and thousands of clauses” in the introduction of the paper, is not totally correct.",pol,pol_negative,1.0,1,1
588,"To sum up, I would recommend to compare the CNNSAT architecture with well-known SAT solvers such as MinSAT, Glucose, March, or Dimetheus [9] which has been one of the strongest solvers in recent years for tackling random instances.",pol,pol_neutral,2.0,2,2
589,"Also, as mentioned above, it would be interesting to incorporate some structures (such as, for example, community attachments or popularity-similarities) in SAT instances, in order to estimate whether CNNSAT could handle pseudo-industrial problems.",pol,pol_negative,1.0,2,1
590,"[1] D. Mitchell, B. Selman, H. Levesque, Hard and easy distributions of SAT problems, in: Proceedings of the 10th National Conference on Artificial Intelligence, AAAI’92, 1992, pp. 459–465.",pol,none,0.0,0,0
591,"[2] Nudelman, E., Leyton-Brown, K., Hoos, H. H., Devkar, A., & Shoham, Y. Understanding random SAT: Beyond the clauses-to-variables ratio. In 10th International Conference on Principles and Practice of Constraint Programming (CP’04), pp. 438–452.",pol,none,0.0,0,0
592,"[3] B. Selman, H.A. Kautz, D.A. McAllester, Ten challenges in propositional reasoning and search, in: Proceedings of the 15th International Joint Conference on Artificial Intelligence, IJCAI’97, 1997, pp. 50–54.",pol,none,0.0,0,0
593,"[4] J. Giráldez-Cru and J. Levy. Generating sat instances with community structure. Artificial Intelligence, 238:119 – 134, 2016.",pol,none,0.0,0,0
594,[5] The 2014 SMT Competition https://satassociation.org/jsat/index.php/jsat/article/download/122/114,pol,none,0.0,0,0
595,[6] The 2018 SAT Competition,pol,none,0.0,0,0
596,http://sat2018.forsyte.tuwien.ac.at/index.php?cat,pol,none,0.0,0,0
597,=results,pol,none,0.0,0,0
598,"[7] N. Eén, N. Sörensson, An extensible SAT-solver, in: Proceedings of the 6th International Conference on Theory and Applications of Satisfiability Testing, SAT’03, 2003, pp. 502–518.",pol,none,0.0,0,0
599,"[8] ] G. Audemard, L. Simon, Predicting learnt clauses quality in modern SAT solvers, in: Proceedings of the 21st International Joint Conference on Artificial Intelligence, IJCAI’09, 2009, pp. 399–404",pol,none,0.0,0,0
600,[9] Dimetheus,pol,none,0.0,0,0
601,https://www.gableske.net/dimetheus,pol,none,0.0,0,0
602,This paper introduces a new approach to solve optimization problems without relying on any human-provided data beyond the specification of the optimization problem itself.,pol,pol_positive,3.0,0,3
603,The approach is inspired by the two-player zero-sum game paradigm and follow a generative adversarial network (GAN) setting.,pol,none,0.0,0,0
604,"One network is trained to output the optimal behavior for a given problem, while the other is trained to output difficult instances of the given problem.",pol,none,0.0,0,0
605,These two networks are trained simultaneously and compete against the other until some equilibrium is achieved.,pol,none,0.0,0,0
606,This approach is tested on two small problems for which the optimal behavior is known and seems to perform near theoretical optimality.,pol,none,0.0,0,0
607,"I weakly reject this paper because although the approach is indeed interesting, the paper is lacking some structure, as described below:",pol,pol_negative,1.0,1,1
608,- The paper clearly mentions that no optimization of the training setup or the hyperparameters has been done because the authors are not interested in extending ML techniques.,pol,none,0.0,1,0
609,"However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings.",pol,pol_negative,1.0,0,1
610,It is thus unclear if the approach is robust against different hyperparameter settings.,pol,pol_negative,1.0,1,1
611,"- Very little details (apart from the optimization algorithm) are given regarding the architecture used (types of input, output, neural units, activation functions, number of hidden layers, loss function, etc...), which makes it very hard to reproduce this approach.",pol,pol_negative,1.0,1,1
612,- Section 1.1 presents results with too many details without introducing the problem.,pol,pol_negative,1.0,1,1
613,"I would suggest the authors to either introduce the two problems earlier or to simply say that near-optimal results are achieved, without giving detailed results, because it is very hard to understand them without any introduction of the task being achieved.",pol,pol_negative,1.0,1,1
614,"- One task is presented in Section 2 ""Preliminaries"" while the other task is presented in Section 4 ""AdWords"".",pol,none,0.0,0,0
615,It is hard to follow the flow of ideas present in the paper when similar things are not together.,pol,pol_negative,1.0,1,1
616,I would suggest restructuring the paper into a more classical structure such as: <intro without detailed results - previous work & problematic - approach taken with more details for reproducibility - description of the two tasks - description of experiments with more details for reproducibility - results - conclusion>.,pol,pol_negative,1.0,2,1
617,- The paper mentions the MSVV algorithm twice but no reference or explanation is provided.,pol,pol_negative,1.0,1,1
618,It is very hard to understand sentences referring to this.,pol,pol_negative,1.0,1,1
619,"- This work only considers problems for which the optimal input distribution is known, but is motivated by the fact that it could be applied to problems for which the optimal distribution is unknown and thus being able to discover new algorithms.",pol,none,0.0,1,0
620,It is hard to support this motivation when no experiments are done in its favor.,pol,pol_negative,1.0,1,1
621,- No comparison has been made between their approach and other previous approaches.,pol,pol_negative,1.0,1,1
622,We only know that the proposed approach finds near-optimal solutions with a difference of 0.01 competitive ratio.,pol,none,0.0,1,0
623,It is thus very hard to know if this new approach brings any improvement to previous work.,pol,pol_negative,1.0,1,1
624,"Below are a few things that were not considered to make a decision, but are only details that would make the paper slightly better:",pol,none,0.0,0,0
625,"- typo at the beginning of section 3.1: missing 'be' in  ""This can either *be* by an ...""",pol,pol_negative,1.0,1,1
626,"- typo at the beginning os section 4:  missing 'be' in ""... the algorithm must irrevocably *be* allocated to ...""",pol,pol_negative,1.0,1,1
627,- Axis' names to the different plots in the Figures would help understand them better.,pol,pol_neutral,2.0,2,2
628,"Also, the description of some figures could benefit more details that could be taken off from the text.",pol,pol_neutral,2.0,1,2
629,"Overview: This work is an interesting work to understand the generalization capabilities of a two layered neural network in a high dimensional setting (samples, features and neurons tend to infinity).",pol,none,0.0,0,0
630,"It studies the conditions under which the ""double descent phenomenon"" may be observed.",pol,none,0.0,0,0
631,Summary: The work shows,pol,none,0.0,0,0
632,that in two layered neural networks with non-linearity,pol,none,0.0,0,0
633,1) the double descent phenomenon of the bias-variance decomposition may be observed when the second layer weights are optimized assuming that the first layer weights are constant.,pol,none,0.0,0,0
634,2) the bias-variance decomposition does not exhibit double descent when optimizing only the first layer with both vanishing and non-vanishing initialization of weights.,pol,none,0.0,0,0
635,"3) For vanishing initalization of weights for the first layer with non-linear activation , the gradient flow solution is asymptotically close to a two layered linear network.",pol,none,0.0,0,0
636,It is independent of overparametrization.,pol,none,0.0,0,0
637,"However, the condition for this is smooth activation and the result does not hold for ReLU activation.",pol,none,0.0,0,0
638,"4) For non-vanishing initilization of the weights for the first layer with non-linear activation, the gradient flow solution is well approximated by a kernel model.",pol,none,0.0,0,0
639,"However, the risk is independent of overparametrization.",pol,none,0.0,0,0
640,I believe this is an interesting work that needs to be accepted.,pol,pol_positive,3.0,1,3
641,"This paper addresses a novel variant of AutoML, to automatically learn and generate optimization schedules for iterative alternate optimization problems.",pol,none,0.0,0,0
642,"The problem is formulated as a RL problem, and comprehensive experiments on four various applications have demonstrated that the optimization schedule produced can guide the task model to achieve better quality of convergence, more sample-efficient, and the trained controller is transferable between datasets and models.",pol,none,0.0,0,0
643,"Overall, the writing is quite clear, the problem is interesting and important, and the results are promising.",pol,pol_positive,3.0,3,3
644,Some suggestions:,pol,none,0.0,0,0
645,"1. What are the key limitations of AutoLoss ? Did we observe some undesirable behavior of the learned optimization schedule, especially when transfer between different datasets or different models ?",pol,pol_negative,1.0,2,1
646,More discussions on these questions can be very helpful to further understand the proposed method.,pol,pol_negative,1.0,2,1
647,"2. As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility.",pol,pol_negative,1.0,2,1
648,3. Any plan for open source ?,pol,pol_neutral,2.0,2,2
649,This paper proposes to use codes and codebooks to compress the weights.,pol,none,0.0,0,0
650,The authors also try minimizing the layer reconstruction error instead of weight approximation error for better quantization results.,pol,none,0.0,0,0
651,Distillation loss is also used for fine-tuning the quantized weight.,pol,none,0.0,0,0
652,Empirical results on resnets show that the proposed method has a good compression ratio while maintaining competitive accuracy.,pol,pol_positive,3.0,3,3
653,This paper is overall easy to follow.,pol,pol_positive,3.0,3,3
654,My main concern comes from the novelty of this paper.,pol,pol_negative,1.0,1,1
655,The two main contributions of the paper:,pol,pol_negative,1.0,0,1
656,(1) using codes and codebooks to compress weights; and,pol,pol_negative,1.0,0,1
657,(2) minimizing layer reconstruction error instead of weight approximation error,pol,pol_negative,1.0,0,1
658,are both not new.,pol,pol_negative,1.0,1,1
659,"For instance, using codes and codebooks to compress the weights has already been used in [1,2].",pol,pol_negative,1.0,0,1
660,"A weighted k-means solver is also used in [2], though the ""weighted"" in [2] comes from second-order information instead of minimizing reconstruction error.",pol,pol_negative,1.0,0,1
661,"In addition, minimizing reconstruction error has already been used in low-rank approximation[3] and network pruning[4].",pol,pol_negative,1.0,0,1
662,"Clarification of the connections/differences, and comparison with these related methods should be made to show the efficacy of the proposed method.",pol,pol_negative,1.0,1,1
663,It is not clear how the compression ratio in table 1 is obtained.,pol,pol_negative,1.0,1,1
664,"Say for block size d=4, an index is required for each block, and the resulting compression ratio is at most 4 (correct me if I understand it wrong).",pol,pol_negative,1.0,0,1
665,Can the authors provide an example to explain how to compute the compression ratio?,pol,pol_negative,1.0,2,1
666,[1].,pol,none,0.0,0,0
667,"Model compression as constrained optimization, with application to neural nets.",pol,none,0.0,0,0
668,part ii: quantization.,pol,none,0.0,0,0
669,[2]. Towards the limit of network quantization.,pol,none,0.0,0,0
670,[3]. Efficient and Accurate Approximations of Nonlinear Convolutional Networks.,pol,none,0.0,0,0
671,[4]. ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression.,pol,none,0.0,0,0
672,"Summary: This paper introduces a new Neural Network training procedure, designed for tabular data, that seeks to leverage feature clusters extracted from GBDTs.",pol,none,0.0,0,0
673,Strengths: The idea of leveraging feature groups in a neural network structure; the novelty of the RESE model;,pol,pol_positive,3.0,3,3
674,"Weaknesses: The main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use GBDT.",pol,pol_negative,1.0,1,1
675,Minor typos:,pol,none,0.0,0,0
676,(abstract),pol,none,0.0,0,0
677,"- ""NN has achieved"" => ""Neural Networks have achieved""",pol,pol_neutral,2.0,2,2
678,"- ""performances"" => performance",pol,pol_neutral,2.0,2,2
679,"- ""explicitly leverages"" => ""explicitly leverage""",pol,pol_neutral,2.0,2,2
680,Questions:,pol,none,0.0,0,0
681,"- (top of p. 2) What exactly is the difference between ""implicit feature combinations"" and ""explicit (?), expressive feature combinations""",pol,pol_negative,1.0,2,1
682,"- (top of p. 2) ""encourage parameter sharing"" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]",pol,pol_negative,1.0,2,1
683,"- what is the benefit brought by the 'Structural Knowledge' transfer? is this quantified anywhere? based on the description, I don't understand if this is an add-on to TabNN or whether it is incorporated in TabNN.",pol,pol_negative,1.0,1,1
684,"Recommendations for the authors: Would it be possible to provide an analysis of the cases when TabNN is expected to outperform GBDT by a sizable margin? Or, if not, are there other reasons why using a neural network would make more sense than just simply running GBDT?",pol,pol_negative,1.0,2,1
685,This paper studied a random projection of word embeddings in neural language modeling.,pol,none,0.0,0,0
686,"Instead of having |V| x m embeddings, the author(s) represented a word with a random, sparse, linear combination {1, 0, -1} of k vector of size m.",pol,none,0.0,0,0
687,The experiment on PTB dataset showed that k had to be somewhat close to |V| in order to achieve the comparable perplexity to a feed-forward NLM.,pol,none,0.0,0,0
688,"Overall, I am not sure what we could gain from this research direction.",pol,pol_negative,1.0,1,1
689,"The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).",pol,pol_negative,1.0,0,1
690,"In addition, the fact that the random projections preserved the inner product (centered at zero) was probably not desirable.",pol,pol_negative,1.0,1,1
691,It might be more fruitful if these linear combinations were learned or sub-senses of words (e.g. [1]).,pol,pol_negative,1.0,1,1
692,The experiments were quite extensive on the hyper-parameters and showed how the models performed under different settings.,pol,pol_positive,3.0,3,3
693,"However, these were done using 1 dataset and also a simple feed-forward network (rather than LSTM).",pol,pol_negative,1.0,0,1
694,"I can understand the point that training NNLM accelerates the experiments, but the author(s) should consider trying a simply LSTM model after the best settings had been discovered (e.g. Table 1).",pol,pol_negative,1.0,1,1
695,PTB also has a very unnatural vocabulary distribution as pointed out in [2].,pol,none,0.0,1,0
696,"Thus, it might be helpful to test the result on another dataset (e.g. WikiText).",pol,pol_negative,1.0,2,1
697,Other comments,pol,none,0.0,0,0
698,1. I do not get the point of bringing up NCE. Did you actually use NCE loss? Did you only refer to NCE as a weight tying which can be used in a standard XENT loss [3]? The first paragraph of 3.3 did not help clarify this point either.,pol,pol_negative,1.0,1,1
699,"2. In Figure 3, the baseline got different perplexity between 3(a) and 3(b).",pol,pol_negative,1.0,0,1
700,"3. Shouldn't random indexing produce non-uniform numbers of non-zero entries depending on alpha? Why did you have an exact number of non-zero entries, s, in the experiments?",pol,pol_negative,1.0,2,1
701,3. Some typos,pol,none,0.0,1,0
702,"- ""... is that instead of trying to probability ..."" => ""... tying ...""",pol,pol_negative,1.0,2,1
703,"- ""... All models sare trained ..."" => ""... are",pol,pol_negative,1.0,2,1
704,"...""",pol,none,0.0,0,0
705,"- ""... Tho get the feature ..."" => ?",pol,pol_negative,1.0,2,1
706,References,pol,none,0.0,0,0
707,"[1] S. Arora et al., 2016. Linear Algebraic Structure of Word Senses, with Applications to Polysemy",pol,none,0.0,0,0
708,"[2] S. Merity et al., 2016. Pointer Sentinel Mixture Models",pol,none,0.0,0,0
709,"[3] Y. Gal et al., 2015. A Theoretically Grounded Application of Dropout in Recurrent Neural Networks",pol,none,0.0,0,0
710,This paper addresses the problem of building models for NLP tasks that are robust against spurious correlations in the data by introducing a human-in-the-loop method: annotators are asked to modify data-points minimally in order to change the label.,pol,none,0.0,0,0
711,They refer to this process as counterfactual augmentation.,pol,none,0.0,0,0
712,The authors apply this method to the IMDB sentiment dataset and to SNLI and show (among other things) that many models cannot generalize from the original dataset to the counterfactually-augmented one.,pol,pol_positive,3.0,0,3
713,This contribution is timely and addresses a very important problem that needs to be addressed in order to build more robust NLP systems.,pol,pol_positive,3.0,3,3
714,"Because, however, of a few limitations, I recommend weak acceptance.",pol,pol_neutral,2.0,1,2
715,My main hesitation comes from a lack of clarity about the main lesson we have learned.,pol,pol_negative,1.0,1,1
716,"In particular, if the goal is to use this method to augment the data we use to train NLP systems in order to make them more robust, it seems that the time cost of the process will be prohibitive.",pol,pol_negative,1.0,0,1
717,"On the other hand, perhaps these methods could be used to identify the kind of spurious correlations that models tend to rely on, which could then be used in a more automated data augmentation process.",pol,pol_positive,3.0,0,3
718,"If that's the goal, however, a more detailed error analysis would need to be included.",pol,pol_negative,1.0,1,1
719,A few small comments:,pol,none,0.0,0,0
720,"* There was some analysis of the augmented IMDB dataset, but none of the SNLI dataset.",pol,pol_negative,1.0,1,1
721,I would love to see a more detailed investigation of what annotators usually did.,pol,pol_neutral,2.0,2,2
722,"For instance, a reason that hypothesis-only models do well is that certain words are very predictive of certain labels (e.g. ""not"" and contradiction).",pol,none,0.0,0,0
723,"Do people leave the negations in when modifying such examples for entailment or neutrality, thus breaking the simple correspondence?",pol,pol_negative,1.0,2,1
724,"That's a very simple kind of question; more generally, I'd like to see more analysis of the new dataset.",pol,pol_negative,1.0,0,1
725,* The BiLSTM they use is very small (embedding and hidden dimension 50).,pol,pol_negative,1.0,1,1
726,"Given that BERT is most robust against their manipulation, it would be good to see a more powerful recurrent model for comparison.",pol,pol_negative,1.0,2,1
727,"It would be easy to use ELMo here, if the main question is about Transformers vs recurrent models.",pol,pol_negative,1.0,2,1
728,Some very minor / typographic comments:,pol,none,0.0,0,0
729,"* abstract: ""with revise"" should be ""with revising""",pol,pol_negative,1.0,2,1
730,* first paragraph page 2: some references to causality literature and definition of spuriousness as common cause,pol,pol_negative,1.0,1,1
731,"* page 2, ""We show that..."" I'd break this into two sentences to make it easier to parse.",pol,pol_negative,1.0,1,1
732,* Table 3: I would make two columns for each model with accuracy on original versus revised.,pol,pol_positive,3.0,0,3
733,"With the current table, one has to compare cells in the top half of the table to those in the bottom half of the table, which is quite difficult to do.",pol,pol_negative,1.0,1,1
734,The paper develops a new 'convolution' operation.,pol,none,0.0,0,0
735,"I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.",pol,pol_negative,1.0,1,1
736,"p2-3, Section 3.1 - I found the equations impossible to read. What",pol,pol_negative,1.0,1,1
737,are the subscripts over?,pol,none,0.0,2,0
738,"In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??",pol,pol_negative,1.0,2,1
739,"Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?",pol,pol_negative,1.0,2,1
740,"Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?",pol,pol_negative,1.0,2,1
741,"Experimental section: Like depthwise convolutions, you seem to achieve reasonable accuracy at fairly low computational cost.",pol,pol_positive,3.0,0,3
742,"It would therefore be much more interesting to compare your networks with ShuffleNet style networks designed for computational efficiency, rather than networks designed mainly to push the benchmark numbers down whatever the cost.",pol,pol_positive,3.0,2,3
743,"It would be helpful to have the computational cost of the network in FLOPs, and running time compared a regular ConvNet using Winograd/Fourier convolutions.",pol,pol_neutral,2.0,2,2
744,"The paper proposes a neural net implementation of counterfactual regret minimization where 2 networks are learnt, one for estimating the cumulative regret (used to derive the immediate policy) and the other one for estimating a cumulative mixture policy.",pol,none,0.0,0,0
745,In addition the authors also propose an original MC sampling strategy which generalize outcome and external sampling strategies.,pol,none,0.0,0,0
746,The paper is interesting and easy to read. My main concern is about the feasibility of using a neural networks to learn cumulative quantities.,pol,pol_positive,3.0,3,3
747,The problem of learning cumulative quantities in a neural net is that we need two types of samples:,pol,none,0.0,0,0
748,"- the positive examples: samples from which we train our network to predict its own value plus the new quantity,",pol,none,0.0,0,0
749,but also:,pol,none,0.0,0,0
750,"- the negative examples: samples from which we should train the network to predict 0, or any desired initial value.",pol,none,0.0,1,0
751,"However in the approach proposed here, the negative examples are missing.",pol,pol_negative,1.0,1,1
752,So the network is not trained to predict 0 (or any initial values) for a newly encountered state.,pol,none,0.0,1,0
753,"And since neural networks generalize (very well...) to states that have not been sampled yet, the network would predict an arbitrary values in states that are visited for the first time.",pol,none,0.0,0,0
754,"For example the network predicting the cumulative regret may generalize to large values at newly visited states, instead of predicting a value close to 0.",pol,none,0.0,1,0
755,"The resulting policy can be arbitrarily different from an exploratory (close to uniform) policy, which would be required to minimize regret from a newly visited state.",pol,none,0.0,0,0
756,"Then, even if that state is visited frequently in the future, this error in prediction will never be corrected because the target cumulative regret depends on the previous prediction.",pol,none,0.0,1,0
757,So there is no guarantee this algorithm will minimise the overall regret.,pol,pol_negative,1.0,1,1
758,"This is a well known problem for exploration (regret minimization) in reinforcement learning as well (see e.g. the work on pseudo-counts [Bellemare et al., 2016, Unifying Count-Based Exploration and Intrinsic Motivation] as one possible approach based on learning a density model).",pol,none,0.0,0,0
759,"Here, maybe a way to alleviate this problem would be to generate negative samples (where the network would be trained to predict low cumulative values) by following a different (possibly more exploratory) policy.",pol,none,0.0,1,0
760,Other comments:,pol,none,0.0,0,0
761,- It does not seem necessary to predict cumulative mixture policies (ASN network).,pol,pol_negative,1.0,1,1
762,One could train a mixture policy network to directly predict the current policy along trajectories generated by MC.,pol,none,0.0,0,0
763,"Since the samples would be generated according to the current policy \sigma_t, any information nodes I_i would be sampled proportionally to \pi^{\sigma^t}_i(I_i), which is the same probability as in the definition of the mixture policy (4).",pol,none,0.0,0,0
764,This would remove the need to learn a cumulative quantity.,pol,none,0.0,0,0
765,"- It would help to have a discussion about how to implement (7), for example do you use a target network to keep the target value R_t+r_t fixed for several steps?",pol,pol_neutral,2.0,2,2
766,- It is not clear how the initialisation (10) is implemented.,pol,pol_negative,1.0,1,1
767,"Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?",pol,pol_negative,1.0,2,1
768,Pros,pol,none,0.0,0,0
769,Solid technical innovation/contribution:,pol,pol_positive,3.0,0,3
770,- The paper proposed a novel method FICM that bridged the intrinsic reward in DRL with optical flow loss in CV to encourage exploration in an environment with sparse rewards.,pol,pol_positive,3.0,0,3
771,"To the best of my knowledge, this was the first paper proposed to use moving patterns in two consecutive observations to motivate agent exploration.",pol,pol_positive,3.0,0,3
772,Balanced view:,pol,none,0.0,0,0
773,"- The authors discussed both the advantages of FICM and settings that FICM might fail to perform well, and conducted experiments to better help the readers understand such nuances.",pol,pol_positive,3.0,0,3
774,Such balanced view should be valuable to RL communities in both academia and industry.,pol,none,0.0,2,0
775,Clarity:,pol,none,0.0,0,0
776,"- In general this was a very well-written paper, I had no difficulty in following the paper throughout.",pol,pol_positive,3.0,1,3
777,"The proposed method (FICM) was clearly motivated, and the authors provided good coverage of related works.",pol,pol_positive,3.0,3,3
778,"Notably, the authors reviewed two relevant methods upon which FICM was motivated, which made the paper self-contained.",pol,pol_positive,3.0,1,3
779,Cons,pol,none,0.0,0,0
780,Experiments:,pol,none,0.0,0,0
781,"- Experiments were conducted only using a few recent results as baselines (ICM, forward dynamics, RND).",pol,pol_negative,1.0,1,1
782,It would be interesting to compare FICM against simpler exploration baselines such as epsilon-greedy or entropy regularization.,pol,pol_neutral,2.0,2,2
783,"- I’d also like to see more extensive comparisons between FICM and ICM across different datasets, for example, Super Mario Bros. and the Atari games, instead of only comparing FICM against ICM on ViZDoom.",pol,pol_negative,1.0,2,1
784,Significance of the innovation:,pol,none,0.0,0,0
785,"- The proposed exploration method seemed to be applicable with a particular RL setting: the environment changes could be represented through consecutive frames (e.g., video games), and optical flow could be used to interpret any object displacements in such consecutive frames.",pol,pol_positive,3.0,3,3
786,"And as the authors discussed, even under such constraints the applicability of proposed method depends on how much changes of the environment were relevant to the goal.",pol,none,0.0,0,0
787,Reproducibility:,pol,none,0.0,0,0
788,"- Although the authors discussed the experiment setting in detail in supplements, I believe open-sourcing the code / software used to conduct the experiments would be greatly help with the reproducibility of the proposed method for researchers or practitioners.",pol,pol_negative,1.0,2,1
789,Summary,pol,none,0.0,0,0
790,"A good paper overall, but the experiments were relatively weak (common for most ICLR submissions) and the novelty was somewhat limited.",pol,pol_negative,1.0,1,1
791,The submission proposes a new method for agent design to learn about the behaviour of other fixed agents inhabiting the same environment.,pol,none,0.0,0,0
792,The method builds on imitation learning (behavioural cloning) to model the agent’s behaviour and reinforcement learning to learn a probing policy to more broadly explore different target agent behaviours.,pol,none,0.0,0,0
793,"Overall, the approach falls into the field of intrinsic motivation / curiosity-like reward generation procedures but with respect to target agent behaviour instead of the agent’s environment.",pol,none,0.0,0,0
794,"While learning to model the target agent’s inner state, the RL reward is generated based on the difference of the target agent’s inner state between consecutive time steps.",pol,none,0.0,0,0
795,The approach is evaluated against a small set of baselines in various toy grid-world scenarios and a sorting task and overall performs commensurate or better than the investigated baselines.,pol,none,0.0,0,0
796,"Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours.",pol,pol_negative,1.0,1,1
797,It would be highly beneficial to evaluate these aspects.,pol,pol_positive,3.0,2,3
798,"Furthermore, it would be beneficial to provide more information about the baselines; in particular the type of count-based exploration.",pol,pol_neutral,2.0,2,2
799,"For the generated figures, it would be beneficial to include standard deviation and mean over multiple runs to not only evaluate performance but also robustness.",pol,pol_neutral,2.0,2,2
800,"Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming).",pol,pol_negative,1.0,1,1
801,"One additional aspect pointing towards the necessity of further evaluation is the strong dependence of performance on the dimensionality of the latent, internal state (Fig.4).",pol,pol_neutral,2.0,0,2
802,Minor issues:,pol,none,0.0,0,0
803,- Reward formulations for the baselines as part of the appendix.,pol,pol_neutral,2.0,1,2
804,- Same scale for the y-axes across figures,pol,pol_neutral,2.0,1,2
805,"This paper proposes a pair of LSTM networks, one of which estimates the current strategy at iteration t+1 and the other estimates the average strategy after t iterations.",pol,none,0.0,0,0
806,"By using these networks within a CFR framework, the authors manage to avoid huge memory requirements traditionally needed to save cumulative regret and average strategy values for all information sets across many iterations.",pol,none,0.0,0,0
807,"The neural networks are trained via a novel sampling method with lower variance/memory-requirements that outcome/external sampling, and are amendable to continual improvement by warm-starting the networks based on cloned tabular regret values.",pol,none,0.0,0,0
808,"Overall, the paper is well-written with clear definitions/explanations plus  comprehensive ablation-analyses throughout, and thus constitutes a nice addition to the recent literature on leveraging neural networks for IIG.",pol,pol_positive,3.0,3,3
809,"I did not find many flaws to point out, except I believe the paper could benefit from more extensive  comparisons in Figure 4A against other IIG methods such as Deep Stack, as well as comparing on much larger IIG settings with many more states to see how the neural CFR methods hold up in the regime where they are most needed.",pol,pol_neutral,2.0,1,2
810,"Typo:  ""care algorithm design"" -> ""careful algorithm design""",pol,pol_neutral,2.0,2,2
811,The paper proposes a new way to generate adversarial images that are perturbed based on natural images called Shadow Attach.,pol,none,0.0,0,0
812,The generated adversarial images are imperceptible and have a large norm to escape the certification regions.,pol,none,0.0,0,0
813,"The proposed method incorporates the quantities of total variation of the perturbation, change in the mean of each color channel, and dissimilarity between channels, into the loss function, to make sure the generate adversarial images are smooth and natural.",pol,none,0.0,0,0
814,Quantitative studies on CIFAR-10 and ImageNet shows that the new attack method can generate adversarial images that have larger certified radii than natural images.,pol,none,0.0,0,0
815,"To further improve the paper, it would be great if the authors can address the following questions:",pol,none,0.0,1,0
816,"- In Table 1, for ImageNet, Shadow Attach does not always generate adversarial examples that have on average larger certified radii than the natural parallel, at least for sigma=0.5 and 1.0. Could the authors explain the reason?",pol,pol_negative,1.0,2,1
817,"- In Table 2, it is not clear to me what is the point for comparing errors of the natural images (which measures the misclassification rate of a natural image) and that of the adversarial images (which measures successful attacks rate), and why this comparison helps to support the claim that the attack results in a stronger certificates.",pol,pol_negative,1.0,1,1
818,"In my opinion, to support the above claim, shouldn’t the authors provide a similar table as Table 1, directly comparing the certified radii of the natural images and adversarial images?",pol,pol_negative,1.0,2,1
819,"- From Figure 9, we see the certificate radii of the natural have at least two peaks. Though on average the certificate radii of the adversarial attacks is higher than that of the natural images, it is smaller than the right peak. Could the authors elaborate more of the results?",pol,pol_negative,1.0,2,1
820,- Sim(delta) should be Dissim(delta) which measures the dissimilarity between channels.,pol,pol_negative,1.0,1,1
821,A smaller dissimilarity suggests a greater similarity between channels.,pol,none,0.0,0,0
822,- Lambda sim and lambda s are used interchangeably. Please make it consistent.,pol,pol_negative,1.0,1,1
823,- The caption of Table 1 is a little vague. Please clearly state the meaning of the numbers in the table.,pol,pol_negative,1.0,1,1
824,"The paper proposes a new joint learning algorithm that works for two tasks, NER and RE.",pol,none,0.0,0,0
825,"The model is based on a pre-trained BERT model, which provides the word vectors of the input word sequence.",pol,none,0.0,0,0
826,"Then it solves two tasks with two network branches: the first branch minimizes the loss for NER, and the second branch minimizes the loss for RE.",pol,none,0.0,0,0
827,"The second branch uses entity labels predicted by the first branch, so joint learning may benefit both tasks.",pol,none,0.0,0,0
828,"The design of the architecture is novel, but it is also not groundbreaking.",pol,pol_positive,3.0,1,3
829,"Each network branch is from known structures, but the combination is not proposed before.",pol,pol_positive,3.0,1,3
830,The submission has evaluated the proposed algorithms on four datasets and improved SOTA performances.,pol,pol_positive,3.0,3,3
831,The ablation study justifies the design details.,pol,pol_positive,3.0,3,3
832,The writing is generally clear.,pol,pol_positive,3.0,3,3
833,Now critics:,pol,none,0.0,0,0
834,Ablation study:,pol,none,0.0,0,0
835,"1. As pointed by one public comment, the ablation study should show how much improvement is from BERT vectors.",pol,pol_negative,1.0,1,1
836,"2. I'd like to see another ablation study of whether RE helps NER. If you remove the RE component, does the NER performance suffer?",pol,pol_negative,1.0,2,1
837,Writing:,pol,none,0.0,0,0
838,3. how are predicted labels embedded? Do you learn a vector of each tag of BIOES and then take a weighted sum of these vectors with predicted probabilities as weights?,pol,pol_negative,1.0,2,1
839,This paper introduces deficiency bottleneck for learning a data representation and represent  complicated channels using simpler ones.,pol,none,0.0,0,0
840,This problem has a natural variational form that can be easily implemented from VIB.,pol,none,0.0,0,0
841,Experiments show good performance comparing to VIB.,pol,pol_positive,3.0,3,3
842,This paper is well-written and easy to read.,pol,pol_positive,3.0,3,3
843,The idea using KL divergence creating a deficiency channel to learn data representation is very natural.,pol,pol_positive,3.0,3,3
844,"It is interesting that this formulation could be understood as minimizing a regularized risk gap of statistical decision problems, which justifies the usage of deficiency bottleneck (eq.9).",pol,pol_positive,3.0,0,3
845,"My biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem.",pol,pol_negative,1.0,1,1
846,"However, it looks like authors only compared with VIB which is similar to the proposed method in terms of the objective function.",pol,pol_negative,1.0,1,1
847,"For example, how does the method compare with (variants of) Variational Autoencoder?",pol,pol_neutral,2.0,1,2
848,A discussion on this or some empirical evaluations would be nice.,pol,pol_positive,3.0,2,3
849,The paper introduces adaptive kernels (that adapts its weights as a function of image content) to the framework of CNN.,pol,none,0.0,0,0
850,The benefit of adaptive kernels is the reduction of memory usage (at training and at the inference time) as well as training speedups (up to 2x).,pol,none,0.0,0,0
851,The kernels are evaluated on two datasets MNIST and CIFAR10,pol,none,0.0,0,0
852,I like the idea of building models that are memory efficient at training and at evaluation time.,pol,none,0.0,3,0
853,"However, the evaluation of the proposed adaptive kernels is rather limited.",pol,pol_negative,1.0,1,1
854,"In order to improve the paper, the authors could take into consideration the following points:",pol,none,0.0,0,0
855,1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?,pol,pol_negative,1.0,2,1
856,"2. I might have missed it, but I couldn't find any motivation on why tanh is used as nonlinearity. Would the method work with relu?",pol,pol_negative,1.0,2,1
857,3. Traditional convolutional kernels together with max pooling operations ensures some degree of translation invariance.,pol,none,0.0,0,0
858,How big is the generalization gap for the tested models when adaptive kernel is used?,pol,pol_negative,1.0,2,1
859,4. How sensitive are the results to the number of adaptive kernels in the layers.,pol,pol_negative,1.0,1,1
860,"5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?",pol,pol_negative,1.0,2,1
861,6. On CIFAR10 the results seem to be worse that other methods.,pol,pol_negative,1.0,1,1
862,"However, it is important to note that the Adaptive Kernels CNN has way less parameters.",pol,none,0.0,0,0
863,It would be interesting to see how the performance of adaptive kernels based CNNs scales with the number of parameters.,pol,pol_negative,1.0,2,1
864,"7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.",pol,pol_negative,1.0,2,1
865,8. The authors acknowledge the similarities (and some differences) with Brabandere et al (2016).,pol,none,0.0,0,0
866,It might be beneficial to include comparison to this approach in the experimental section.,pol,pol_negative,1.0,2,1
867,"Moreover, given the similarities, it might be good to discuss the differences in the approaches in the introduction section.",pol,pol_negative,1.0,2,1
868,"9. The ideas presented in the paper seems related to general concept of hypernetworks, where one network learns (or helps to learn) paramenters of the other network.",pol,none,0.0,3,0
869,It would be nice to position the ideas from the paper w.r.t. this line of research too.,pol,pol_negative,1.0,2,1
870,10. Another related paper seems to be Spatial Transformer Networks (Jaderberg et al.).,pol,pol_negative,1.0,0,1
871,"I like the drawings, however, the font on the drawings is too small - making it hard to read.",pol,pol_negative,1.0,1,1
872,Some typos:,pol,none,0.0,0,0
873,1. the difficult to train the network,pol,pol_negative,1.0,1,1
874,2. table 2: Dynamic -> Adaptive?,pol,pol_negative,1.0,2,1
875,"Overall, the paper presents interesting ideas with some degree of originality.",pol,pol_positive,3.0,3,3
876,I'd encourage the authors to extend the intro and position the ideas w.r.t. existing works and extend the evaluation.,pol,pol_negative,1.0,2,1
877,"This papers tackles the following question. Is it possible to learn the ""most"" complex instance of a class of (combinatorial) problem while finding (or recovering) algorithms with strong minimax rate.",pol,none,0.0,0,0
878,This is very interesting and clearly a nice line of work (in theory though).,pol,pol_positive,3.0,3,3
879,The techniques used rely on GANs since it can be shown that finding the best (random) algorithm and the worst (deterministic) instance is equivalent to finding the worst random instance against the best deterministic algorithm.,pol,none,0.0,0,0
880,"This is actually a direct consequence of any minmax theorem in game theory; the authors decided to credit that result to Yao (I tend to *strongly* disagree with that point as, even if he stated this fact in CS, this result was quite standard several decades before him - anyway.).",pol,pol_negative,1.0,0,1
881,Then this idea is evaluated in two examples.,pol,none,0.0,0,0
882,A toy problem (the ski rental) and a more or less concrete ones (adwords pb of Mehta).,pol,none,0.0,0,0
883,This is the major disappointment in the paper.,pol,pol_negative,1.0,1,1
884,"The basic idea is very interesting, but I would have expect more interesting use cases as teased by the first sentence of the abstract ""find algorithms with strong worst-case guarantees for online combinatorial optimization problems"".",pol,pol_negative,1.0,3,1
885,"So at the end, I am a bit puzzled. I really like the idea, but I have the feeling that this technique should have been developed for more complicated setting. Or maybe it is actually not working on more difficult combinatorial problem (and this is hidden in the paper).",pol,pol_negative,1.0,1,1
886,I believe that this paper is thus not in its final form and could be largely improved.,pol,pol_negative,1.0,1,1
887,Summary:,pol,none,0.0,0,0
888,This paper proposes a way to train a manager agent which would manage a bunch of worker agents to achieve a high-level goal.,pol,none,0.0,0,0
889,Each worker has its own set of skills and preferences and the manager tries to assign sub-tasks to these agents along with bonuses such that the agents can even perform tasks that are not preferred by them.,pol,none,0.0,0,0
890,Authors achieve this by training a manager which tracks the skills and preferences of the agents on the fly.,pol,none,0.0,0,0
891,Authors have done an extensive analysis of the proposed approach in two simple domains: resource collection and crafting.,pol,none,0.0,0,0
892,Major comments:,pol,none,0.0,0,0
893,This paper focuses on multi-agent settings with self-interested agents.,pol,none,0.0,0,0
894,The problem formulation and the solution are novel enough.,pol,pol_positive,3.0,3,3
895,Experiments are on toy domains with very few goals and sub-task dependencies.,pol,pol_negative,1.0,0,1
896,"However, authors have done a good job in doing an extensive analysis of the proposed approach.",pol,pol_positive,3.0,3,3
897,1.	Can you comment about the scalability of the proposed solution when the number of possible subtasks increases? When the sub-task dependency graph size increases?,pol,pol_neutral,2.0,2,2
898,2.	What is the reason for using rule-based agents in all the experiments? It would have been more useful if all the analysis are done with RL agents rather than rule-based agents.,pol,pol_negative,1.0,2,1
899,It would also make the paper stronger.,pol,none,0.0,0,0
900,3.	Are the authors willing to release the code? Overall the model looks complicated and the appendix is not sufficient to reproduce the results in the paper.,pol,pol_negative,1.0,1,1
901,I would increase my rating if the authors are willing to release the code to reproduce all the results reported in the paper.,pol,pol_neutral,2.0,2,2
902,Minor comments:,pol,none,0.0,0,0
903,"1.	Page 3, line 9: “typical” -> “typically”",pol,pol_neutral,2.0,2,2
904,"2.	Page 3, “intention” section: “Based on the its reward ..” Check grammar.",pol,pol_neutral,2.0,2,2
905,"3.	Page 5, last line: “the total quantitative is 10” check grammar.",pol,pol_neutral,2.0,1,2
906,"4.	Page 8, conclusions, second line: “nad” -> “and”",pol,pol_neutral,2.0,2,2
907,"5.	Page 8, conclusions, 4th line: “combing” -> “combine”",pol,pol_neutral,2.0,2,2
908,The authors proposed an AutoLoss controller that can learn to take actions of updating different parameters and using different loss functions.,pol,none,0.0,0,0
909,Pros,pol,none,0.0,0,0
910,1. Propose a unified framework for different loss objectives and parameters.,pol,pol_positive,3.0,0,3
911,2. An interesting idea in meta learning for learning loss objectives/schedule.,pol,pol_positive,3.0,3,3
912,Cons:,pol,none,0.0,0,0
913,"1. The formulation uses REINFORCE, which is often known with high variance.",pol,pol_negative,1.0,0,1
914,Are the results averaged across different runs? Can you show the variance?,pol,pol_neutral,2.0,2,2
915,It is hard to understand the results without discussing it.,pol,pol_negative,1.0,1,1
916,The sample complexity should be also higher than traditional approaches.,pol,pol_neutral,2.0,1,2
917,2. It is hard to understand what the model has learned compared to hand-crafted schedule. Are there any analysis other than the results alone?,pol,pol_negative,1.0,1,1
918,3. Why do you set S=1 in the experiments? What’s the importance of S?,pol,pol_neutral,2.0,2,2
919,4. I think it is quite surprising the AutoLoss can resolve mode collapse in GANs.,pol,pol_neutral,2.0,3,2
920,I think more analysis is needed to support this claim.,pol,pol_neutral,2.0,1,2
921,"5. The evaluation metric of multi-task MT is quite weird. Normally people report BLEU, whereas the authors use PPL.",pol,pol_negative,1.0,1,1
922,"6. According to https://github.com/pfnet-research/chainer-gan-lib, I think the bested reported DCGAN results is not 6.16 on CIFAR-10 and people still found other tricks such as spectral-norm is needed to prevent mode-collapse.",pol,none,0.0,1,0
923,Minor:,pol,none,0.0,0,0
924,1. The usage of footnote 2 is incorrect.,pol,pol_negative,1.0,1,1
925,"2. In references, some words should be capitalized properly such as gan->GAN.",pol,pol_neutral,2.0,1,2
926,"Summary: This paper uses visual representation learned over monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs for multimodal NMT.",pol,none,0.0,0,0
927,Their approach enables visual information to be integrated into large-scale text-only NMT.,pol,none,0.0,0,0
928,Experiments on four widely used translation datasets show that the proposed approach achieves significant improvements over strong baselines.,pol,none,0.0,3,0
929,Strengths:,pol,none,0.0,0,0
930,- This paper is well motivated and well written. I especially like how they use external paired sentence-image data from Multi30k to learn weak pairs for sentences in machine translation.,pol,pol_positive,3.0,3,3
931,- Experimental results are convincing. I like how low-resource translation is included as a priority in their experiments.,pol,pol_positive,3.0,3,3
932,Weaknesses:,pol,none,0.0,0,0
933,"- Do you have any explanations as to why the number of images, if too large, actually hurts translation performance? Is it because more images also leads to a higher chance of noisy images?",pol,pol_negative,1.0,2,1
934,- It would be nice to have an experiment that varies the size of the external paired sentence-image dataset and tested the impact on performance.,pol,pol_negative,1.0,2,1
935,- Please comment on the extra computation required for obtaining image data for MT sentences and for learning image representations.,pol,pol_negative,1.0,1,1
936,- Why are there missing BLEU scores and the number of parameters in Table 1?,pol,pol_negative,1.0,2,1
937,### Post rebuttal,pol,none,0.0,0,0
938,#,pol,none,0.0,0,0
939,##,pol,none,0.0,0,0
940,Thank you for your detailed answers to my questions.,pol,none,0.0,0,0
941,"The paper is well written and the main contribution, a methodology to find “blind-spot attacks” well motivated and differences to prior work stated clearly.",pol,pol_positive,3.0,3,3
942,The empirical results presented in Figure 1 and 2 are very convincing.,pol,pol_positive,3.0,3,3
943,"The gain of using a sufficiently more complicated approach to assess the overall distance between the test and training dataset is not clear, comparing it to the very insightful histograms.",pol,pol_negative,1.0,1,1
944,"Why for example not using a simple score based on the histogram, or even the mean distance?",pol,pol_neutral,2.0,1,2
945,Of course providing a single measure would allow to leverage that information during training.,pol,none,0.0,0,0
946,"However, in its current form this seems rather complicated and computationally expensive (KL-based).",pol,none,0.0,1,0
947,As stated later in the paper the histograms themselves are not informative enough to detect such blind-spot transformation.,pol,none,0.0,1,0
948,Intuitively this makes a lot of sense given that the distance is based on the network embedding and is therefore also susceptible to this kind of data.,pol,pol_positive,3.0,0,3
949,"However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue.",pol,pol_negative,1.0,1,1
950,"The paper considers an actor-critic scheme for multiagent RL, where the critic is specific to each agent and has access to all other agents' embedded observations.",pol,none,0.0,0,0
951,The main idea is to use an attention mechanism in the critic that learns to selectively scale the contributions of the other agents.,pol,none,0.0,0,0
952,"The paper presents sufficient motivation and background, and the proposed algorithmic implementation seems reasonable.",pol,pol_positive,3.0,3,3
953,"The proposed scheme is compared to two recent algorithms for centralized training of decentralized policies, and shows comparable or better results on two synthetic multiagent problems.",pol,none,0.0,0,0
954,I believe that the idea and approach of the paper are interesting and contribute to the multiagent learning literature.,pol,pol_positive,3.0,3,3
955,Regarding cons:,pol,none,0.0,0,0
956,"- The critical structural choices (such as the attention model in section 3.2) are presented without too much justification, discussion of alternatives, etc.",pol,none,0.0,1,0
957,"- The experiments show the learning results, but do not provide a peak ""under the hood"" to understand the way attention evolved and contributed to the results.",pol,pol_negative,1.0,1,1
958,"- The experiments show good results compared to existing algorithms, but not impressively so.",pol,pol_negative,1.0,1,1
959,"This paper suggests a quantization approach for neural networks, based on the Product Quantization (PQ) algorithm which has been successful in quantization for similarity search.",pol,none,0.0,0,0
960,"The basic idea is to quantize the weights of a neuron/single layer with a variant of PQ, which is modified to optimize the quantization error of inner products of sample inputs with the weights, rather than the weights themselves.",pol,none,0.0,0,0
961,This is cast as a weighted variant of k-means.,pol,none,0.0,0,0
962,"The inner product is more directly related to the network output (though still does not account for non-linear neuron activations) and thus is expected to yield better downstream performance, and only requires introducing unlabeled input samples into the quantization process.",pol,none,0.0,0,0
963,This approach is built into a pipeline that gradually quantizes the entire network.,pol,none,0.0,0,0
964,"Overall, I support the paper and recommend acceptance.",pol,pol_positive,3.0,3,3
965,"PQ is known to be successful for quantization in other contexts, and the specialization suggested here for neural networks is natural and well-motivated.",pol,pol_positive,3.0,3,3
966,"The method can be expected to perform well empirically, which the experiments verify, and to have potential impact.",pol,pol_positive,3.0,3,3
967,Questions:,pol,none,0.0,0,0
968,1. Can you comment on the quantization time of the suggested method? Repeatedly solving the EM steps can add up to quite an overhead.,pol,pol_negative,1.0,2,1
969,Does it pose a difficulty? How does it compare to other methods?,pol,pol_neutral,2.0,2,2
970,"2. Can you elaborate on the issue of non-linearity? It is mentioned only briefly in the conclusion. What is the difficulty in incorporating it? Is it in solving equation (4)? And perhaps, how do you expect it to effect the results?",pol,pol_neutral,2.0,2,2
971,"This paper combines the global and local stability prediction and tries to get interpretable results using the stethoscope design, which is actually a weighted subbranch for the main branch.",pol,none,0.0,0,0
972,There are several concerns regarding the proposed framework.,pol,none,0.0,1,0
973,1) How to choose \lambda?,pol,pol_neutral,2.0,2,2
974,A better design could be a learnable \lambda.,pol,pol_negative,1.0,1,1
975,"Instead of just one scalar value, it could be better to learn a map of \lambdas, which indicates the distribution of local stability and how it is related to global stability.",pol,pol_neutral,2.0,1,2
976,The visualization of the \lambda map might be more interpretable for understanding the stability prediction.,pol,pol_neutral,2.0,2,2
977,"2) The global stability prediction does not have a consistent correlation with the local stability prediction, as shown by the easy and hard examples.",pol,pol_negative,1.0,1,1
978,This complex relationship will confuse the network during the training.,pol,none,0.0,1,0
979,"That is, the current design hasn't well considered the local and global stability relation, but just simply sum them up.",pol,pol_negative,1.0,0,1
980,This is hard to provide a meaningful interpretation of the task.,pol,pol_negative,1.0,1,1
981,This paper provides an approach to use visual information to improve text only neural machine translation systems.,pol,none,0.0,0,0
982,"The approach creates a ""topic word to images"" map using an existing image aligned translation corpora.",pol,none,0.0,0,0
983,"Given a source sentence, the model extracts relevant images, extracts their Resnet features and fuses them with the features generated from the word sequence.",pol,none,0.0,0,0
984,The decoder uses these fused representation to generate the target sentence.,pol,none,0.0,0,0
985,"Overall, I like the approach, seems like it can be easily augmented to existing NMT systems.",pol,pol_positive,3.0,3,3
986,One of the claims of the paper was to be able to use monolingual image aligned data.,pol,none,0.0,0,0
987,However image captioning datasets are not mentioned.,pol,pol_negative,1.0,1,1
988,It would make sense to use image captioning data to create the image lookup.,pol,pol_negative,1.0,2,1
989,"Also, what will be the performance of a standard image captioning system on the task ?",pol,pol_negative,1.0,2,1
990,"I believe it will not be great, but I think for completeness, you should add such a baseline.",pol,pol_negative,1.0,1,1
991,Minor comments:,pol,none,0.0,0,0
992,1. What is M in Algorithm 1 ?,pol,pol_negative,1.0,2,1
993,"2. First paragraph in related work is very unrelated to the current subject, please remove.",pol,pol_negative,1.0,1,1
994,"Gradient-free evolutionary search methods for Reinforcement Learning are typically very stable, but scale poorly with the number of parameters when optimizing highly-parametrized policies (e.g. neural networks).",pol,none,0.0,0,0
995,"Meanwhile, gradient-based deep RL methods, such as DDPG are often sample efficient, particularly in the off-policy setting when, unlike evolutionary search methods, they can continue to use previous experience to estimate values.",pol,none,0.0,0,0
996,"However, these approaches can also be unstable.",pol,none,0.0,0,0
997,This work combines the well-known CEM search with TD3 (an improved variant of DDPG).,pol,none,0.0,0,0
998,"The key idea of of this work is in each generation of CEM, 1/2 the individuals are improved using TD3 (i.e. the RL gradient).",pol,none,0.0,0,0
999,This method is made more practical by using a replay buffer so experience from previous generations is used for the TD3 updates and importance sampling is used to improve the efficiency of CEM.,pol,none,0.0,0,0
1000,"This work shows, on some simple control tasks, that this method appears to result in much stronger performance compared with CEM, and small improvements over TD3 alone.",pol,none,0.0,0,0
1001,It also typically out-performs ERL.,pol,none,0.0,0,0
1002,"Intuitively, it seems like it may be possible to construct counter-examples where the gradient updates will prevent convergence.",pol,pol_negative,1.0,0,1
1003,Issues of convergence seem like they deserve some discussion here and potentially could be examined empirically (is CEM-TD3 converging in the swimmer?).,pol,pol_negative,1.0,2,1
1004,"The justification that the method of Khadka & Tumer (2018) cannot be extended to use CEM, since the RL policies do not comply with the covariance matrix is unclear to me.",pol,pol_negative,1.0,1,1
1005,"Algorithm 1, step 20, the covariance matrix is updated after the RL step so regardless of how the RL policies are generated, the search distribution on the next distribution includes them.",pol,pol_negative,1.0,0,1
1006,"In both this work, and Khadka & Tumer, the RL updates lead to policies that differ from the search distribution (indeed that is the point), and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.",pol,pol_negative,1.0,1,1
1007,"It sees like the more important distinction is that, in this approach, the information flows both from ES to RL and vice-versa, rather than just from RL to ES.",pol,none,0.0,0,0
1008,"One view of this method would be that it is an ensemble method for learning the policy [e.g. similar to Osband et al., 2016 for DQN].",pol,none,0.0,1,0
1009,"This could be discussed and a relevant control would be to keep a population (ensemble) of policies, but only update using RL while sharing experience across all actors.",pol,pol_neutral,2.0,1,2
1010,This would isolate the ensemble effect from the evolutionary search.,pol,none,0.0,0,0
1011,Minor issues:,pol,none,0.0,0,0
1012,- The ReLU non-linearity in DDPG and TD3 prior work is replaced with tanh.,pol,none,0.0,1,0
1013,"This change is noted, but it would be useful to make a least a brief (i.e. one sentence) comment on the motivation for this change.",pol,pol_negative,1.0,1,1
1014,- The paper is over the hard page limit for ICLR so needs to be edit to reduce the length.,pol,pol_negative,1.0,1,1
1015,"Osband I, Blundell C, Pritzel A, Van Roy B. Deep exploration via bootstrapped DQN. InAdvances in neural information processing systems 2016 (pp. 4026-4034).",pol,none,0.0,0,0
1016,Summary:,pol,none,0.0,0,0
1017,This paper looks at the MARL problem in high-dimensional continuous control settings.,pol,pol_positive,3.0,0,3
1018,"To improve learning in this multi-agent setting, they propose to pre-train a lower-level policy that takes as input foot-step goals and is executed for a fixed number of timestep, thereby simplifying both the learning and exploration.",pol,pol_positive,3.0,0,3
1019,I'm a bit unsure of how to evaluate this paper.,pol,none,0.0,1,0
1020,"On the one hand, I believe it has several contributions:",pol,pol_positive,3.0,0,3
1021,- Proposing a new MARL - continuous control environment,pol,pol_positive,3.0,0,3
1022,"- Proposing a new lower-level policy for high-demensional continuous control environments, including how to learn it",pol,pol_positive,3.0,0,3
1023,- Using it to perform MARL in this environment,pol,pol_positive,3.0,0,3
1024,"On the other hand, it is hard to say what the _main_ contribution is, which in turn makes it difficult to evaluate whether the experimental evaluation is sufficient:",pol,pol_negative,1.0,1,1
1025,"Clearly, a main part of the paper is the work done to construct the hierarchical setup, including goal space, observation space and reward functions.",pol,pol_positive,3.0,0,3
1026,"However, this work, as far as I can tell, is separate from the MARL problem.",pol,pol_negative,1.0,0,1
1027,"Furthermore, there are several similar ideas already published, so comparison against those (for example by J. Peng, N. Heess or J. Mere) either as argument or even better as experiment, would be helpful to evaluate the quality of the proposed hierarchy.",pol,pol_negative,1.0,1,1
1028,"On the other hand, there is the application of the hierarchical setup to the MARL problem.",pol,pol_positive,3.0,0,3
1029,"However, as far as I can tell, there is no difference between applying such a hierarchy to the MARL case and to the single agent problem.",pol,pol_negative,1.0,1,1
1030,"Especially if the lower-level component of the hierarchy is pre-trained in a non-MARL setup, it can just be seen as part of the environment from the point of view of the MARL training, offerring limited new insight into MARL.",pol,pol_negative,1.0,1,1
1031,"I believe in the second paragraph of 4.1 the authors provide some insight into this matter, however, I have to admit I do not understand this paragraph:",pol,pol_negative,1.0,1,1
1032,- Why does temporal correlation reduce the non-stationarity of the MARL problem?,pol,pol_negative,1.0,2,1
1033,- Why does structured exploration reduce the number of network parameters that need to be learned?,pol,pol_negative,1.0,2,1
1034,- Why does partial parameter sharing make it easier for each agent to estimate other agents potential changes in behavior?,pol,pol_negative,1.0,2,1
1035,"In summary, I think this is interesting work, but a clearer explanation of the relationship between HRL and MARL, as well as a clearer main argument, supported by experimental evidence, would greatly improve this paper.",pol,pol_negative,1.0,2,1
1036,Edit:,pol,none,0.0,0,0
1037,Thank you for your response.,pol,none,0.0,0,0
1038,"Unfortunately, I don't feel like it sufficiently addresses my questions and concerns.",pol,pol_negative,1.0,1,1
1039,"I do apologize if my original comment wasn't clear regarding the contribution part of the paper. What I was trying to say is not that I didn't see the individual contributions of the paper, but instead that the paper does multiple things simultaneously, without comparing against the relevant baselines for any of the individual contributions.",pol,pol_negative,1.0,1,1
1040,Regarding my questions: I understand where the temporal correlation is coming from in an HRL setting.,pol,pol_positive,3.0,0,3
1041,"However, what was not clear to me is how this reduces the non-stationarity of MARL.",pol,pol_negative,1.0,1,1
1042,"I also understand that HRL can reduce the number of parameters, but I don't see how structured exploration reduces the number of parameters.",pol,pol_negative,1.0,1,1
1043,"And lastly, I also can see how parameter sharing can simplify the learning, but I still don't see how it would allow agents to estimate the behaviour change of other agents easier.",pol,pol_negative,1.0,1,1
1044,"I feel like in the paragraph in questions, a lot of causes and effects are mixed up and more careful descriptions of the benefits of the algorithm would help.",pol,pol_negative,1.0,1,1
1045,"I want to re-iterate that I think that the submitted work by the authors is impressive and can provide valuable insights, but I believe it requires more work and more relevant baselines.",pol,none,0.0,1,0
1046,"The authors extend an existing approach to adaptive softmax classifiers used for the output component of neural language models into the input component, once again allowing tying between the embedding and softmax.",pol,none,0.0,0,0
1047,"This fills a significant gap in the language modeling architecture space, and the perplexity results bear out the advantages of combining adaptively-sized representations with weight tying.",pol,pol_positive,3.0,0,3
1048,"While the advance is in some sense fairly incremental, the centrality of unsupervised language modeling to modern deep NLP (ELMo, BERT, etc.) implies that perplexity improvements as large as this one may have meaningful downstream effects on performance on other tasks.",pol,pol_positive,3.0,0,3
1049,Some things I noticed:,pol,none,0.0,0,0
1050,"- One comparison that I believe is missing (I could be misreading the tables) is comparing directly to Merity et al.'s approach (adaptive softmax but fixed embedding/softmax dimension among the bands). Presumably you're faster, but is there a perplexity trade-off?",pol,pol_neutral,2.0,2,2
1051,- The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; I think it could benefit from tightening up the wording but mostly I just had to read it a couple times. Perhaps all that's complicated is the distinction between embedding and projection weights; it would definitely be helpful to be as explicit about that as possible upfront.,pol,pol_negative,1.0,1,1
1052,- The loss by frequency-bin plots are really fantastic.,pol,pol_positive,3.0,3,3
1053,You could also try a scatterplot of log freq vs. average loss by individual word/BPE token.,pol,pol_positive,3.0,0,3
1054,- Do you have thoughts as to why full-softmax BPE is worse than adaptive softmax word level?,pol,pol_neutral,2.0,2,2
1055,That goes against the current (industry) conventional wisdom in machine translation and large-scale language modeling that BPE is solidly better than word-level approaches because it tackles the softmax bottleneck while also sharing morphological information between words.,pol,none,0.0,0,0
1056,"I vote to reject the paper at this stage, mainly because of the following three points:",pol,pol_negative,1.0,0,1
1057,1) The motivation is unclear and overall structure of the paper is confusing.,pol,pol_negative,1.0,1,1
1058,"It should be better motivated why one should use the duality gap as an upper bound for the ""F-distance"".",pol,pol_negative,1.0,1,1
1059,Minimizing the F-distance as is usually done seems like the more direct and simple approach.,pol,pol_negative,1.0,0,1
1060,"Since the results are far from state of the art, a clean and neat presentation of the theoretical advantages and contributions is crucial.",pol,pol_negative,1.0,3,1
1061,"2) The presentation is not professional, hard to follow and the submission overall looks very rushed:",pol,pol_negative,1.0,1,1
1062,"- In equations, please use \inf, \sup, and \text{...} for text such as distance, data, ...",pol,pol_negative,1.0,1,1
1063,- I have trouble understanding the overall idea behind Algorithm 1 and Eq. (22).,pol,pol_negative,1.0,1,1
1064,What is the definition of f^* and g^* in Eq. (22)? Some explanatory text would be valuable.,pol,pol_negative,1.0,2,1
1065,"- The set F in Definition 3.5 looks odd, as it appears to be recursive and might not be unique.",pol,pol_negative,1.0,1,1
1066,"- The writing looks very rushed, and should be improved.",pol,pol_negative,1.0,2,1
1067,"For example, I have trouble understanding the sentence ""So the existed algorithms should be heuristic or it can get a bad result even we train the neural networks with lots of datasets."" in the introduction.",pol,pol_negative,1.0,1,1
1068,- The aspect ratio in Fig. 5 should be fixed.,pol,pol_negative,1.0,1,1
1069,3) The experiments are completely preliminary and not reasonable:,pol,pol_negative,1.0,1,1
1070,"- The WGAN-GP baseline is very weak, i.e. does not show any reasonable generated images (Fig. 9).",pol,pol_negative,1.0,1,1
1071,There are countless open pytorch implementations on GitHub which out-of-the-box produce much better results.,pol,pol_negative,1.0,0,1
1072,- The shown inception scores are far from state-of-the-art.,pol,pol_negative,1.0,1,1
1073,"It is unclear, why one should use the proposed duality gap GAN.",pol,pol_negative,1.0,1,1
1074,"The paper explores how the architecture, smoothness of the decision boundary and test accuracy of a model impacts the transferability of examples produced from it.",pol,none,0.0,0,0
1075,"The paper provides a couple of novel insights, such as the asymmetry when transferring adversarial examples from one model to another.",pol,none,0.0,0,0
1076,"In addition, a novel method is proposed to enhance the transferability of adversarial examples from any model, through using smoothed gradients.",pol,none,0.0,0,0
1077,"The experiments seem to show that the effect is rather large, and also makes the examples more robust to other transformations such as JPEG compression.",pol,pol_positive,3.0,0,3
1078,"Overall, these are interesting insights that could lead to further developments in making models more robust to adversarial examples.",pol,pol_positive,3.0,0,3
1079,"In particular, deriving adversarial examples that are both transferable and resilient to certain usual image transformations shows that the scope of the issue with adversarial examples may be even greater than what is understood today.",pol,pol_positive,3.0,1,3
1080,The paper is rather clear.,pol,pol_positive,3.0,3,3
1081,"Unfortunately, it is riddled with grammatical errors and should be proof-read carefully. A lot of singular/plurals are off, and some formulations are odd or downright unclear.",pol,pol_negative,1.0,1,1
1082,Some examples (there are way too many to report them all):,pol,none,0.0,0,0
1083,"- ""Transfer-based attackS ... since they ...*",pol,pol_neutral,2.0,1,2
1084,"- ""of adversarial exampleS ...""",pol,pol_neutral,2.0,2,2
1085,"- ""from model A can transfer to model B""",pol,pol_neutral,2.0,2,2
1086,"- ""less transferable than *those from* a shallow model""?",pol,pol_neutral,2.0,2,2
1087,"- ""investigations, We "": don't capitalize",pol,pol_neutral,2.0,1,2
1088,"- ""the averaging *has* a smoothing effect""",pol,pol_neutral,2.0,2,2
1089,"- ""our motivation are""",pol,pol_neutral,2.0,2,2
1090,"- ""contributed it to""",pol,pol_neutral,2.0,2,2
1091,"- ""available *to the* adversary""",pol,pol_neutral,2.0,2,2
1092,"- ""crafting adversarial perturbationS""",pol,pol_neutral,2.0,2,2
1093,"- ""directly evaluation""",pol,pol_neutral,2.0,2,2
1094,"- ""be fixed 100""",pol,pol_neutral,2.0,2,2
1095,Pros:,pol,none,0.0,0,0
1096,- Transferability and robustness of adversarial examples is a very important problem,pol,pol_positive,3.0,0,3
1097,"- Interesting insights, esp. the construction and evaluation of examples that are more resilient to certain image transformations",pol,pol_positive,3.0,3,3
1098,- Experimental results are convincing,pol,pol_positive,3.0,3,3
1099,Cons:,pol,none,0.0,0,0
1100,- Contribution overall may be a bit limited,pol,pol_negative,1.0,1,1
1101,- Grammatical errors and odd formulations all over the place,pol,pol_negative,1.0,1,1
1102,Quality/clarity:,pol,none,0.0,0,0
1103,- The problem setting description is neither formal nor intuitive which made it very hard for me to understand exactly the problem you are trying to solve.,pol,pol_negative,1.0,1,1
1104,Starting with S and i: I guess S and i are both simply varying-length sequences in U.,pol,none,0.0,1,0
1105,"- In general the intro should focus more on an intuitive (and/or formal) explanation of the problem setting, with some equations that explain the problem you want to work on. Right now it is too heavy on 'related work' (this is just my opinion).",pol,pol_negative,1.0,1,1
1106,Originality/Significance:,pol,none,0.0,0,0
1107,I have certainly never seen a ML-based paper on this topic.,pol,none,0.0,1,0
1108,The idea of 'learning' prior information about the heavy hitters seems original.,pol,pol_positive,3.0,3,3
1109,Pros:,pol,none,0.0,0,0
1110,It seems like a creative and interesting place to use machine learning.,pol,pol_positive,3.0,3,3
1111,the plots in Figure 5.2 seem promising.,pol,pol_positive,3.0,3,3
1112,Cons:,pol,none,0.0,0,0
1113,- The formalization in Paragraph 3 of the Intro is not very formal. I guess S and i are both simply varying-length sequences in U.,pol,pol_negative,1.0,1,1
1114,"- In general the intro should focus more on an intuitive (and/or formal) explanation of the problem setting, with some equations that explain the problem you want to work on. Right now it is too heavy on 'related work' (this is just my opinion).",pol,pol_negative,1.0,1,1
1115,"-In describing Eqn 3 there are some weird remarks, e.g. ""N is the sum of all frequencies"". Do you mean that N is the total number of available frequencies? i.e.",pol,pol_negative,1.0,2,1
1116,should it be |D|? It's not clear to me that the sum of frequencies would be bounded if D is not discrete.,pol,none,0.0,1,0
1117,- Your F and \tilde{f} are introduced as infinite series.,pol,pol_negative,1.0,0,1
1118,"Maybe they should be {f1, f2,..., fN}, i.e. N queries, each of which you are trying to be estimate.",pol,none,0.0,1,0
1119,"- In general, you have to introduce the notation much more carefully.",pol,pol_negative,1.0,1,1
1120,Your audience should not be expected to be experts in hashing for this venue!,pol,none,0.0,1,0
1121,!,pol,none,0.0,0,0
1122,"'C[1,...,B]' is informal abusive notation.",pol,pol_negative,1.0,1,1
1123,You should clearly state using both mathematical notation AND using sentences what each symbol means.,pol,pol_negative,1.0,1,1
1124,"My understanding is that that h:U->b, is a function from universe U to natural number b, where b is an element from the discrete set {1,...,B}, to be used as an index for vector C. The algorithm maintains this vector C\in N^B (ie C is a B-length vector of natural numbers).",pol,none,0.0,0,0
1125,"In other words, h is mapping a varying-length sequence from U to an *index* of the vector C (a.k.a: a bin).",pol,none,0.0,0,0
1126,"Thus C[b] denotes the b-th element/bin of C, and C[h(i)] denotes the h(i)-th element.",pol,none,0.0,0,0
1127,"- Still it is unclear where 'fj' comes from. You need to state in words eg ""C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum.""",pol,pol_negative,1.0,1,1
1128,"- What I don't understand is how fj is dependent on h. When you say ""at the end of the stream"", you mean that given S, we are analyzing the frequency of a series of sequences {i_1,...,i_N}?",pol,pol_negative,1.0,1,1
1129,"- Sorry, it's just confusing and I didn't really understand ""Single Hash Function"" from Sec 3.2 until I started typing this out.",pol,none,0.0,1,0
1130,"- The term ""sketch"" is used in Algorithm1, like 10, before 'sketch' is defined!!",pol,pol_negative,1.0,1,1
1131,"-I'm not going to trudge through the proofs, because I don't think this is self-contained (and I'm clearly not an expert in the area).",pol,pol_negative,1.0,1,1
1132,Conclusion:,pol,none,0.0,0,0
1133,"Honestly, this paper is very difficult to follow.",pol,pol_negative,1.0,1,1
1134,"However to sum up the idea: you want to use deep learning techniques to learn some prior on the hash-estimation problem, in the form of a heavy-hitter oracle.",pol,none,0.0,0,0
1135,"It seems interesting and shows promising results, but the presentation has to be cleaned up for publication in a top ML venue.",pol,pol_positive,3.0,1,3
1136,******,pol,none,0.0,0,0
1137,Update after response:,pol,none,0.0,0,0
1138,"The authors have provided improvements to the introduction of the problem setting, satisfying most of my complaints from before. I am raising my score accordingly, since the paper does present some novel results.",pol,none,0.0,3,0
1139,Pros:,pol,none,0.0,0,0
1140,- The finding that SVHN has larger likelihood than CIFAR according to networks is interesting.,pol,pol_positive,3.0,3,3
1141,"- The empirical and theoretical analyses are clear, seem thorough, and make sense.",pol,pol_positive,3.0,3,3
1142,- Section 5 can provide some insight when the model is too rigid and too log-concave (e.g. Gaussian).,pol,pol_positive,3.0,2,3
1143,Cons:,pol,none,0.0,0,0
1144,"- The premises of the analyses are not very convincing, limiting the significance of the paper.",pol,pol_negative,1.0,1,1
1145,"- In particular, Section 4 is a series of empirical analyses, based on one dataset pair.",pol,pol_negative,1.0,0,1
1146,"In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain.",pol,pol_negative,1.0,1,1
1147,- It is good that Section 5 has some theoretical analysis. But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images.,pol,pol_negative,1.0,1,1
1148,At least this limitation should be pointed out in the paper.,pol,pol_negative,1.0,2,1
1149,- Some parts of the paper feel long-winded and aimless.,pol,pol_negative,1.0,1,1
1150,[Quality],pol,none,0.0,0,0
1151,See above pros and cons.,pol,none,0.0,0,0
1152,A few less important disagreement I have with the paper:,pol,none,0.0,0,0
1153,- I don't think Glow necessarily is encouraged to increase sensitivity to perturbations.,pol,pol_negative,1.0,1,1
1154,"The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.",pol,pol_negative,1.0,1,1
1155,"- Figure 6(a) clearly suggests that the data mean for SVHN and CIFAR are very different, instead of similar.",pol,pol_negative,1.0,1,1
1156,[Clarity],pol,none,0.0,0,0
1157,"In general, the paper is clear and easy to understand given enough reading time, but feels at times long-winded.",pol,pol_neutral,2.0,1,2
1158,Section 2 background takes too much space.,pol,pol_negative,1.0,1,1
1159,"Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.",pol,pol_negative,1.0,1,1
1160,"Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.",pol,pol_negative,1.0,1,1
1161,A few editorial issues:,pol,none,0.0,0,0
1162,"- On page 4 footnote 2, as far as I know the paper did not define BPD.",pol,pol_negative,1.0,1,1
1163,"- There are two lines of text between Fig. 4 and Fig. 5, which is confusing.",pol,pol_negative,1.0,1,1
1164,[Originality],pol,none,0.0,0,0
1165,"I am not an expert in this specific field (analyzing generative models), but I believe this analysis is novel.",pol,pol_positive,3.0,3,3
1166,"However, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite:",pol,pol_negative,1.0,1,1
1167,Vít Škvára et al. Are generative deep models for novelty detection truly better?,pol,none,0.0,2,0
1168,"^ at first glance, their AUROC is never under 0.5, indicating that this phenomenon did not appear in their experiments although a lot of inlier-novelty pairs are tried.",pol,none,0.0,1,0
1169,A part of the paper's contribution (section 5 conclusion) seem to overlap with others' work.,pol,pol_negative,1.0,1,1
1170,"The section concludes that if the second dataset has small variances, it will get higher likelihood.",pol,none,0.0,0,0
1171,But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).,pol,pol_negative,1.0,1,1
1172,[Significance],pol,none,0.0,0,0
1173,The paper has a very interesting finding; pointing out and in-depth analysis of negative results should benefit the community greatly.,pol,pol_positive,3.0,3,3
1174,"However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis.",pol,pol_negative,1.0,1,1
1175,"According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that ""lies within"" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test?",pol,pol_negative,1.0,2,1
1176,"Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.",pol,pol_negative,1.0,1,1
1177,"The paper presents a method of learning representations that is based on minimizing ""deficiency"" rather than optimizing for information sufficiency.",pol,none,0.0,0,0
1178,"While perfect optimization of the sufficiency term in IB is equivalent to minimizing deficiency, the thesis of the paper is that the variational upper bound on deficiency is easier to optimize, and when optimized produces",pol,none,0.0,0,0
1179,"better (more compressed representations), while performing equally on test accuracy.",pol,none,0.0,0,0
1180,The paper is well written and easy to read.,pol,pol_positive,3.0,3,3
1181,"The idea behind the paper (optimizing for minimizing deficiency instead of sufficiency in IB) is interesting, especially because the variational formulation of DB is a generalization of VIB (in that VIB reduces to VDB for M=1).",pol,pol_positive,3.0,3,3
1182,"What takes away from the paper is that while perfect optimization of IB/sufficiency is equivalent to perfect optimization of DB, it is not clear what happens when perfection is not achieved.",pol,pol_negative,1.0,1,1
1183,"Further, the authors claim that DB is able to obtain more compressed representations (But is the goal a compressed representation, or an informative one?).",pol,pol_positive,3.0,1,3
1184,"The paper would also benefit from evaluation of the representation itself, and comparison to other non-information bottleneck based algorithms.",pol,pol_positive,3.0,2,3
1185,This paper studies the problem of learning from multiple tasks and additional noisy data.,pol,none,0.0,0,0
1186,The proposed representation learning method first assigns each noisy data a relevance score using the topological information.,pol,none,0.0,0,0
1187,Then the authors propose to minimize a combination of the loss of a class-prototype learning loss and a cosine classifier learning loss to learn a good representation generator g_theta.,pol,none,0.0,0,0
1188,The empirical study validates the effectiveness of the proposed method.,pol,none,0.0,3,0
1189,"I have the following comments,",pol,none,0.0,0,0
1190,"1. The studied problem that learning from few-shot data and large-scale noisy data is interesting. According to the experimental results, the proposed method seems to be promising.",pol,pol_positive,3.0,3,3
1191,2. The learning procedure is confusing.,pol,pol_negative,1.0,1,1
1192,It is highly recommended to provide the pseudocode of the proposed method.,pol,pol_negative,1.0,0,1
1193,"3. Since there are many tasks and each task has a large-scale data, I'm afraid that the running time will explode. How to deal with this issue?",pol,pol_negative,1.0,2,1
1194,The paper considers 'replica exchange' Langevin dynamics.,pol,none,0.0,0,0
1195,"These methods are very popular among practitioners, and developing some theory backing the empirical successes is an important goal.",pol,none,0.0,0,0
1196,Unfortunately this paper offers only weak results.,pol,pol_negative,1.0,1,1
1197,- The first 6 pages set up the general formalism. This is textbook material adapted to the current problem.,pol,pol_negative,1.0,0,1
1198,"- Page 7 offers a result (expression for the Dirichlet form), which is hardly more than an exercise for anybody familiar with Markov Chains theory.",pol,pol_negative,1.0,1,1
1199,- Page 8 gives a Poincare inequality.,pol,none,0.0,0,0
1200,"Again, this follows from known results.",pol,pol_negative,1.0,0,1
1201,More importantly: (1) It does not show any advantage of replica exchange over standard dynamics; (2) It does not provide any quantitative insight for high-dimensional problems.,pol,pol_negative,1.0,1,1
1202,- Similar comments hold for the following pages.,pol,none,0.0,0,0
1203,"They are an exercise in applying standard formalism to this problem, without really showing any significative advantage of replica exchange.",pol,pol_negative,1.0,0,1
1204,Summary,pol,none,0.0,0,0
1205,This paper derives a new policy gradient method for when continuous actions are transformed by a,pol,none,0.0,0,0
1206,"normalization step, a process called angular policy gradients (APG).",pol,none,0.0,0,0
1207,A generalization based on,pol,none,0.0,0,0
1208,a certain class of transformations is presented.,pol,none,0.0,0,0
1209,The method is an instance of a,pol,none,0.0,0,0
1210,Rao-Blackwellization process and hence reduces variance.,pol,none,0.0,0,0
1211,Detailed comments,pol,none,0.0,0,0
1212,"I enjoyed the concept and, while relatively niche, appreciated the work done here and do believe it has clear applications.",pol,pol_positive,3.0,3,3
1213,I am not convinced that the measure theoretic perspective is always,pol,pol_negative,1.0,1,1
1214,"necessary to convey the insights, although I appreciate the desire for technical correctness. Still,",pol,none,0.0,1,0
1215,"appealing to measure theory does reduces readership, and I encourage the authors to keep this in",pol,none,0.0,1,0
1216,mind as they revise the text.,pol,none,0.0,0,0
1217,Generally speaking it seems like a lot of technicalities for a relatively simple result:,pol,pol_negative,1.0,0,1
1218,marginalizing a distribution onto a lower-dimensional surface.,pol,none,0.0,0,0
1219,"The paper positions itself generally as dealing with arbitrary transformations T, but really is",pol,pol_negative,1.0,1,1
1220,about angular transformations (e.g. Definition 3.1).,pol,none,0.0,0,0
1221,The generalization is relatively,pol,pol_negative,1.0,1,1
1222,straightforward and was not too surprising given the APG theory.,pol,none,0.0,3,0
1223,The paper would gain in clarity,pol,pol_negative,1.0,0,1
1224,if its scope was narrowed.,pol,none,0.0,0,0
1225,"It's hard for me to judge of the experimental results of section 5.3, given that there are no other",pol,pol_negative,1.0,1,1
1226,"benchmarks or provided reference paper. As a whole, I see APG as providing a minor benefit over PG.",pol,none,0.0,0,0
1227,"Def 4.4: ""a notion of Fisher information"" -- maybe ""variant"" is better than ""notion"", which implies there are different kinds of Fisher information",pol,pol_neutral,2.0,1,2
1228,Def 3.1 mu is overloaded: parameter or measure?,pol,pol_neutral,2.0,2,2
1229,"4.4, law of total variation -- define",pol,pol_negative,1.0,2,1
1230,Overall,pol,none,0.0,0,0
1231,"This was a fun, albeit incremental paper.",pol,pol_negative,1.0,0,1
1232,"The method is unlikely to set new SOTA, but I appreciated",pol,pol_neutral,2.0,3,2
1233,the appeal to measure theory to formalize some of the concepts.,pol,none,0.0,0,0
1234,Questions,pol,none,0.0,0,0
1235,What does E_{pi|s} refer to in Eqn 4.1?,pol,pol_neutral,2.0,2,2
1236,Can you clarify what it means for the map T to be a sufficient statistic for theta? (Theorem 4.6),pol,pol_neutral,2.0,2,2
1237,Experiment 5.1: Why would we expect APG with a 2d Gaussian to perform better than a 1d Gaussian,pol,pol_neutral,2.0,1,2
1238,on the angle?,pol,none,0.0,2,0
1239,Suggestions,pol,none,0.0,0,0
1240,Paragraph 2 of section 3 seems like the key to the whole paper -- I would make it more prominent.,pol,none,0.0,2,0
1241,I would include a short 'measure theory' appendix or equivalent reference for the lay reader.,pol,pol_neutral,2.0,2,2
1242,I wonder if the paper's main aim is not actually to bring measure theory to the study of policy,pol,pol_neutral,2.0,2,2
1243,"gradients, which would be a laudable goal in and of itself.",pol,none,0.0,0,0
1244,ICLR may not in this case be the right,pol,pol_negative,1.0,1,1
1245,venue (nor are the current results substantial enough to justify this) but I do encourage authors,pol,none,0.0,0,0
1246,to,pol,none,0.0,0,0
1247,"consider this avenue, e.g. in a journal paper.",pol,none,0.0,0,0
1248,= Revised after rebuttal =,pol,none,0.0,0,0
1249,I thank the authors for their response.,pol,none,0.0,0,0
1250,"I think this work deserves to be published, in particular because it presents a reasonably straightforward result that others will benefit from.",pol,pol_positive,3.0,0,3
1251,"However, I do encourage further work to",pol,none,0.0,0,0
1252,1) Provide stronger empirical results (these are not too convincing).,pol,pol_negative,1.0,3,1
1253,"2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators.",pol,pol_negative,1.0,1,1
1254,The paper proposed a new pipelined training strategy to fully utilize the memory and computational power to speed up the training process.,pol,none,0.0,0,0
1255,"In order to overcome the generalization degradation of the proposed method, the authors further introduced the so-called hybrid method to combine their proposed pipelined method and normal training.",pol,none,0.0,0,0
1256,The pipelined method is interesting.,pol,pol_positive,3.0,3,3
1257,"For the pipelined process itself, it is similar to model parallelization.",pol,none,0.0,0,0
1258,"For the method proposed by the paper,  it is like the async-SGD method.",pol,none,0.0,0,0
1259,"The paper merged these two ideas together but did not solve the problem from async-SGD, i.e. with a large number of processes, the generalization performance degrades (in the paper, it is so-called ""stages"").",pol,none,0.0,1,0
1260,"Even with the hybrid method, the accuracy still drops.",pol,pol_negative,1.0,1,1
1261,"Also, the sentence, ""We demonstrate the implementation and performance of our pipelined backpropagation in PyTorch on 2 GPUs using ResNet, achieving speedups of up to 1.8X over a 1-GPU baseline, with a small drop in inference accuracy."", is confusing. If I use data parallelization, the gain should be also around 2.",pol,pol_negative,1.0,1,1
1262,The ResNet on Cifar-10 results are not convincing.,pol,pol_negative,1.0,1,1
1263,The normal accuracy of ResNet20 on Cifar-10 is around 92 but the paper reported 91.1%.,pol,none,0.0,0,0
1264,"Based on this, I think the paper has some room for improvement.",pol,none,0.0,0,0
1265,The privacy definition employed in this work is problematic.,pol,pol_negative,1.0,1,1
1266,"The authors claim that ""Privacy can be quantified by the difficulty of reconstructing raw data via a generative model"".",pol,none,0.0,0,0
1267,This is not justified sufficiently.,pol,pol_negative,1.0,1,1
1268,Why larger reconstruction error achieves stronger privacy protection? I could not find any formal relationship between reconstruction error and privacy.,pol,pol_negative,1.0,1,1
1269,The proposed method is not appropriately compared with the other methods in experiments.,pol,pol_negative,1.0,1,1
1270,In Fig. 3 the author claim that the proposed method dominates the other methods in terms of privacy and utility but this is not correct.,pol,pol_negative,1.0,1,1
1271,"At the specific point that the proposed method is evaluated with MNIST and Sound, it achieves better utility and better ""privacy"".",pol,none,0.0,0,0
1272,"However, the Pareto front of the proposed method is concentrated on a specific point.",pol,pol_negative,1.0,0,1
1273,"For example, the proposed method does not achieve high ""privacy"" as ""noisy"" does.",pol,pol_negative,1.0,1,1
1274,"In this sense, the proposed method is not comparable with ""noisy"".",pol,pol_negative,1.0,1,1
1275,"In my understanding, this concentration occurs because the range of \lambda is inappropriately set.",pol,none,0.0,0,0
1276,This kind of regularization parameter should be exponentially varied so that the privacy-utility Pareto front covers a wide range.,pol,pol_neutral,2.0,1,2
1277,--,pol,none,0.0,0,0
1278,Minor:,pol,none,0.0,0,0
1279,"In Eq. 1, the utility is evaluated as the probability Yi=Yi'.",pol,none,0.0,0,0
1280,What randomness is considered in this probability?,pol,pol_neutral,2.0,1,2
1281,"In Eq 2, privacy is defined as maxmin of |Ii - Ii'|.",pol,none,0.0,0,0
1282,Do you mean privacy guaranteed by the proposed method is different for each data? This should be defined as expectation over T or max over T.,pol,pol_neutral,2.0,2,2
1283,"In page 4. ""The reason we choose this specific architecture is that an exactly reversed mode is intuitively the mode powerful adversarial against the Encoder."" I could not find any justification for this setting. Why ""exactly reversed mode"" can be the most powerful adversary? What is an exactly reversed mode?",pol,pol_neutral,2.0,1,2
1284,Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously.,pol,pol_negative,1.0,1,1
1285,The resulting model would thus be highly affected by the setting of n and k.,pol,none,0.0,1,0
1286,How can you choose k and n?,pol,pol_neutral,2.0,1,2
1287,1) Summary,pol,none,0.0,0,0
1288,This paper proposes a method for learning an agent by interacting and probing an expert agents behavior.,pol,none,0.0,0,0
1289,"This method is composed of a policy that learns to imitate an expert’s action, and a policy that challenges the expert in order to get it to take multiple possible routes to solve a task.",pol,none,0.0,0,0
1290,"The two policies share a “behavior tracker” that models the expert’s behavior, and communicates it to both policies being learned.",pol,none,0.0,0,0
1291,The probing policy is optimized using a curiosity-driven reward in order to get the expert take trajectories the probing policy has not seen before.,pol,none,0.0,0,0
1292,"In experiments, the authors perform experiments to show how the learned agent can generalize to unseen configurations in the corresponding environments in which the agents were trained, and also use the proposed technique in a sorting task in which the method generalizes to longer arrays to be sorted.",pol,none,0.0,0,0
1293,2) Pros:,pol,none,0.0,0,0
1294,+ Neat idea for exploring an experts behavior by changing the environment surrounding it (probing it).,pol,pol_positive,3.0,3,3
1295,+ Cool experiments for applicability.,pol,pol_positive,3.0,3,3
1296,+ Well written paper and easy to understand.,pol,pol_positive,3.0,3,3
1297,3 Comments:,pol,none,0.0,0,0
1298,- Equation 1 typo?:,pol,pol_neutral,2.0,2,2
1299,"To my understanding, in curiosity driven exploration, the exploration is driven based on how well the next state can be predicted by the agent.",pol,none,0.0,0,0
1300,"In equation 1, different time steps are being compared, m^t and m^{t-1}, but the comparison should be between the predicted time step t and real time step t. Can the authors clarify why different time steps are compared in the equation?",pol,pol_neutral,2.0,2,2
1301,- Baseline missing: Random actions from expert,pol,pol_negative,1.0,1,1
1302,A simple baseline to compare against could be to simply force the expert to take a few random actions during its trajectory and let the imitator learn from these.,pol,pol_neutral,2.0,1,2
1303,Comparing against this baseline could serve as evidence that we need to actually learn the probing agent to acquire a more optimal policy.,pol,pol_neutral,2.0,1,2
1304,- Baseline missing: Simple RNN policies that communicate hidden states.,pol,pol_negative,1.0,1,1
1305,Another baseline could be to simply model the imitator and probing policies as RNNs and let them communicate with each other via the hidden states.,pol,pol_negative,1.0,1,1
1306,While optimizing the curiosity reward the hidden states could be used as well.,pol,pol_neutral,2.0,1,2
1307,"If successful, this baseline can show that we actually need to model the “behavior” with a separate network.",pol,pol_neutral,2.0,1,2
1308,- Ablation study for the importance of fusion:,pol,none,0.0,0,0
1309,The authors have a “fusion” layer within the imitator and probing policies.,pol,none,0.0,0,0
1310,An ablation study showing that this layer is actually necessary is missing from the paper.,pol,pol_negative,1.0,1,1
1311,- Generalizability argument,pol,none,0.0,1,0
1312,"The authors claim that they show a single starting configuration for the agents during training, and different starting configurations during testing.",pol,none,0.0,0,0
1313,"While I agree with this to some extent, I also think this argument may not be fully right. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment.",pol,pol_negative,1.0,1,1
1314,"It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert.",pol,pol_negative,1.0,0,1
1315,A more drastic change of the environment could make for a stronger argument.,pol,pol_negative,1.0,1,1
1316,4) Conclusion:,pol,none,0.0,0,0
1317,"Overall, I like the idea of having a policy that tries to figure out the general behavior of a demonstrator by probing it.",pol,pol_positive,3.0,3,3
1318,"Having said that, I feel this paper needs to improve in the aspects mentioned above.",pol,pol_positive,3.0,1,3
1319,"If the authors present more convincing evidence that successfully address the comments above, I am willing to increase my score.",pol,pol_positive,3.0,0,3
1320,The authors proposed a Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer.,pol,none,0.0,0,0
1321,The authors define timbre transfer as applying parts of the auditory properties of a musical instrument onto another.,pol,none,0.0,0,0
1322,It replaces the usual adversarial translation criterion by a Maximum Mean Discrepancy (MMD) objective.,pol,none,0.0,0,0
1323,"By further conditioning our system on several different instruments, the proposed method can generalize to many-to-many transfer within a single variational architecture able to perform multi-domain transfers.",pol,none,0.0,0,0
1324,"Some detailed comments are listed as follow,",pol,none,0.0,0,0
1325,1 The implementation steps of the proposed method (MoVE) are not clear.,pol,pol_negative,1.0,1,1
1326,"Some details are missing, which is hardly reproduced by the other researchers.",pol,pol_negative,1.0,1,1
1327,2 The experimental settings are not reasonable.,pol,pol_negative,1.0,1,1
1328,The current experimental settings are not matched with the practice environment.,pol,pol_negative,1.0,1,1
1329,3 The proposed method can transfer the positive knowledge.,pol,none,0.0,3,0
1330,"However, some negative knowledge information can be also transferred.",pol,none,0.0,0,0
1331,So how to avoid the negative transferring?,pol,pol_neutral,2.0,2,2
1332,"4 For the model, the optimization details or inferring details are missing, which are important for the proposed model.",pol,pol_neutral,2.0,1,2
1333,I want thank the authors for solving this long-standing GAN challenge in raw waveform synthesis.,pol,none,0.0,0,0
1334,"With all due respect, previous GAN trials for audio synthesis are inspiring, but their audio qualities are far away from the state-of-the-art results.",pol,none,0.0,1,0
1335,"Although the speech fidelity of GAN-TTS is still worse than WaveNet and Parallel WaveNet from the posted sample, it has begun to close the significant performance gap that has existed between autoregressive models and GANs for raw audios.",pol,pol_positive,3.0,0,3
1336,"Overall, this is a very good paper with significant contributions to the filed.",pol,pol_positive,3.0,3,3
1337,Detailed comment:,pol,none,0.0,0,0
1338,"1, In WaveNet, the conditional features (linguistic / mel-spectrogram) are added as bias terms in the convolutional layers.",pol,none,0.0,0,0
1339,"Did the authors tried this alternative architecture for the generator, which uses the white noisy z as network input (similar as flow-based models, e.g., Parallel WaveNet) and the conditional features as bias term in the convolutional layers?",pol,pol_neutral,2.0,2,2
1340,"2, Could the authors comment the importance of serval architecture choices in this work?",pol,pol_neutral,2.0,2,2
1341,"From Table 1, it seems to me that the ensemble of random window discriminators is the most important (perhaps the only important) contributing factor for the success.",pol,none,0.0,0,0
1342,"For example, the MOS score was boosted from 1.889 to 4.213 by replacing a single full discriminator to the ensemble of RWDs.",pol,none,0.0,0,0
1343,"3, The notations in Eq. (1) and (2) are messy.",pol,pol_negative,1.0,1,1
1344,"Although I can figure their meaning from the context, one may clarify certain notations if they appear at the first time.",pol,pol_negative,1.0,1,1
1345,"4, The stable training (NO model collapses) is pretty impressive.",pol,pol_positive,3.0,3,3
1346,Could the authors shed some light on the potential reason? Does the ensemble of RWD regularizes the training?,pol,pol_neutral,2.0,2,2
1347,What's your experience for training FullD (does not have random window ) and cRWD_1 (only has one random window discriminator),pol,pol_neutral,2.0,1,2
1348,?,pol,none,0.0,0,0
1349,Are they still very stable?,pol,pol_neutral,2.0,2,2
1350,"Also, could the authors comment on the importance of large batch size -- 1024 for stable training of GAN-TTS?",pol,pol_neutral,2.0,2,2
1351,"5, Although there is a notable difference, one may properly mention previous work Yamamoto et al. (2019), which uses GAN as an auxiliary loss within ClariNet and obtains high-fidelity speech ( https://r9y9.github.io/demos/projects/interspeech2019/ ).",pol,pol_negative,1.0,1,1
1352,Yamamoto et al. Probability Density Distillation with Generative Adversarial Networks for High-Quality Parallel Waveform Generation.,pol,none,0.0,0,0
1353,2019.,pol,none,0.0,0,0
1354,=== update ===,pol,none,0.0,0,0
1355,Thank you for the detailed response.,pol,none,0.0,0,0
1356,"2,  Thanks for the elaboration.",pol,none,0.0,0,0
1357,"4,  It would be very interesting to see an analysis of model stability with smaller batch sizes.",pol,pol_neutral,2.0,2,2
1358,"The authors study the generalization error of two-layer neural nets, where an asymptotic point of view is taken.",pol,none,0.0,0,0
1359,Their main results can be summarized as follows.,pol,none,0.0,0,0
1360,"1. If only the second layer is optimized, they observe the double-descent phenomenon.",pol,none,0.0,0,0
1361,"2. However, if only the first layer is optimized, the double-descent is not observed.",pol,none,0.0,1,0
1362,"This shows that recent results for certain linear models (e.g. Song, Montanari 2019) do not directly transfer to neural networks.",pol,none,0.0,0,0
1363,"As the authors point out, however, if a different scaling is used in the asymptotics, double descent might still be observed.",pol,none,0.0,0,0
1364,I see the following strengths of the paper.,pol,none,0.0,0,0
1365,-This is a very well-written paper with a clear message.,pol,pol_positive,3.0,3,3
1366,-The result is important and gives new insights into the generalization properties of neural networks.,pol,pol_positive,3.0,3,3
1367,"In my view, this is an interesting contribution, which should be accepted.",pol,pol_positive,3.0,3,3
1368,---------,pol,none,0.0,0,0
1369,Thank you for your response. I will leave the rating unchanged.,pol,none,0.0,0,0
1370,The authors suggest a method to create combined low-dimensional representations for combinations of pairs of words which have a specific syntactic relationship (e.g. adjective - noun).,pol,none,0.0,0,0
1371,"Building on the generative word embedding model provided by Arora et al. (2015), their solution uses the core tensor from the Tucker decomposition of a 3-way PMI tensor to generate an additive term, used in the composition of two word embedding vectors.",pol,none,0.0,0,0
1372,"Although the method the authors suggest is a plausible way to explicitly model the relationship between syntactic pairs and to create a combined embedding for them, their presentation does not make this obvious and it takes effort to reach the conclusion above.",pol,pol_negative,1.0,1,1
1373,"Unlike Arora's original work, the assumptions they make on their subject material are not supported enough, as in their lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by their method makes this a good idea.",pol,pol_negative,1.0,1,1
1374,"Though the title promises a contribution to an understanding of word embedding compositions in general, they barely expound on the broader implications of their idea in representing elements of language through vectors.",pol,pol_negative,1.0,1,1
1375,Their lack of willingness to ground their claims or decisions is even more apparent in two other cases.,pol,pol_negative,1.0,1,1
1376,The authors claim that the Arora's RAND-WALK model does not capture any syntactic information.,pol,pol_negative,1.0,0,1
1377,This is not true.,pol,none,0.0,1,0
1378,"The results presented by Arora et al. indeed show that RAND-WALK captures syntactic information, albeit to a lesser extent than other popular methods for word embedding (Table 1, Arora et al. 2015).",pol,none,0.0,0,0
1379,Another unjustified choice by the authors is their choice of weighing the Tensor term (when it is being added to two base embedding vectors) in the phrase similarity experiment.,pol,pol_negative,1.0,0,1
1380,The reason the authors provide for weighing the composition Tensor is the fact that in the unweighted version their model produced a worse performance than the additive composition.,pol,pol_neutral,2.0,1,2
1381,One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general.,pol,pol_negative,1.0,1,1
1382,"Arora's generative model for word embeddings, on which the current paper is largely based upon, not only make the mathematical relationship among different popular word embedding methods explicit, but also by making and verifying explicit assumptions with regard to properties of the word embeddings created by their model, they are able to explain why low-dimensional embeddings provide superior performance in tasks that implicate semantic relationships as linear algebraic relations.",pol,none,0.0,0,0
1383,"Present work, however interesting with regard to its potential implications, strays away from providing such theoretical insights and suffices with demonstrating limited improvements in empirical tasks.",pol,pol_negative,1.0,1,1
1384,"The authors consider the scenario of two agents, a demonstrator acting in an environment to achieve a goal, and a learner, which can also interact with the environment, but whose goal is to learn the demonstrator’s policy by carrying out actions eliciting strong changes in the demonstrator’s trajectory.",pol,none,0.0,0,0
1385,"The former is implemented as imitation learning, i.e. policy learning, the latter as curiosity driven RL.",pol,none,0.0,0,0
1386,"The authors are encouraged to review some of the related literature on optimal teaching, which also has developed a rich set of approaches to agent modeling, e.g. the work by Patrick Shafto.",pol,pol_neutral,2.0,2,2
1387,It may also be relevant to think about the relationship to active learning in IRL.,pol,pol_neutral,2.0,2,2
1388,I am not sure whether I would be able to implement and reproduce the presented work on the basis of the current manuscript including the appendix.,pol,pol_negative,1.0,1,1
1389,It would be very helpful for the community to be able to do so.,pol,pol_neutral,2.0,2,2
1390,"E.g., details on the the training of the demonstrators, their reward functions, and the behavior tracker.",pol,pol_neutral,2.0,0,2
1391,"Particularly the ""fusion"" module remains extremely unclear.",pol,pol_negative,1.0,1,1
1392,"Overall, this is a nice paper, despite the fact that the example domains and problems considered are engineered strongly to allow for the proposed algorithm to be useful.",pol,pol_neutral,2.0,3,2
1393,"Particularly for the claim of generalization to different environments, the details are all in the engineering of the particular grid world tasks, how they relate to each other and the sate representation used for the demonstrator s_d.",pol,pol_neutral,2.0,0,2
1394,"I am not sure why it was submitted to ICLR and not the Annual Meeting of the Cognitive Science Society, though.",pol,pol_negative,1.0,1,1
1395,Minor points:,pol,none,0.0,0,0
1396,“differs from this in two folds”,pol,pol_neutral,2.0,0,2
1397,“by generate queries”,pol,pol_neutral,2.0,0,2
1398,"This paper presents a novel approach to bundle adjustment, where traditional geometric optimization is paired with deep learning.",pol,none,0.0,0,0
1399,"Specifically, a CNN computes both a multi-scale feature pyramid and a depth prediction, expressed as a linear combination of ""depth bases"".",pol,none,0.0,0,0
1400,"These values are used to define a dense re-projection error over the images, akin to that of dense or semi-dense methods.",pol,none,0.0,0,0
1401,"Then, this error is optimized with respect to the camera parameters and depth linear combination coefficients using Levenberg-Marquardt (LM).",pol,none,0.0,0,0
1402,"By unrolling 5 iterations of LM and expressing the dampening parameter lambda as the output of a MLP, the optimization process is made differentiable, allowing back-propagation and thus learning of the networks' parameters.",pol,none,0.0,0,0
1403,"The paper is clear, well organized, well written and easy to follow.",pol,pol_positive,3.0,3,3
1404,"Even if the idea of joining BA / SfM and deep learning is not new, the authors propose an interesting novel formulation.",pol,pol_positive,3.0,3,3
1405,"In particular, being able to train the CNN with a supervision signal coming directly from the same geometric optimization process that will be used at test time allows it to produce features that  will make the optimization smoother and the convergence easier.",pol,pol_positive,3.0,0,3
1406,The experiments are quite convincing and seem to clearly support the efficacy of the proposed method.,pol,pol_positive,3.0,3,3
1407,"I don't really have any major criticism, but I would like to hear the authors' opinions on the following two points:",pol,none,0.0,0,0
1408,"1) In page 5, the authors write ""learns to predict a better damping factor lambda, which gaurantees that the optimziation will converged to a better solution within limited iterations"".",pol,none,0.0,0,0
1409,I don't really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.,pol,pol_negative,1.0,1,1
1410,"The word ""guarantee"" usually implies that the effect can be somehow mathematically proved, which is not done in the paper.",pol,none,0.0,1,0
1411,"2) As far as I can understand, once the networks are learned, possibly on pairs of images due to GPU memory limitations, the proposed approach can be easily applied to sets of images of any size, as the features and depth predictions can be pre-computed and stored in main system memory.",pol,none,0.0,0,0
1412,"Given this, I wonder why all experiments are conducted on sets of two to five images, even for Kitti where standard evaluation protocols would demand predicting entire sequences.",pol,pol_negative,1.0,1,1
1413,This paper tackles vulnerability to poisoning.,pol,none,0.0,0,0
1414,An important subtopic of adversarial ML.,pol,none,0.0,0,0
1415,"The authors propose using a GAN to generate poisoning data points, as an alternative to existing methods.",pol,none,0.0,0,0
1416,"While most (or all) of the paper is devoted to illustrate the effectiveness of the approach against *non-protected* ML. My only and biggest concern with this paper is that no defense mechanism has been tested against, and there are many in the literature. (see e.g. Diakonikolas et al ICML 2019).",pol,pol_negative,1.0,1,1
1417,"Thus my question for the rebuttal period: How would pGAN perform when defense mechanisms are deployed during the learning phase? (ideally, a thorough experiment illustrating the strength of pGAN against a few defense mechanisms would help re-evaluating the score)",pol,pol_negative,1.0,1,1
1418,"edit: the authors added several experiments (better evaluation of the predicted lambda, comparison with CodeSLAM), which address my concerns.",pol,pol_positive,3.0,3,3
1419,I think the paper is much more convincing now. I am happy to increase my rating to clear accept.,pol,pol_positive,3.0,0,3
1420,"I also agree with the introduction of the Chi vector, and with the use of the term of ""photometric BA"", since it was used before, even if it is unfortunate in my opinion.",pol,pol_positive,3.0,0,3
1421,"I thank the authors to replace reprojection by alignment, which is much clearer.",pol,none,0.0,1,0
1422,---------------,pol,none,0.0,0,0
1423,This paper presents a method for dense Structure-from-Motion using Deep Learning:,pol,none,0.0,0,0
1424,The input is a set of images; the output is the camera poses and the depth maps for all the images.,pol,none,0.0,0,0
1425,The approach is inspired by Levenberg-Marquardt optimization (LM): A pipeline extracting image features computes the Jacobian of an error function.,pol,none,0.0,0,0
1426,This Jacobian is used to update an estimate of the camera poses.,pol,none,0.0,0,0
1427,"As in LM optimization, this update is done based on a factor lambda, weighting a gradient descent step and a Gauss-Newton step.",pol,none,0.0,0,0
1428,"In LM optimization, this lambda evolves with the improvement of the estimate.",pol,none,0.0,0,0
1429,Here lambda is also predicted using a network based on the feature difference.,pol,none,0.0,0,0
1430,"If I understand correctly, what is learned is how to compute image features that provide good updates, how to predict the depth maps from the features, and how to predict lambda.",pol,none,0.0,0,0
1431,The method is compared against DeMoN and other baselines with good results.,pol,pol_positive,3.0,3,3
1432,"I like the fact that the method is based on LM optimization, which is the standard method in 'geometric bundle adjustment', while related works consider Gauss-Newton-like optimization steps.",pol,pol_positive,3.0,3,3
1433,The key was to include a network to predict lambda as well.,pol,none,0.0,0,0
1434,"However, I have several concerns:",pol,none,0.0,0,0
1435,* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct.,pol,pol_negative,1.0,1,1
1436,The image features learned with the proposed method are re-used in an approach using a fixed lambda.,pol,none,0.0,0,0
1437,"If I understand correctly, there are 2 things wrong with that:",pol,none,0.0,0,0
1438,"- for GN optimization, lambda should be set to 0 - not a constant value.",pol,pol_negative,1.0,1,1
1439,Several constant values should also have been tried.,pol,pol_negative,1.0,2,1
1440,"- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.",pol,pol_negative,1.0,1,1
1441,"Thus, the advantage of using a LM optimization scheme is not very convincing.",pol,pol_negative,1.0,1,1
1442,"Since the LM-like approach is the main contribution, and the reported experiments do not show an advantage over GN-like approaches (already taken by previous work), this is my main reason for proposing rejection.",pol,pol_negative,1.0,0,1
1443,"* CodeSLAM (best paper at CVPR'18) is referenced but there is no comparison with it, while a comparison on the EuRoC dataset should be possible.",pol,pol_negative,1.0,1,1
1444,Less critical concerns that still should be taken into account if the paper is accepted:,pol,none,0.0,0,0
1445,"- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.",pol,pol_negative,1.0,1,1
1446,- the name 'Bundle Adjustment' is actually not adapted to the proposed method.,pol,pol_negative,1.0,1,1
1447,"'Bundle Adjustment' in 'geometric computer vision' comes from the optimization of several rays to intersect at the same 3D point, which is done by minimizing the reprojection errors.",pol,none,0.0,0,0
1448,Here the objective function is based on image feature differences.,pol,none,0.0,0,0
1449,I thus find the name misleading.,pol,pol_negative,1.0,1,1
1450,The end of Section 3 also encourages the reader to think that the proposed method is based on the reprojection error.,pol,none,0.0,0,0
1451,The proposed method is more about dense alignment for multiple images.,pol,none,0.0,0,0
1452,More minor points:,pol,none,0.0,0,0
1453,1st paragraph:,pol,none,0.0,0,0
1454,Marquet -> Marquardt,pol,pol_negative,1.0,2,1
1455,title of Section 3: revisitED,pol,pol_negative,1.0,0,1
1456,1st paragraph of Section 3: audience -> reader,pol,pol_negative,1.0,0,1
1457,caption of Fig 1: extractS,pol,pol_negative,1.0,0,1
1458,Eq (2) cannot have Delta Chi on the two sides.,pol,pol_negative,1.0,1,1
1459,"Typically, the left side should be \hat{\Delta \Chi}",pol,none,0.0,1,0
1460,before Eq (3): the 'photometric ..' -> a 'photometric ..',pol,pol_negative,1.0,2,1
1461,1st paragraph of Section 4.3: difficulties -> reason,pol,pol_negative,1.0,0,1
1462,typo in absolute in caption of Fig 4,pol,pol_negative,1.0,0,1
1463,Eq (6): Is B the same for all scenes?,pol,pol_neutral,2.0,2,2
1464,It would be interesting to visualize it.,pol,pol_neutral,2.0,2,2
1465,Section 4.5: applies -,pol,pol_negative,1.0,0,1
1466,> apply,pol,pol_negative,1.0,0,1
1467,This paper introduced a proximal approach to optimize neural networks by linearizing the network output instead of the loss function.,pol,none,0.0,0,0
1468,"They demonstrate their algorithm on multi-class hinge loss, where they can show that optimal step size can be computed in close form without significant additional cost.",pol,none,0.0,0,0
1469,Their experimental results showed competitive performance to SGD/Adam on the same network architectures.,pol,pol_positive,3.0,0,3
1470,1.,pol,none,0.0,0,0
1471,Figure 1 is crucial to the algorithm design as it aims to prove that Loss-Preserving Linearization (LPL) preserves information on loss function.,pol,pol_positive,3.0,0,3
1472,"While the authors provided numerical plots to compare it with the SGD linearization, I personally prefer to see some analytically comparsion between SGD linearization and LPL even on the simplest case.",pol,pol_negative,1.0,1,1
1473,An appendix with more numerical comparisons on other loss functions might also be insightful.,pol,pol_negative,1.0,2,1
1474,2. It seems LPL is mainly compared to SGD for convergence (e.g. Fig 2).,pol,none,0.0,0,0
1475,In Table 2 I saw some optimizers end up with much lower test accuracy.,pol,pol_negative,1.0,1,1
1476,Can the authors show the convergence plots of these methods (similar to Figure 2)?,pol,pol_neutral,2.0,2,2
1477,"This paper investigates an SGD variant (PowerSGD) where the stochastic gradient is raised to a power of $\gamma \in [0,1]$.  The authors introduce PowerSGD and PowerSGD with momentum (PowerSGDM).",pol,none,0.0,0,0
1478,The theoretical proof of  the convergence is given and experimental results show that the proposed algorithm converges faster than some of the existing popular adaptive SGD techniques.,pol,none,0.0,0,0
1479,"Intuitively, the proposed PowerSGD can boost the gradient (since $\gamma \in [0,1]$) so it may be helpful for the gradient of the lower layers of a deep network which may be hit by the vanishing gradient issue.",pol,none,0.0,0,0
1480,This may give rise to a faster convergence.,pol,none,0.0,0,0
1481,So overall the idea makes sense but I have the following concerns.,pol,none,0.0,0,0
1482,1.,pol,none,0.0,0,0
1483,The major issue I have with this paper is Theorem 3.1 on the ergodic convergence rate of the proposed PowerSGD.,pol,pol_negative,1.0,1,1
1484,"At the first glance, it is $O(\frac{1}{T})$ which is faster than the conventional SGD convergence rate $O(\frac{1}{\sqrt{T}})$.  But after a closer look, this rate is obtained by a very strong assumption on the batch size $B_{t}=T$.",pol,pol_negative,1.0,0,1
1485,"In other words, when the number of iterations is large, the batch size will be large too.",pol,pol_negative,1.0,0,1
1486,I consider this assumption unrealistic.,pol,pol_negative,1.0,1,1
1487,"Given that $T$ is typically very large (it is iterations, not epochs),  it will require a huge batch size, probably close to the whole training set.",pol,none,0.0,0,0
1488,"In this case, it is basically a GD, not SGD any more.",pol,pol_negative,1.0,1,1
1489,"That's why the rate is $O(\frac{1}{T})$, which is the convergence rate of GD.",pol,none,0.0,0,0
1490,I would like to see a convergence proof where the batch size $B_{t}$ is treated as a small constant like other SGD proofs assume.,pol,pol_negative,1.0,2,1
1491,Actually in the experiments the authors never use an increasing batch size.,pol,pol_negative,1.0,1,1
1492,"Instead, a constant batch size 128 is used.",pol,none,0.0,0,0
1493,"Therefore,  the faster convergence demonstrated in the experiments can not be explained by Theorem 3.1 or Theorem 3.2.",pol,pol_negative,1.0,1,1
1494,2. There are numerous inaccuracies in the proof given the supplementary material.,pol,pol_negative,1.0,1,1
1495,"For instance, in Eq.7,",pol,none,0.0,0,0
1496,$\nabla f(x) \sigma(\nabla f(x))$,pol,none,0.0,0,0
1497,"should be $\nabla f(x)^{T} \sigma(\nabla f(x))$   The random variable $\xi_{t}$ should be a scalar on training samples, not a vector, etc..  The authors should clean it up.",pol,pol_negative,1.0,1,1
1498,3. It would be helpful to show the $\gamma$ value on each experiment with different tasks.,pol,pol_negative,1.0,2,1
1499,It would be good to know how $\gamma$ varies across tasks.,pol,pol_negative,1.0,2,1
1500,"4. I think in the comparative experiments, the plain SGD should be added as another reference algorithm.",pol,pol_negative,1.0,2,1
1501,"5. The term ""PowerSGD"" seems to have been used by other papers.",pol,pol_negative,1.0,1,1
1502,This paper presents a classification method when the data consists of few clean labels and many noisy labels.,pol,none,0.0,0,0
1503,The authors propose to construct a graph structure within each class and use graph convolutional network to determine the clean/noisy labels of samples in each class.,pol,none,0.0,0,0
1504,"The model is based on a binary cross entropy loss function in each class, which learns the probability of labels to be clean. And such ""clean"" probability is used as the measure of relevance score between the sample different classes.",pol,none,0.0,0,0
1505,The idea of this paper is straightforward and the experimental results seem promising.,pol,pol_positive,3.0,3,3
1506,The authors compare with several related methods and show the proposed method has better performance in few shot learning experiments.,pol,pol_positive,3.0,0,3
1507,"For the motivation of this methods, why would the graph be constructed within each class? If there is correlation between different classes, how could the model use such class-wise correlation to clean the label?",pol,pol_neutral,2.0,1,2
1508,"Maybe I missed it, but how is the relevance score / predicted label determined for testing data given the graphs constructed in each class of training data?",pol,pol_neutral,2.0,1,2
1509,Authors use control theory to analyze and stabilize GAN's training.,pol,none,0.0,0,0
1510,"Their method, effectively, adds an L2 regularization to the output of the discriminator.",pol,none,0.0,0,0
1511,"I have some concerns regarding the novelty, analysis and also the experiments.",pol,none,0.0,1,0
1512,"- The analysis has focused on a very simple case of having a linear discriminator which for example in WGAN, forces the first moments to match. How does the analysis extend to more realistic cases?",pol,pol_negative,1.0,1,1
1513,"- In eq 9 in the dynamics of WGAN section, the discriminator should be restricted to Lip functions.",pol,none,0.0,1,0
1514,This has not been considered in the analysis.,pol,pol_negative,1.0,1,1
1515,- There are a few work in the literature that analyze local stability of GANs (e.g. https://arxiv.org/abs/1706.04156) as well as using some control theory for analyzing global stability of GANs (e.g. https://arxiv.org/abs/1710.10793).,pol,none,0.0,1,0
1516,The connections of the proposed approach with existing literature should be better explained.,pol,pol_negative,1.0,2,1
1517,"- In the empirical results, the performance of the proposed method and Reg-GAN (from the numerics of GAN paper) are quite similar. Are there instances that the proposed approach significantly improves the stability of practical GAN architectures?",pol,pol_negative,1.0,2,1
1518,Regularizing RKHS norm is a classic way to prevent overfitting.,pol,none,0.0,0,0
1519,The authors,pol,none,0.0,0,0
1520,note the connections between RKHS norm and several common regularization and,pol,none,0.0,0,0
1521,"robustness enhancement techniques, including gradient penalty, robust",pol,none,0.0,0,0
1522,optimization via PGD and spectral norm normalization.,pol,none,0.0,0,0
1523,They can be seen as upper,pol,none,0.0,0,0
1524,or lower bounds of the RKHS norm.,pol,none,0.0,0,0
1525,"There are some interesting findings in the experiments. For example, for",pol,pol_positive,3.0,0,3
1526,"improving generalization, using the gradient penalty based method seems to work",pol,none,0.0,3,0
1527,best.,pol,none,0.0,0,0
1528,"For improving robustness, adversarial training with PGD has the best",pol,none,0.0,0,0
1529,results (which matches the conclusions by Madry et al.); but as shown in Figure,pol,none,0.0,0,0
1530,"2,",pol,none,0.0,0,0
1531,"because adversarial training only decreases a lower bound of RKHS norm, it",pol,none,0.0,0,0
1532,does not necessarily decrease the upper bound (the product of spectral norms).,pol,none,0.0,0,0
1533,This can be shown as a weakness of adversarial training if the authors explore,pol,none,0.0,0,0
1534,further and deeper in this direction.,pol,none,0.0,0,0
1535,"Overall, this paper has many interesting results, but its contribution is",pol,pol_negative,1.0,1,1
1536,limited because:,pol,none,0.0,0,0
1537,1. The regularization techniques in reproducing kernel Hilbert space (RKHS) has,pol,pol_negative,1.0,0,1
1538,been well studied by previous literature.,pol,none,0.0,3,0
1539,This paper simply applies these,pol,none,0.0,0,0
1540,"results to deep neural networks, by treating the neural network as a big",pol,none,0.0,0,0
1541,black-box function f(x),pol,none,0.0,0,0
1542,.,pol,none,0.0,0,0
1543,Many of the results have been already presented in,pol,pol_negative,1.0,0,1
1544,previous works like Bietti & Mairal (2018).,pol,none,0.0,0,0
1545,"2. In experiments, the authors explored many existing methods on improving",pol,pol_negative,1.0,0,1
1546,generalization and robustness. However all these methods are known and not new.,pol,none,0.0,1,0
1547,"Ideally, the authors can go further and propose a new regularization method",pol,none,0.0,1,0
1548,"based on the connection between neural networks and RKHS, and conduct",pol,none,0.0,0,0
1549,experiments to show its effectiveness.,pol,none,0.0,0,0
1550,"The paper is overall well written, and the introductions to RKHS and each",pol,pol_positive,3.0,3,3
1551,regularization techniques are very clear.,pol,none,0.0,3,0
1552,The provided experiments also include,pol,pol_positive,3.0,0,3
1553,some interesting findings.,pol,none,0.0,3,0
1554,My major concern is the lack of novel contributions,pol,pol_negative,1.0,1,1
1555,in this paper.,pol,none,0.0,0,0
1556,"This paper presents a set of statistical tools, that are applicable to quantitatively measuring the mode collapse of GANs.",pol,none,0.0,0,0
1557,The authors consistently observe strong mode collapse on several state-of-the-art GANs using the proposed toolset.,pol,none,0.0,0,0
1558,"The authors analyze possible causes, and for the first time present two simple yet effective “black-box” methods to calibrate the GAN learned distribution, without accessing either model parameters or the original training data.",pol,none,0.0,0,0
1559,The writing and presentation are good.,pol,pol_positive,3.0,3,3
1560,My concerns regarding this paper are as below.,pol,none,0.0,0,0
1561,"1) I wonder if the proposed method work for most GAN models, more experiments evaluated on more recent GAN-based  models should be added to verify the superiority claimed in this paper, e.g., TP-GAN [Huang et al., ICCV 2017], PIM [Zhao et al., CVPR 2018], DR-GAN [Tran et al., CVPR 2017], DA-GAN [Zhao et al., NIPS 2017], MH-Parser [Li et al., 2017], 3D-PIM [Zhao et al., IJCAI 2018], SimGAN [Shrivastava et al., CVPR 2016], AIM [Zhao et al., AAAI 2019].",pol,pol_negative,1.0,2,1
1562,2) The main contributions of this paper are not quite clear to me.,pol,pol_negative,1.0,1,1
1563,"3) Typos need to be corrected in next version, e.g., all equations should have punctuation mark at the end, all e.g., i.e., et al., etc. should be italic, format of references should be consistent.",pol,pol_negative,1.0,1,1
1564,"Based on my comments above, I decide to give the rate of WA for this paper.",pol,none,0.0,0,0
1565,"Privacy concerns arise when data is shared with third parties, a common occurrence.",pol,none,0.0,0,0
1566,"This paper proposes a privacy-preserving classification framework that consists of an encoder that extracts features from data, a classifier that performs the actual classification, and a decoder that tries to reconstruct the original data.",pol,none,0.0,0,0
1567,"In a mobile computing setting, the encoder is deployed at the client side and the classification is performed on the server side which accesses only the output features of the encoder.",pol,none,0.0,0,0
1568,The adversarial training process guarantees good accuracy of the classifier while there is no decoder being able to reconstruct the original input sample accurately.,pol,none,0.0,3,0
1569,Experimental results are provided to confirm the usefulness of the algorithm.,pol,none,0.0,3,0
1570,"The problem of privacy-preserving learning is an important topic and the paper proposes an interesting framework for that. However, I think it needs to provide more solid evaluations of the proposed algorithm, and presentation also need to be improved a bit.",pol,pol_neutral,2.0,3,2
1571,Detailed comments:,pol,none,0.0,0,0
1572,I don’t see a significant difference between RAN and DNN in Figure 5. Maybe more explanation or better visualization would help.,pol,pol_negative,1.0,1,1
1573,The decoder used to measure privacy is very important. Can you provide more detail about the decoders used in all the four cases?,pol,pol_neutral,2.0,2,2
1574,"If possible, evaluating the privacy with different decoders may provide a stronger evidence for the proposed method.",pol,pol_neutral,2.0,0,2
1575,It seems that DNN(resized) is a generalization of DNN.,pol,none,0.0,0,0
1576,"If so, by changing the magnitude of noise and projection dimensions for PCA should give a DNN(resized) result (in Figure 3) that is close to DNN.",pol,none,0.0,1,0
1577,"If the two NNs used in DNN and DNN(resized) are different, I believe it’s still possible to apply the algorithm in DNN(resized) to the NN used in DNN, and get a full trace in the figure as noise and projection changes, which would lead to more fair comparison.",pol,pol_negative,1.0,1,1
1578,The abstract mentioned that the proposed algorithm works as an “implicit regularization leading to better classification accuracy than the original model which completely ignores privacy”. But I don’t see clearly from the experimental results how the accuracy compares to a non-private classifier.,pol,pol_negative,1.0,1,1
1579,Section 2.2 mentioned how different kind of layers would help with the encoder’s utility and privacy. It would be better to back up the argument with some experiments.,pol,pol_negative,1.0,1,1
1580,I think it needs to be made clearer how reconstruction error works as a measure of privacy.,pol,pol_negative,1.0,1,1
1581,"For example, an image which is totally unreadable for human eye might still leak sensitive information when fed into a machine learning model.",pol,none,0.0,1,0
1582,"In term of reference, it’s better to cite more articles with different kind of privacy attacks for how raw data can cause privacy risks.",pol,pol_neutral,2.0,1,2
1583,"For the “Noisy Data” method, it’s better to cite more articles on differential privacy and local differential privacy.",pol,pol_neutral,2.0,1,2
1584,"Some figures, like Figure 3 and 4, are hard to read.",pol,pol_negative,1.0,1,1
1585,"The author may consider making the figures larger (maybe with a 2 by 2 layout), adjusting the position of the legend & scale of x-axis for Figure 3, and using markers with different colors for Figure 4.",pol,pol_neutral,2.0,2,2
1586,"This work addresses the important problem of generation bias and a lack of diversity in generative models, which is often called model collapse.",pol,none,0.0,0,0
1587,"It proposed a new metric to measure the diversity of the generative model's ""worst"" outputs based on the sample clustering patterns.",pol,none,0.0,0,0
1588,"Furthermore, it proposed two blackbox approaches to increasing the model diversity through resampling the latent z. Unlike most existing works that address the model collapse problem, a blackbox approach does not make assumptions about having access to model weights or the artifacts produced during model training, making it more widely applicable than the white-box approaches.",pol,none,0.0,0,0
1589,"In terms of experiment setup, the authors chooses face generation as the area to investigate and measures the diversity by detecting the generated face identity.",pol,none,0.0,0,0
1590,"With the proposed methods, the authors showed that most STOA methods have a wide gap between the top p faces of the most popular face identities and randomly sampled faces.",pol,none,0.0,0,0
1591,It further showed that the proposed blackbox approaches increases the proposed diversity metric without sacrificing image quality.,pol,none,0.0,0,0
1592,The proposed diversity measuring metric is lacking both in terms of experimental proofs and intuitive motivations.,pol,pol_negative,1.0,1,1
1593,"While the black-box calibration of a GAN model may be attractive under specific settings, the authors did not consider the restrictions under those situations and their design may be hard to implement as a result.",pol,pol_negative,1.0,1,1
1594,"For those reasons, I propose to REJECT this paper.",pol,none,0.0,0,0
1595,Missing key experiments that will provide more motivation that 1. the new metric reflects human perception of diversity 2. the new metric works better than existing ones:,pol,none,0.0,1,0
1596,1. Please provide experiments and/or citation for using the face identity as a proxy for face image diversity.,pol,pol_negative,1.0,0,1
1597,this is important since all your experiments rely on that assumption.,pol,pol_negative,1.0,0,1
1598,2. Were there experiments that applies your metric to the training datasets like CelebA and FFHQ? In theory your metric should show no gap between N_R_obs and N_R_ref measured on the training dataset since that's the sampled ground truth.,pol,pol_negative,1.0,2,1
1599,Missing assumptions about blackbox calibration approaches:,pol,none,0.0,1,0
1600,"1. If we do not have access to the model parameter, the training data, or the artifacts during training like the discriminator, what are some of the real world situations that fit this description? In those cases, is it too much to assume that we can control the random seed input to G?",pol,pol_negative,1.0,2,1
1601,2. Is it reasonable to assume some constraints on how much data we can get from the blackbox generator?,pol,pol_negative,1.0,2,1
1602,"A website that just exposes the image generation API may not allow you to ping their service 100k times to improve the generation diversity. If you are allowed to do that, it may be reasonable to assume that you can contact the API provider to get access to the rest of the model.",pol,none,0.0,1,0
1603,Minor improvements that did not have a huge impact on the score,pol,none,0.0,0,0
1604,"1. I found the argument about FID in section 2.1 unconvincing. Are there proofs or citations for the claim that real images don't follow multivariate gaussian distribution after applying FID? Copying is indeed an issue that FID cannot detect, but it may be tangential to model collapse for real world concerns like privacy.",pol,pol_negative,1.0,1,1
1605,"2. The statement ""IS, FID and MODE score takes both visual fidelity and diversity into account."" under ""Evaluation of Mode Collapse"" is contradictory to the description in sec 2.1 that IS in fact does not measure diversity.",pol,pol_negative,1.0,1,1
1606,"3. You may want to consider stating the work as ""a pilot study"" (sec 6.) earlier in the abstract or in the introduction, so that the reader knows what to expect.",pol,pol_negative,1.0,2,1
1607,This work proposes an approach for explicitly placing information in a subset of the latent variables.,pol,none,0.0,0,0
1608,"The approach is to construct an auxiliary generative model that takes as input the set of latent variables subtracted from the target subset, which is used to model modified data samples that do not contain the desired information.",pol,none,0.0,0,0
1609,Experiments focus on learning global information.,pol,none,0.0,0,0
1610,The auxiliary model is then given data that have their global information destroyed via random shuffling of image patches.,pol,none,0.0,0,0
1611,# Approach seems limited.,pol,none,0.0,1,0
1612,"- This approach seems very limited, as there must exist a known transformation that removes the desired information.",pol,pol_negative,1.0,1,1
1613,"Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)",pol,pol_negative,1.0,2,1
1614,- Can this approach learn multiple factors as opposed to just two?,pol,pol_negative,1.0,2,1
1615,- What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.),pol,pol_negative,1.0,1,1
1616,# More ablations or experiments with comparable settings would be desirable.,pol,none,0.0,2,0
1617,- What is the choice of beta in the beta-VAE training objective?,pol,pol_negative,1.0,2,1
1618,"Apart from 1.2, this isn't mentioned.",pol,none,0.0,1,0
1619,My concern here is that beta might be affecting the result more than the proposed training algorithm.,pol,pol_negative,1.0,1,1
1620,Can the proposed approach perform just as well without a modified objective?,pol,pol_negative,1.0,2,1
1621,Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper.,pol,pol_negative,1.0,1,1
1622,"(e.g. this approach with normal VAE objective, and normal VAE objective without auxiliary task for the clustering experiment.)",pol,none,0.0,0,0
1623,"- Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?",pol,pol_negative,1.0,2,1
1624,# Related work.,pol,none,0.0,0,0
1625,"There are some well-cited works that the authors may have missed. These are ultimately different approaches, but perhaps the authors can obtain some inspiration from these:",pol,pol_negative,1.0,0,1
1626,"- Tranforming autoencoders [1] also apply a transformation to the image, but the goal is to learn the factor corresponding to the transformation, rather than the complement as in this work.",pol,none,0.0,0,0
1627,- An opposing approach for explicit information placement with a modified training procedure (where the target information is directly placed in the target subset and can handle multiple factors) is DC-IGN [2].,pol,none,0.0,0,0
1628,"I believe the DC-IGN approach is more general and can handle a superset of the tasks of this approach, without requiring an auxiliary decoder.",pol,none,0.0,0,0
1629,"Comparing to this approach, I wonder if it would be better to provide samples that exhibit a particular factor, or samples that conceal the factor?",pol,pol_negative,1.0,2,1
1630,"[1] Hinton, Geoffrey E., Alex Krizhevsky, and Sida D. Wang. ""Transforming auto-encoders."" International Conference on Artificial Neural Networks. Springer, Berlin, Heidelberg, 2011.",pol,none,0.0,0,0
1631,"[2] Kulkarni, Tejas D., et al. ""Deep convolutional inverse graphics network."" Advances in neural information processing systems. 2015.",pol,none,0.0,0,0
1632,---- Update since rebuttal ----,pol,none,0.0,0,0
1633,I thank the authors for clarifying how this work fits in with related works and clarifying the hyperparameters.,pol,none,0.0,0,0
1634,I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement.,pol,pol_negative,1.0,1,1
1635,More experiments based on different transformations that the authors have mentioned would make this a stronger contribution.,pol,pol_negative,1.0,2,1
1636,"The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation.",pol,pol_negative,1.0,1,1
1637,"This paper tries to quantify how ""dense"" representations we need for a specific task -- more specifically, how many dimensions are needed from a given representation (for a given task) to achieve a percentage of the performance of the entire representation.",pol,none,0.0,0,0
1638,The second thing the paper tries to quantify is how well representations learned for one task can be fine tuned for another.,pol,none,0.0,0,0
1639,Experiments are conducted with 4 different representation technique on a dozen or so tasks.,pol,none,0.0,0,0
1640,"Quick summary: While I liked aspects of this -- including the motivation of having a lightweight way of understanding how well representations transfer across tasks, overall my concerns surrounding the methodology and some missing analysis leads me to believe this needs more work before it is ready for publication.",pol,pol_neutral,2.0,1,2
1641,Quality: Below average,pol,pol_negative,1.0,0,1
1642,I believe the proposed techniques have some flaws which hurt the eventual method.,pol,pol_negative,1.0,1,1
1643,There are also concerns about the motivations behind parts of the technique.,pol,pol_negative,1.0,1,1
1644,Clarity: Fair,pol,pol_neutral,2.0,0,2
1645,There were some experimental details that were poorly explained but in general the paper was readable.,pol,pol_negative,1.0,3,1
1646,Originality: Fair,pol,pol_neutral,2.0,0,2
1647,There were some nice ideas in the work but I remain concerned about aspects of it.,pol,pol_neutral,2.0,1,2
1648,Significance: Below average,pol,pol_negative,1.0,0,1
1649,My concern is that the flaws in the method do not make it conducive to use as is.,pol,pol_negative,1.0,1,1
1650,Strengths / Things I liked:,pol,none,0.0,0,0
1651,+ I really liked the motivating problem of being able to (hopefully cheaply / efficiently) estimate transfer potential to understand how well representations will perform on a different task.,pol,pol_positive,3.0,3,3
1652,+ Multiple representations and tasks experimented with,pol,pol_positive,3.0,3,3
1653,Weaknesses / Things that concerned me:,pol,none,0.0,0,0
1654,(In no specific order),pol,none,0.0,1,0
1655,- (W1) Adversely affected by rotations: One of my big concerns with the work is the way the CFS is computed.,pol,pol_negative,1.0,1,1
1656,"While it seems ok to estimate these different metrics using only linear models, my concern with this is that the linear models are only given a subset of the **exact** dimensions of the original representations.",pol,pol_negative,1.0,1,1
1657,This is very much unlike the learning objectives of most of these representation learning methods and hence is highly biased and dependent on the actual methods and the random seeds used and the rotations it performs. (In many cases the representations are used starting with a fully connected layer bottom layer on top of the representations and hence rotations of the representations do not affect performance),pol,pol_negative,1.0,0,1
1658,Let's take an example: Say there is a single dimension of the representation that is a perfect predictor of a task.,pol,pol_negative,1.0,0,1
1659,Suppose we rotated these representations. Now the signal from the original dimension is split across multiple dimensions and hence the CFS may be deceivingly high.,pol,pol_negative,1.0,1,1
1660,To me this is a big concern as different runs of the same representation technique can likely have very different CFS scores based on initializations and random seeds.,pol,pol_negative,1.0,0,1
1661,- (W2) Related to the last line: I did not see any experiments / analysis showing how stable these different numbers are across different runs of the representation technique. Nor did I see any error bars in the experiments.,pol,pol_negative,1.0,1,1
1662,This again greatly concerned me as I am not certain how stable these metrics are.,pol,pol_negative,1.0,1,1
1663,- (W3) Baselines for transfer learning: I felt this was another notable oversight.,pol,pol_negative,1.0,1,1
1664,"I would have liked to see results for both trivial baselines like random ranking as well as more informed baselines where we can estimate transfer potential using say k representation techniques, and then use that to help us understand how well it would do on the other representations.",pol,pol_negative,1.0,2,1
1665,This latter baseline is a zero-cost baseline as it is not even dependent on the method.,pol,pol_negative,1.0,0,1
1666,"- (W4) Metrics for ranking of transfer don't make sense (and some are missing) : I also don't understand how ""precision"" and NDCG are used as metrics.",pol,pol_negative,1.0,1,1
1667,"Based on my understanding the authors rank (which itself is questionable) the different tasks in order of potential for transfer and then call this the ""gold"" set. How is precision and NDCG calculated from this?",pol,pol_negative,1.0,1,1
1668,More importantly I don't believe looking at rank alone is sufficient since that completely obscures the actual performance numbers obtained via transfer.,pol,pol_neutral,2.0,1,2
1669,In most cases I would care about how well my model would perform on transfer not just which tasks I should transfer from. I would have wanted to understand something like the correlation of these produced scores with the actual ground truth performance numbers.,pol,pol_negative,1.0,1,1
1670,- (W5) Multi-task learning: I did not see any mention or experiments of what can be expected when the representations are themselves trained on multiple tasks.,pol,pol_negative,1.0,1,1
1671,(This seems like something that could easily be done in the empirical analysis as well and would provide richer empirical signals as well),pol,pol_negative,1.0,0,1
1672,- (W6) Motivation for CFS: I still don't fully understand the need to understand the density of the representation (especially in the manner proposed in the paper). Why is this an important problem? Perhaps expanding on this would be helpful,pol,pol_negative,1.0,1,1
1673,-,pol,none,0.0,0,0
1674,(W7) Alternatives to CFS / Computational concerns: A big concern I had was the computational expense of the proposed approach. Unfortunately I did not see any discussion about this in the paper or empirically.,pol,pol_negative,1.0,1,1
1675,"I find this striking because I can easily come up with cheaper alternatives to get at this ""density"".",pol,pol_negative,1.0,0,1
1676,For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.,pol,pol_negative,1.0,0,1
1677,If I were to go through the computation of then why not just train a smaller version of that representation technique instead and **directly** see how well it can encode data in k dimensions via that technique / for that task?,pol,pol_negative,1.0,2,1
1678,Alternatively why not try using a factorization technique to reduce the rank and then see how well the method does for different ranks?,pol,pol_negative,1.0,2,1
1679,-,pol,none,0.0,0,0
1680,(W7b) Likewise I wonder if we could just measure transfer more directly as well and why we need to go via these CFS sets,pol,pol_negative,1.0,2,1
1681,- (W8),pol,none,0.0,0,0
1682,The proposed  CLF weight difference method has some concerning aspects as well.,pol,pol_negative,1.0,1,1
1683,For example say we had two task with exact opposite labels.,pol,pol_negative,1.0,0,1
1684,They would have a very low weight difference score though they are ideal representations for each other.,pol,pol_negative,1.0,1,1
1685,Likewise looking at a difference of weight vectors seems arbitrary in other ways as well.,pol,pol_negative,1.0,1,1
1686,This paper puts forth adversarial architectures for TTS.,pol,none,0.0,0,0
1687,"Currently, there aren't many examples (e.g. Donahue et al,  Engel et al. referenced in paper) of GANs being used successfully in TTS, so this papers in this area are significant.",pol,pol_positive,3.0,3,3
1688,"The architectures proposed are convolutional (in the manner of Yu and Koltun), with increasing receptive field sizes taking into account the long term dependency structure inherent in speech signals.",pol,none,0.0,0,0
1689,"The input to the generator are linguistic and pitch signals - extracted externally, and noise.",pol,none,0.0,0,0
1690,"In that sense, we are working with a conditional GAN.",pol,none,0.0,0,0
1691,I found the discriminator design very interesting.,pol,pol_positive,3.0,3,3
1692,"As the comment below notes, it is a sort of patch GAN discriminator (See pix2pix, and this comment from Philip Isola - https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/39) and that is could be quite significant in that it classifies at different scales.",pol,pol_positive,3.0,0,3
1693,"In the image world, having a single discriminator for the whole model would not take into account local structure of the images.",pol,none,0.0,1,0
1694,"Likewise, perhaps we can imagine something similar in the case of audio at varying scales - in fact, audio dependencies are even more long range.",pol,none,0.0,1,0
1695,That might be one reason why the variable window sizes work here.,pol,none,0.0,0,0
1696,"The paper also presents to image analogues for metrics based on FID and the KID, with the features being taken from DeepSpeech2.",pol,none,0.0,0,0
1697,I found the speech sample presented very convincing.,pol,pol_positive,3.0,3,3
1698,"In general, the architectures are also presented quite clearly, so it seems that we might be able to reproduce these experiments in our own practice.",pol,pol_positive,3.0,0,3
1699,It is also promising that producing good speech could be achieved by a non-autoregressive or attention based architecture.,pol,pol_positive,3.0,0,3
1700,The authors mention that they hardly encounter any issues with training stability and mode collapse. Is that because of the design of the multiple discriminator architecture?,pol,pol_neutral,2.0,2,2
1701,The paper presents an end-to-end methods for jointly training named entity recognition (NER) and relation extraction (RE).,pol,none,0.0,0,0
1702,"The model leverage pre-trained BERT language models, making it very fast to train.",pol,none,0.0,0,0
1703,The methods is evaluated on 5 standard NER+RE datasets with good performances.,pol,none,0.0,3,0
1704,Pros:,pol,none,0.0,0,0
1705,- the paper is well written and very clear,pol,pol_positive,3.0,3,3
1706,- the proposed model has two main advantages: (1) it is very fast to train due to the use of pre-trained BERT representations and (2) it does not depends on any external NLP tool (such as dependency parser),pol,pol_positive,3.0,3,3
1707,Cons:,pol,none,0.0,0,0
1708,- I think the main source of improvement comes from the BERT representations used as input.,pol,none,0.0,0,0
1709,"As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.",pol,pol_negative,1.0,1,1
1710,"- Without this ablation study, the contributions of this paper are to show that using BERT representations as input (1) leads to better performances for NER+RE  and (2) makes the model faster to train. This is not really surprising...",pol,pol_negative,1.0,1,1
1711,The authors consider the use of tensor approximations to more accurately capture syntactical aspects of compositionality for word embeddings.,pol,none,0.0,0,0
1712,"Given two words a and b, when your goal is to find a word whose meaning is roughly that of the phrase (a,b)",pol,none,0.0,0,0
1713,", a standard approach to to find the word whose embedding is close to the sum of the embeddings, a + b. The authors point out that others have observed that this form of compositionality does not leverage any information on the syntax of the pair (a,b), and the propose using a tensor contraction to model an additional multiplicative interaction between a and b, so they propose finding the word whose embedding is closest to a + b + T*a*b, where T is a tensor, and T*a*b denotes the vector obtained by contracting a and b with T. They test this idea specifically on the use-case where (a,b) is an adjective,noun pair, and show that their form of compositionality outperforms weighted versions of additive compositionality in terms of spearman and pearson correlation with human judgements.",pol,none,0.0,0,0
1714,"In their model, the word embeddings are learned separately, then the tensor T is learned by minimizing an objective whose goal is to minimize the error in predicting observed trigram statistics.",pol,none,0.0,0,0
1715,The specific objective comes from a nontrivial tensorial extension of the original matricial RAND-WALK model for learning word embeddings.,pol,none,0.0,0,0
1716,"The topic is fitting with ICLR, and some attendees will find the results interesting.",pol,pol_positive,3.0,3,3
1717,"As in the original RAND-WALK paper, the theory is interesting, but not the main attraction, as it relies on strong generative modeling assumptions that essentially bake in the desired results.",pol,pol_neutral,2.0,1,2
1718,"The main appeal is the idea of using T to model syntactic interactions, and the algorithm for learning T. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to ICLR attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown).",pol,pol_negative,1.0,1,1
1719,Pros:,pol,none,0.0,0,0
1720,- theoretical justification is given for their assumption that the higher-order interactions can be modeled by a tensor,pol,pol_positive,3.0,0,3
1721,- the tensor model does deliver some improvement over linear composition on noun-adjective pairs when measured against human judgement,pol,pol_positive,3.0,3,3
1722,Cons:,pol,none,0.0,0,0
1723,- no downstream applications are given which show that these higher order interactions can be useful for downstream tasks.,pol,pol_negative,1.0,1,1
1724,"- the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b?",pol,pol_negative,1.0,1,1
1725,"- comparison should be made to the linear composition method in the Arora, Liang, Ma ICLR 2017 paper",pol,pol_negative,1.0,2,1
1726,Some additional citations:,pol,none,0.0,0,0
1727,- the above-mentioned ICLR paper provides a performant alternative to unweighted linear composition,pol,none,0.0,0,0
1728,"- the 2017 Gittens, Achlioptas, Drineas ACL paper provides theory on the linear composition of some word embeddings",pol,none,0.0,0,0
1729,The suggested method proposes a technique to compress neural networks bases on PQ quantization.,pol,none,0.0,0,0
1730,"The algorithm quantizes matrices of linear operations, and, by generalization, also works on convolutional networks.",pol,none,0.0,0,0
1731,"Rather than trying to compress weights (i.e. to minimize distance between original and quantized weights), the algorithm considers a distribution of unlabeled inputs and looks for such quantization which would affect output activations as little as possible over that distribution of data.",pol,none,0.0,0,0
1732,"The algorithm works by splitting each column of W_ij into m equal subvectors, learning a codebook for those subvectors, and encoding each of those subvectors as one of the words from the codebook.",pol,none,0.0,0,0
1733,The method provides impressive compression ratios (in the order of x20-30) but at the cost of a lower performance.,pol,pol_negative,1.0,0,1
1734,Whether this is a valuable trade-off is highly application dependent.,pol,pol_negative,1.0,0,1
1735,Overall I find the paper interesting and enjoyable.,pol,pol_positive,3.0,3,3
1736,"However, as I am not an expert in the research area, I can not assess how state of the art the suggested method is.",pol,none,0.0,1,0
1737,There are a few other questions that I think would be nice to answer. I will try to describe them below:,pol,none,0.0,0,0
1738,Suppose we have a matric W_{ij} with dimensions NxM where changing i for a given j defines a column.,pol,pol_neutral,2.0,0,2
1739,"By definition, linear operation is defined",pol,pol_neutral,2.0,0,2
1740,y_i = sum_j W_ij x_j .,pol,pol_neutral,2.0,0,2
1741,Now say each column of matrix W is quantized into m subvectors.,pol,pol_neutral,2.0,0,2
1742,We can express W_ij in the following way:,pol,pol_neutral,2.0,0,2
1743,W_ij = (V^1_ij + V^2_ij + ... V^m_ij)x_j where V^m_ij is zero everywhere except for the rows covering a given quantized vector.,pol,pol_neutral,2.0,0,2
1744,"For example, if W had dimensions of 8x16 and m=4,",pol,pol_neutral,2.0,0,2
1745,"V^2_{3,j}=0, for all j",pol,pol_neutral,2.0,0,2
1746,", V^2_{4,j}=non_zero, V^2_{7,j}=non_zero, V^2_{8,j}=0, V^2_{i=4:8,j}=one_of_the_quantized_vectors.",pol,pol_neutral,2.0,0,2
1747,y_i = sum_j W_ij x_j = sum_k sum_j (V^k_ij) x_j =def= sum_k z^k_i where z^k are partial products: z^k_i=0 for i<k*N/m and i>(k+1)N/m,pol,pol_neutral,2.0,0,2
1748,"Thus, the suggested solution effectively splits the output vector y_i into m sections, defines sparse matrices V^k_{ij} 1<=k<=m, and performs column-wise vector quantization for these matrices separately.",pol,pol_neutral,2.0,0,2
1749,"Generally, it is not ovious or given that the current method would be able to compress general matrices well, as it implicitly assumes that weight W_{ij} has a high ""correlation"" with weights W_{i+kN/m,j} (which I call ""vertical"" correlation), W_{i,k+some_number} (which I call ""horizontal"" correlation) and W_{i+kN/m,k+some_number} (which I call ""other"" correlation).",pol,pol_neutral,2.0,0,2
1750,It is not given that those kind of redundancies would exist in arbitrary weight matrices.,pol,pol_neutral,2.0,1,2
1751,"Naturally, the method will work well when weight matrices have a lot of structure and then quantized vectors can be reused.",pol,pol_neutral,2.0,0,2
1752,"Matrices can have either ""horizontal"" or ""vertical"" redundancy (or ""other"" or neither).",pol,pol_neutral,2.0,0,2
1753,It would be very interesting to see which kind of redundancy their method managed to caprture.,pol,pol_neutral,2.0,2,2
1754,"In the 'horizontal' case, it should work well when inputs have a lot of redundancy (say x_j' and x_j'' are highly correlated making it possible to reuse code-words horizontally within any given V^k: V^k_ij'=V^k_ij'').",pol,pol_neutral,2.0,0,2
1755,"However, if thise was the case, it would make more sense to simply remove redundancy by prunning input vector x_j by removing either x_j' or x_j'' from it.",pol,pol_neutral,2.0,1,2
1756,This can be dome by removing one of the outputs from the previous layer.,pol,pol_neutral,2.0,0,2
1757,This can be a symptom of a redundant input.,pol,pol_neutral,2.0,0,2
1758,"Another option is exploiting ""vertical"" redundancy: this happens when output y_i' is correlated with output y_{i'+N/m}. This allows the same code-word to be reused vertically.",pol,pol_neutral,2.0,0,2
1759,This can be a symptom of a redundant output.,pol,pol_neutral,2.0,0,2
1760,It could also be the case that compressibility could be further subtantially improved by trying different matrix row permutations.,pol,pol_neutral,2.0,0,2
1761,"Also, if one notices that y_i' ir correlated with y_i'', it might make sense to permute matrix rows in such a way that both rows would end up a multiple N/m apart.",pol,none,0.0,1,0
1762,It would be interesting to see how this would affect compressibility.,pol,pol_neutral,2.0,2,2
1763,The third case is when code words are reused in arbitrary cases.,pol,pol_neutral,2.0,0,2
1764,"Generally, I think that answering the following questions would be interesting and could guide further research:",pol,none,0.0,0,0
1765,"1. It would be very interesting to know what kind of code-word reusa patterns the algorithm was able to capture, as this may guide further research.",pol,pol_neutral,2.0,2,2
1766,2. How invariance copressibility is under random permutations of matrix rows (thus also output vectors)?,pol,pol_neutral,2.0,2,2
1767,This paper proposes use of intra-life coverage (an agent must visit all locations within each episode) for effective exploration in Atari games.,pol,none,0.0,0,0
1768,This is in contrast of approaches that use inter-life coverage or curiosity metrics to incentivize exploration.,pol,none,0.0,0,0
1769,"The paper shows detailed results and analysis on 2 Atari games: Montezuma’s Revenge and Seaquest, and reports results on other games as well.",pol,none,0.0,0,0
1770,Strengths,pol,none,0.0,0,0
1771,"1. Intuitively, the idea of intra-life curiosity is reasonable.",pol,pol_positive,3.0,3,3
1772,The paper pursues this idea and provides experimental evidence towards it on 2 Atari games.,pol,none,0.0,0,0
1773,It is able to show compelling improvements on the challenging Montezuma’s Revenge game.,pol,pol_positive,3.0,3,3
1774,Weaknesses,pol,none,0.0,0,0
1775,1. The two primary comparison points are missing:,pol,none,0.0,1,0
1776,1a. Comparison to other exploration methods.,pol,pol_negative,1.0,0,1
1777,"A number of methods that use state visitation counts (also referred to as diversity, eg. [A,B]), or prediction error (also referred to as curiosity, eg [C]) have been proposed in recent years.",pol,none,0.0,0,0
1778,It is important to place the contributions in this paper in context of these other works.,pol,pol_negative,1.0,0,1
1779,A number of these references are missing and no experimental comparison to these methods has been made.,pol,pol_negative,1.0,1,1
1780,1b. Comparison between inter and intra life curiosity.,pol,none,0.0,0,0
1781,"One of the central motivation is the utility of intra-life curiosity vs inter-life curiosity, yet no comparisons to this effect have been provided.",pol,pol_negative,1.0,1,1
1782,2.,pol,none,0.0,0,0
1783,"Additionally, the paper employs a custom way of computing coverage (or diversity).",pol,none,0.0,0,0
1784,"It is in terms of location of agent on the screen, as opposed to featurization of the full game screen as used in prior works.",pol,none,0.0,0,0
1785,It is possible that a large part of the gain comes from the clever design of the space for computing intrinsic exploration reward.,pol,pol_negative,1.0,0,1
1786,"The paper tries to control for it, however that description is rather short and vague (not clear how the proposed reward is computed without there being a grid, or how is the grid useful without the intrinsic reward).",pol,pol_negative,1.0,1,1
1787,"More details should be provided, and when comparisons to past works or inter-life curiosity are made, this should be controlled for.",pol,pol_negative,1.0,1,1
1788,"The two ideas (use of grids, and intra-life curiosity vs inter-life curiosity) should be independently investigated and put in context of past work.",pol,pol_negative,1.0,1,1
1789,3. I will encourage investigation on a more varied set of tasks.,pol,pol_negative,1.0,2,1
1790,"Perhaps, also using some MuJoCo environments, or 3D navigation environments.",pol,pol_negative,1.0,0,1
1791,"Table 1 tries to provide some comparisons on Atari, however number of samples is different for different methods making the comparisons invalid.",pol,pol_negative,1.0,1,1
1792,"Additionally, all of these are still on Atari.",pol,none,0.0,0,0
1793,"[A] Diversity is All You Need: Learning Skills without a Reward Function Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine",pol,none,0.0,0,0
1794,"[B] EX2: Exploration with Exemplar Models for Deep Reinforcement Learning Justin Fu, John D. Co-Reyes, Sergey Levine",pol,none,0.0,0,0
1795,"[C] Curiosity-driven Exploration by Self-supervised Prediction Deepak Pathak, Pulkit Agrawal, Alexei A. Efros and Trevor Darrell International Conference on Machine Learning (ICML), 2017",pol,none,0.0,0,0
1796,This paper proposed a new training framework to disentangle global structures from local structures,pol,none,0.0,0,0
1797,based on Variational Autoencoders (VAEs). They first generate a transformed image by shuffling the,pol,none,0.0,0,0
1798,patches of the original image to destroy the global structures.,pol,none,0.0,0,0
1799,The training task forces the model to,pol,none,0.0,0,0
1800,"reconstruct the original image and shuffled images from different latent variables, thus separating",pol,none,0.0,0,0
1801,global long-range structural correlations and local patch-wise correlations .,pol,none,0.0,0,0
1802,"Instead of adjusting the objective function or model structure, the paper proposed a new and simple",pol,pol_positive,3.0,0,3
1803,"training framework to disentangle the global and local structures, which is novel.",pol,none,0.0,3,0
1804,The experiment results are good on SVHN.,pol,pol_positive,3.0,3,3
1805,Some visual inspection experiments on CIFAR10 are performed.,pol,none,0.0,0,0
1806,The plot (Figure2 (d)) is very blurry and people cannot really tell local structure from it.,pol,pol_negative,1.0,1,1
1807,The rest experiments,pol,pol_negative,1.0,0,1
1808,"are all based on SVHN, which is too simple.",pol,none,0.0,1,0
1809,More experiments based on other types of data sets with clear global structures such as faces or stop signs will,pol,pol_negative,1.0,1,1
1810,be more convincing.,pol,none,0.0,1,0
1811,"In the digit dataset, the local and global structures are relatively easy to separate.",pol,none,0.0,1,0
1812,However,pol,pol_negative,1.0,0,1
1813,", in Table 1, the",pol,none,0.0,0,0
1814,performance of VAE+Auxiliary is not better than two of the other methods.,pol,none,0.0,1,0
1815,The idea in this paper is novel but experiments do not seem to be enough.,pol,pol_negative,1.0,1,1
1816,More experiments on datasets,pol,pol_negative,1.0,0,1
1817,with clear global and local structure separations with careful analyses are required to make the paper stronger.,pol,none,0.0,1,0
1818,"This paper is about using ""neural stethoscopes"", small complementary neural networks that are added to a main network which with their auxilary loss functions can measure suitability of features or guide the learning process.",pol,none,0.0,0,0
1819,"The idea is incremental to multi-task learning and enable, in a single framework, to validate intermediate features for additional related tasks.",pol,none,0.0,0,0
1820,Moreover it can promote or suppress the correlation of such features to the tasks related to the main one.,pol,none,0.0,0,0
1821,The framework is applied to the task of visual stability prediction of block towers.,pol,none,0.0,0,0
1822,"The paper builds upon Groth et al. 2018, adding the concept of local stability as correlated secondary task, used with the proposed neural stethoscopes.",pol,none,0.0,0,0
1823,"Experiments with an extension of ShapeStacks (Groth et al. 2018) dataset where the local stability is added to the global stability class, show that it is possibile increase the performance using the additional task.",pol,none,0.0,0,0
1824,"Moreover, it is shown that neural stethoscopes can suppress nuisance information when using a biased training dataset where the local and global stability are purposely inversely correlated.",pol,none,0.0,0,0
1825,Strengths:,pol,none,0.0,0,0
1826,"+ A very nice paper, well written and easy to read. Figures are helpful and the structure is clear.",pol,pol_positive,3.0,3,3
1827,+ The concept of neural stethoscope is interesting and simplify the concepts behind multitask learning.,pol,pol_positive,3.0,3,3
1828,"+ Experiments are convincing, interesting and there is some novelty in vision stability prediction.",pol,pol_positive,3.0,3,3
1829,Weaknesses:,pol,none,0.0,0,0
1830,"- The novelty is limited related to multitask learning, thus it is an incremental paper.",pol,pol_negative,1.0,1,1
1831,"The authors propose a new way to augment textual datasets for the task of sentiment analysis, in order to help the learning methods to generalize better by concentrating on learning the different that makes a difference.",pol,none,0.0,0,0
1832,"The main idea of the paper is to augment existing datasets with minimally counteractual versions of them, that change the sentiment of the documents.",pol,none,0.0,0,0
1833,"In this way, all spurious factors will naturally cancel out.",pol,none,0.0,0,0
1834,"The authors use the newly created datasets and show that indeed, the retrained algorithms on the augmented datasets generalize much better.",pol,pol_positive,3.0,0,3
1835,The main contribution of the paper is the introduction of the idea of counterfactual datasets for sentiment analysis.,pol,pol_positive,3.0,0,3
1836,"Overall, I find the idea of the paper quite interesting and I’m excited to use the datasets they have created.",pol,pol_positive,3.0,3,3
1837,"However, I think the relative novelty of the paper does not meet ICLR standards, and it’s better suited as a whitepaper attached to an open dataset release.",pol,pol_negative,1.0,1,1
1838,"In the paper, the authors propose a pipelined backpropagation algorithm faster than the traditional backpropagation algorithm.",pol,none,0.0,0,0
1839,The proposed method allows computing gradients using stale weights such that computations in different layers can be executed in parallel.,pol,none,0.0,0,0
1840,They also conduct experiments to evaluate the effect of staleness and show that the proposed method is faster than compared methods.,pol,none,0.0,0,0
1841,I have the following concerns:,pol,none,0.0,0,0
1842,"1) There are several important works on model-parallelism and convergence guarantee of pipeline-based methods missing in this paper, for example [1][2].",pol,pol_negative,1.0,1,1
1843,2) Does the proposed method store immediate activations or recompute the activations in the backward pass?,pol,pol_negative,1.0,2,1
1844,"3) In the experiments, the accuracy values are too low for me.",pol,pol_negative,1.0,1,1
1845,"For example, resnet110 on cifar10 is 91.99% only, it should be around 93%, an example online",pol,none,0.0,1,0
1846,https://github.com/akamaster/pytorch_resnet_cifar10.,pol,none,0.0,0,0
1847,"4) In the experiments, more comparisons with methods in [1] or [2] should be conducted given they are all parallelizing the backpropagation algorithm and achieve speedup in the training.",pol,pol_negative,1.0,1,1
1848,"5) Last but not least, convergence analysis of the proposed method should be provided given that asynchrony may lead to divergence in the optimization.",pol,pol_negative,1.0,2,1
1849,"[1] Huo, Zhouyuan, et al. ""Decoupled parallel backpropagation with convergence guarantee."" arXiv preprint arXiv:1804.10574 (2018).",pol,none,0.0,0,0
1850,"[2] Huo, Zhouyuan, Bin Gu, and Heng Huang. ""Training neural networks using features replay."" Advances in Neural Information Processing Systems. 2018.",pol,none,0.0,0,0
1851,"This paper looks to predict ""unstructured"" set output data.",pol,none,0.0,0,0
1852,It extends Rezatofighi et al 2018 by modeling a latent permutation.,pol,none,0.0,0,0
1853,"Unfortunately, there is a bit of an identity crisis happening in this paper. There are several choices that do not follow based on the data the paper considers.",pol,pol_negative,1.0,1,1
1854,"1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\pi | x_i, w) (1); this feels like a very odd choice to me.",pol,pol_negative,1.0,1,1
1855,"The outputs are either unordered sets, where you would have a permutation invariant (or exchangeable) likelihood, or they are ordered sequence where the order of the outputs does matter, as some are more likely than others.",pol,pol_negative,1.0,0,1
1856,2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others.,pol,pol_negative,1.0,1,1
1857,"The way the permutation, or the order of the data, accounts in the likelihood (2) does not make sense.",pol,pol_negative,1.0,1,1
1858,"Conditioned on the permutation of the set, the points are exchangeable.",pol,none,0.0,0,0
1859,"Let's just consider a 2 element ""set"" at the moment Y = (y_1, y_2).",pol,none,0.0,0,0
1860,"Order matters, so either this is being observed as pi=(1, 2) or pi=(2, 1), both of which depend on the input x. However, the likelihood of the points does not actually depend on the order in any traditional sense of the word.",pol,none,0.0,1,0
1861,we have:,pol,none,0.0,0,0
1862,"p_\pi((1, 2) | x, w) p_y(y_1 |  x, w, (1, 2)) p_y(y_2 |  x, w, (1, 2)) + p_\pi((2, 1) | x, w) p_y(y_1 |  x, w, (2, 1)) p_y(y_2 |  x, w, (2, 1))",pol,none,0.0,0,0
1863,"*Note that in here (as in eq. 2) the output distribution p_y does not know what the index is of what it is outputting, since it is iid.* So what does this mean? It means that the order (permutation) can only affect the distribution in an iid (exchangeable, order invariant) way.",pol,none,0.0,2,0
1864,Essentially the paper has just written a mixture model for the output points where there are as many components as permutations.,pol,none,0.0,0,0
1865,"I don't think this makes much sense, and if it was an intentional choice, the paper did a poor job of indicating it.",pol,pol_negative,1.0,1,1
1866,"3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues.",pol,pol_negative,1.0,1,1
1867,It is very unclear how the dependence on \pi drops out when getting a MAP estimate of outputs in section 3.3.,pol,pol_negative,1.0,1,1
1868,This needs to be justified.,pol,pol_negative,1.0,0,1
1869,There are some stylistic shortcomings as well.,pol,pol_negative,1.0,1,1
1870,"For example, the related works paper would read better if it wasn't one long block (i.e. break it into several paragraphs)",pol,pol_negative,1.0,1,1
1871,.,pol,none,0.0,0,0
1872,"Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1.",pol,pol_negative,1.0,0,1
1873,But these and other points are minor.,pol,none,0.0,1,0
1874,"The paper should not be published until it can resolve or make sense of the methodological discrepancies between what it says it looks to do and what it actually does as described in points 1), 2), and 3) above.",pol,pol_negative,1.0,1,1
1875,"In their abstract, the authors claim to provide state-of-the-art perplexity on Penn Treebank, which is not true.",pol,pol_negative,1.0,1,1
1876,"As the authors state, their notion of ""state-of-the-art"" excludes exactly that earlier work, which does provide state-of-the-art perplexity on Penn Treebank (Yang et al. 2017), as stated in Sec. 4.1.",pol,none,0.0,1,0
1877,"The question is, why one would exlude the mixture-of-softmax approach here?",pol,pol_negative,1.0,1,1
1878,This is clearly misleading.,pol,pol_negative,1.0,1,1
1879,The authors introduce the idea of past decoding for the purpose of regularization.,pol,none,0.0,0,0
1880,"It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.",pol,pol_negative,1.0,1,1
1881,The results obtained show moderate improvements of approx. 1 point in perplexity on top of their best current result on Penn Treebank.,pol,none,0.0,0,0
1882,"Considering the small size of the corpus for the evaluation of a regularization method, the results even seem optimistic - it remains unclear, if this approach would readily scale to larger datasets.",pol,pol_negative,1.0,1,1
1883,"The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications.",pol,pol_negative,1.0,1,1
1884,"Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches.",pol,pol_negative,1.0,0,1
1885,"It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.",pol,pol_negative,1.0,1,1
1886,Update to the Review after the rebuttal from the Authors:,pol,none,0.0,0,0
1887,After carefully reviewing the responses by the authors especially on my concerns about the significance of solving an instance of a given problem and the improvement in the exposition of the ideas I would like to amend my earlier decision and recommend to accept.,pol,none,0.0,0,0
1888,For completeness below is the original review.,pol,none,0.0,0,0
1889,This paper introduces a framework to learn to generate solutions to online combinatorial optimization problems with worst case guarantees.,pol,none,0.0,0,0
1890,"The framework as the authors claim eliminates the need for manual hard to solve instance/data creation, which is necessary to teach the model to provide the aforementioned worst case guarantees.",pol,none,0.0,0,0
1891,"Therefore the main contribution of the paper can be said that this framework shows that it is possible to train a machine learning model, which can learn an algorithm to solve hard online combinatorial optimization problems and this training can be done without knowing much about the actual optimization problem domain.",pol,none,0.0,0,0
1892,The only input required is the way to calculate the objective function of the actual problem.,pol,none,0.0,1,0
1893,This contribution is demonstrated on two classes of problems: Ski-Rental and Fractional AdWords.,pol,none,0.0,0,0
1894,The framework requires two neural networks one for solution generation agent and one for problem instance generation.,pol,none,0.0,0,0
1895,These two networks are trained jointly from scratch and the underlying algorithm for the training is provided.,pol,none,0.0,0,0
1896,Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems.,pol,pol_negative,1.0,1,1
1897,Is it because the ML solution would be faster to compute with big instances? Is it because with the proposed approach one can curate sophisticated heuristic solutions when provable optimality is out of reach?,pol,pol_negative,1.0,2,1
1898,"This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.",pol,pol_negative,1.0,1,1
1899,"Although they show that the Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, ie. problem instances are not generated by use of a machine learning model, which is one of main claims the authors are making.",pol,pol_negative,1.0,1,1
1900,Therefore I do not find being able to solve this problem as a supporting evidence for the contributions claimed.,pol,pol_negative,1.0,1,1
1901,In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions.,pol,pol_negative,1.0,1,1
1902,I find this important because for combinatorial optimization usually scale matters a lot.,pol,none,0.0,0,0
1903,While a small instance of a problem can be solve by a general purpose solver quickly a small increase in the problem size can turn out to be intractable.,pol,none,0.0,1,0
1904,When proposing a machine learning approach to such problems I would expect the model to scale better than pure optimization approach so that there would be demonstrable benefit.,pol,none,0.0,0,0
1905,Although the paper proposes an interesting framework I would argue that it is a “green apple” in the sense that authors need to motivate the approach better and expand the contribution beyond solving a particular instance.,pol,pol_negative,1.0,1,1
1906,"Authors acknowledge the fact that their experimental setup is rather limited in Appendix C.1, which I agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the AdWords problem, however they leave this as a future work, which I find unfortunate. I would recommend taking this direction rigorously and expand the contribution, which would prove to be a very sound contribution.",pol,pol_negative,1.0,1,1
1907,In order to clarify the exposition the following are some questions:,pol,none,0.0,0,0
1908,1. Authors call the approach YaoGAN due to its structural similarity to GANs.,pol,none,0.0,0,0
1909,"I understand the fact that they are training two neural networks in an alternating scheme, which is similar to the GAN training.",pol,none,0.0,0,0
1910,How can one evaluate the solutions generated by this framework similar to how GAN generators are evaluated?,pol,pol_neutral,2.0,1,2
1911,"Can one walk the latent distribution of the algorithm agent and draw insights, which might lead into tailoring some algorithms that would be appropriate for some input distribution although in general inferior in terms of worst case guarantees?",pol,pol_negative,1.0,2,1
1912,2. The main technical contribution claim needs to be elaborated.,pol,pol_negative,1.0,1,1
1913,I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation.,pol,pol_negative,1.0,1,1
1914,3. Authors claim there are two shortcomings of the previous method proposed in Kong et. al 2018.,pol,none,0.0,0,0
1915,They need to elaborate how their method overcomes these issues better.,pol,pol_negative,1.0,1,1
1916,"4. Authors state that fractional relaxation of combinatorial mainly integer optimization problems, which is accurate.",pol,none,0.0,3,0
1917,Yet their approach is only able to solve the fractional version of the AdWords problem.,pol,pol_negative,1.0,1,1
1918,In addition I agree with the fact that although continuous relaxations to integer optimization problems might provide insightful directions they usually employed to to prove bounds on the heuristic approaches.,pol,none,0.0,0,0
1919,"Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me.",pol,pol_negative,1.0,1,1
1920,I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem.,pol,pol_negative,1.0,1,1
1921,What are the shortcomings?,pol,pol_neutral,2.0,2,2
1922,"5.In Appendix A authors talk about no-regret dynamics, which are relevant.",pol,none,0.0,3,0
1923,"However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims.",pol,pol_negative,1.0,1,1
1924,6. In appendix C.2 authors provide additional plots for the Fractional AdWords problem.,pol,none,0.0,0,0
1925,"However, they retain from providing any intuition about them.",pol,pol_negative,1.0,1,1
1926,In particular what is the conclusion to be drawn from Figure 5.,pol,pol_neutral,2.0,2,2
1927,This needs more elaboration. Is this way of training results expected? What is the lesson learned?,pol,pol_negative,1.0,1,1
1928,7.In Figure 8 they provide example data from experience array.,pol,none,0.0,0,0
1929,What are the significance of these examples? How they help us understand the problem instance generation was actually able to find interesting instances? What kind of dynamics are under covered?,pol,pol_negative,1.0,2,1
1930,These are not directly revealed by only looking at the pictures one needs more explanation to support the claims.,pol,pol_negative,1.0,1,1
1931,This paper proposed a method pGAN based on Generative Adversarial Networks to generate poisoning examples in order to degrade the performance of classifiers when trained on the poisoned training data.,pol,none,0.0,0,0
1932,The authors evaluated pGAN on both synthetic datasets and commonly used MNIST and Fashion MNIST datasets in machine learning.,pol,none,0.0,0,0
1933,The paper is self-contained and easy to read. My main concern is on the experiment results.,pol,pol_positive,3.0,3,3
1934,The detailed questions are as follows:,pol,none,0.0,0,0
1935,Q1: Has the authors tried more complicated datasets such as CIFAR-10 to evaluate the pGAN method? It would make the paper more convincing to add results on more complex datasets.,pol,pol_negative,1.0,2,1
1936,Q2: Can the authors structure the experimental results with different sections? Currently it is just a single section which is difficult to read.,pol,pol_negative,1.0,1,1
1937,"Q3: The authors noticed that “But, as we decrease the value of α, the distribution of red points shifts towards the region where both green and blue distributions overlap”.",pol,none,0.0,0,0
1938,This observation is interesting as it finds that the poisoned input tends to lie on the overlap of two classes.,pol,none,0.0,0,0
1939,But this can easily lead to a defense method: remove those training examples that are close to the other class.,pol,none,0.0,0,0
1940,This defense mechanism can be used together with other sanitization approaches.,pol,none,0.0,0,0
1941,So I would like to see how would pGAN perform in this case?,pol,pol_negative,1.0,2,1
1942,Q4: The authors mentioned “Comparison with existing poisoning attacks in the research literature is challenging: Optimal poisoning attacks as in Munoz-Gonzalez et al. (2017) are computationally very expensive for the size of the networks and datasets used in our experiments in Fig. 2.,pol,none,0.0,1,0
1943,”,pol,none,0.0,0,0
1944,.,pol,none,0.0,0,0
1945,"However, I can not agree because you can simply generate poisoned data and train the neural networks on the poisoned data regardless of the underlying approach that is targeted in generating the poisoned data.",pol,pol_negative,1.0,1,1
1946,This would be an effective baseline to compare. (Correct me if I am wrong here.),pol,pol_negative,1.0,2,1
1947,I will change my score if the authors can address my concerns here.,pol,none,0.0,0,0
1948,================================================================,pol,none,0.0,0,0
1949,Thanks for the rebuttal. I am more convinced now.,pol,none,0.0,0,0
1950,— Summary,pol,none,0.0,0,0
1951,"The method extends [21], which proposes an unordered set prediction model for multi-class classification.",pol,none,0.0,0,0
1952,"For that problem, [21] can assume logistic outputs for all distinct classes.",pol,none,0.0,0,0
1953,"This work extends set prediction to the object detection task, where box identity is not distinct — this is handled by an additional model output that reasons about the most likely object permutations.",pol,none,0.0,0,0
1954,"The permutation predictions are used during training, but are not needed at inference time — as shown in Fig1 and Eq 7.",pol,none,0.0,0,0
1955,Results are on detection of overlapping objects and a CAPTCHA toy summation example.,pol,none,0.0,0,0
1956,— Clarity,pol,none,0.0,0,0
1957,The exposition is not particularly clear in several places:,pol,pol_negative,1.0,1,1
1958,- U^m in Eq 1 is undefined and un-discussed.,pol,pol_negative,1.0,1,1
1959,"What probability term does it correspond to? It is supposed to make probabilities of different cardinalities comparable, but the exact mechanism is unclear.",pol,pol_negative,1.0,1,1
1960,- The term p(w) disappears on the left hand side of Eq 2.,pol,pol_negative,1.0,1,1
1961,"- Notation in Sec. 3.2 is very cumbersome, making it hard to follow.",pol,pol_negative,1.0,1,1
1962,"Furthermore, I found the description ambiguous, preventing me from understanding how exactly the permutation head output is used in Eq 5.",pol,pol_negative,1.0,1,1
1963,"Specifically, there is some confusion about estimation of w~, which seems based on frequency estimation from past SGD iterations (Eq 3).",pol,pol_negative,1.0,1,1
1964,"If so, why does term f2 in Eq 5 contain the permutation head output O2 and how do the two relate?",pol,pol_neutral,2.0,2,2
1965,"- The network architecture is never described, especially the transition from Conv to Dense and the layer sizes, making the work hard to reproduce.",pol,pol_negative,1.0,1,1
1966,The dimensions of the convolutional feature map matter (probably need to be kept tractable).,pol,none,0.0,1,0
1967,— Significance,pol,none,0.0,0,0
1968,"Key aspects of the model are not particularly clear, specifically about how the permutation prediction ( the key novelty here) is used to benefit training.",pol,pol_neutral,2.0,1,2
1969,"— Term f2 in Eq5 uses w~ estimates, which appeared to be based on statistics from past SGD runs, yet also depends on the output of the permutation head O2. Am I misinterpreting the method?",pol,pol_neutral,2.0,2,2
1970,"— In the paragraph right after Eq5, it’s claimed that “Empirically, in our applications, we found out that estimation of the permutations from just f1 [in Eq5] is sufficient to train properly … by using the Hungarian algorithm”. So then f2 term is not even used in. Eq5? If so, what is the significance of the permutation head other than adding an auxiliary loss?",pol,pol_neutral,2.0,1,2
1971,"Furthermore, there are no experimental results demonstrating the effect of the permutation head and the design choices above — if we could get by with only using the Hungarian algorithm, why bother classifying an exponential number of permutations? Do they help when added as an auxiliary loss?",pol,pol_negative,1.0,1,1
1972,"While the failure of NMS to detect overlapping objects is expected, the experiments showing that perm-set prediction handles them well is interesting and promising.",pol,pol_positive,3.0,3,3
1973,"Solving the general case with larger images and many instances would increase the impact significantly — and likely require a combination of perm-set prediction and image tiling, although this is just a hypothesis.",pol,none,0.0,1,0
1974,"The Captcha toy example also shows some interesting behavior emerging — without digit-specific annotations (otherwise it would be multi-class classification setup from [21]), the model can handle the majority of summations correctly.",pol,pol_positive,3.0,3,3
1975,— Experimental results,pol,none,0.0,0,0
1976,The results are interesting proofs-of-concept but a few more experiments/answers would be helpful:,pol,pol_neutral,2.0,0,2
1977,- It still appears that PR curve in the high-precision regime (fig 3b) has lower precision than FRCNN/YOLO.,pol,none,0.0,1,0
1978,Any idea as to why?,pol,pol_neutral,2.0,2,2
1979,"- Ablation results on the effect of the permutation predictions vs Hungarian algorithm, etc would be helpful, as discussed above.",pol,pol_neutral,2.0,2,2
1980,"- How sensitive is the method to seeing a certain cardinality? What if it never sees 3 pedestrians in an image, but only 1,2,4 will it fail to predict 3? Or alternatively, if we train a model that can handle up to 5-6 entities with examples than have <=4? What is the right way of data augmentation for this model (was there any and should there be?)",pol,pol_neutral,2.0,1,2
1981,"- Given that values for U differ across applications, how sensitive is the output / how much sweeping did you have to do?",pol,pol_neutral,2.0,2,2
1982,-- Related work,pol,none,0.0,0,0
1983,To the best of my knowledge it's representative.,pol,pol_neutral,2.0,0,2
1984,It would help to cite more recent work that decreases detector dependence on NMS.,pol,pol_neutral,2.0,2,2
1985,"For example, ""Learning Non-Maximum Suppression"", Hosang, Benenson, Schiele, CVPR 2017 or ""Relation Networks for Object Detection"", by Hu et al, CVPR 2018 and references therein.",pol,pol_neutral,2.0,0,2
1986,This paper proposes a new pipelined training approach to speedup the training for neural networks.,pol,none,0.0,0,0
1987,"The approach separates forward and backpropagation processes into multiple stages, cache the activation and gradients between stages, processes stages simultaneously, and then uses the stored activations to compute gradients for updating the weights.",pol,none,0.0,0,0
1988,The approach leads to stale weights and gradients.,pol,none,0.0,0,0
1989,The authors studied the relation between weight staleness and show that the quality degradation mainly correlates with the percentage of the weights being stale in the pipeline.,pol,none,0.0,0,0
1990,The quality degradation can also be remedied by turning off the pipelining at the later training steps while overall training speed is still faster than without pipelined training.,pol,none,0.0,0,0
1991,"Since this work takes the approach of allowing stale weight updates, the author should also compare with existing distributed training approaches that use asynchronous updates, with or without model parallelism, for example, Dean et al., 2012.",pol,pol_negative,1.0,1,1
1992,Without the comparison it’s not clear how much improvement this approach provides compared to existing work that perform stale updates.,pol,pol_negative,1.0,1,1
1993,"This paper proposes simple metrics for measuring the ""information density"" in learned representations.",pol,none,0.0,0,0
1994,"Overall, this is an interesting direction.",pol,pol_positive,3.0,3,3
1995,"However there are a few key weaknesses in my view, not least that the practical utility of these metrics is not obvious, since they require supervision in the target domain.",pol,pol_negative,1.0,1,1
1996,"And while there is an argument to be made for the inherent interestingness of exploring these questions, this angle would be more compelling if multiple encoder architectures were explored and compared.",pol,pol_negative,1.0,1,1
1997,"+ The overarching questions that the authors set out to answer: How task-specific information is stored and to what extent this transfers, is inherently interesting and important.",pol,pol_positive,3.0,3,3
1998,+ The proposed metrics and simple and intuitive.,pol,pol_positive,3.0,3,3
1999,+ It is interesting that a few units seem to capture most task specific information.,pol,pol_positive,3.0,3,3
2000,- The envisioned scenario (and hence utility) of these metrics is a bit unclear to me here.,pol,pol_negative,1.0,1,1
2001,"As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task.",pol,pol_negative,1.0,0,1
2002,Yet the metrics proposed depend on supervision in the target domain.,pol,pol_negative,1.0,1,1
2003,"If we already have this, then -- as the authors themselves note -- it is trivial to simply try out different source datasets empirically on a target dev set.",pol,pol_neutral,2.0,1,2
2004,"It is argued that this is an issue because it requires training 2n networks, where n is the number of source tasks.",pol,none,0.0,0,0
2005,I am unconvinced that one frequently enough has access to a sufficiently large set of candidate source tasks for this to be a real practical issue.,pol,pol_negative,1.0,1,1
2006,"- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed.",pol,pol_negative,1.0,1,1
2007,"The LSTM architecture used is reasonable, but it would be nice to see how much results change (if at all) with alternative architectures.",pol,pol_positive,3.0,2,3
2008,"- The CFS metric depends on a hyperparameter (the ""retention ratio""), which here is arbitrarily set to 80% without any justification.",pol,pol_negative,1.0,1,1
2009,"- What is the motivation for the restriction to linear models? In the referenced probing paper, for example, MLPs were also used to explore whether attributes were coded for 'non-linearly'.",pol,pol_neutral,2.0,1,2
2010,This paper presents ReMixMatch an improved version of MixMatch.,pol,none,0.0,0,0
2011,The main contributions are the distribution alignment and the augmentation anchoring.,pol,none,0.0,0,0
2012,Distribution alignment rescales the predictions based on the difference between the model marginals and the ground truth running average estimation.,pol,none,0.0,0,0
2013,"Augmentation anchoring instead of computing the guessed probabilities on unlabelled data as the average probabilities on transformed samples (as in MixMatch), it considers as guessed labels the average probabilities obtained from weak transformations (flip+crop) even when using stronger transformations (Autoaugment like).",pol,none,0.0,0,0
2014,"The paper is well written, has interesting experiments and very impressive results.",pol,pol_positive,3.0,3,3
2015,"However, there are some negative points that the authors should clarify:",pol,none,0.0,0,0
2016,"- The final method is a mixup of many different techniques, thus, not a strong contribution, but many smaller contributions.",pol,pol_negative,1.0,1,1
2017,"- As shown in the ablation study, the main contribution on the obtained results seems to be the use of stronger transformations than in MixMatch.",pol,pol_negative,1.0,0,1
2018,"This is not so interesting, even though results are impressive.",pol,pol_negative,1.0,1,1
2019,"If this is the case, authors should state it more clearly in the paper that a large proportion of the gap in performance between MixMatch and ReMixMatch is the introduction of stronger transformations (Autoaugment style).",pol,pol_negative,1.0,1,1
2020,Overall the paper is well presented and contributes to further improve the performance on semi-supervised learning.,pol,pol_positive,3.0,3,3
2021,I there fore recommend it for acceptance.,pol,none,0.0,0,0
2022,"However, I would like to see in the paper a more general overview on the fact that strong transformations can further improve semi-supervised methods and ReMixMatch is a way to leverage those transformations.",pol,pol_neutral,2.0,2,2
2023,Additional comments:,pol,none,0.0,0,0
2024,"- Instead of using the rescaling trick for distribution alignment, what about enforcing the marginal distribution on the annotated data and the marginal distribution of the model to be similar with KL divergence? Would it be better or worse than the proposed approach?",pol,pol_neutral,2.0,2,2
2025,"The paper presents a new attack: Shadow Attack, which can generate imperceptible adversarial samples.",pol,none,0.0,0,0
2026,"This method is based on adding regularization on total variation, color change in each channel and similar perturbation in each channel.",pol,none,0.0,0,0
2027,This method is easy to follow and a lot of examples of different experiments are shown.,pol,pol_positive,3.0,3,3
2028,"However, I have several questions about motivation and method.",pol,none,0.0,0,0
2029,"First, the proposed attack method can yield adversarial perturbations to images that are large in the \ell_p norm.",pol,none,0.0,0,0
2030,"Therefore, the authors claim that the method can attack certified systems.",pol,none,0.0,0,0
2031,"However, attack in Wasserstein distance and some other methods can also do so.",pol,pol_negative,1.0,0,1
2032,They can generate adversarial examples whose \ell_p norm is large.,pol,none,0.0,0,0
2033,I think the author should have some discussions about these related methods.,pol,pol_neutral,2.0,2,2
2034,"Second, I notice that compared to the result in Table 1, PGD attack can yield better results [1].",pol,pol_negative,1.0,0,1
2035,I hope to see some discussions about this.,pol,none,0.0,0,0
2036,"Also, Table 1 is really confused. I would not understand the meaning if I am not familiar with the experiment settings.",pol,pol_negative,1.0,1,1
2037,"[1] Salman, Hadi, et al. ""Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers."" Neuips (2019).",pol,none,0.0,0,0
2038,"The authors make use of the theory of functional gradient, based on optimal transport, to develop a method that can promote the entropy of the generator distribution without directly estimating the entropy itself.",pol,none,0.0,0,0
2039,Theoretical results are provided as well as necessary experiments to support their technique's outperformance in some data sets.,pol,pol_positive,3.0,3,3
2040,"I found that this is an interesting paper, both original ideal and numerical results.",pol,pol_positive,3.0,3,3
2041,Summary: This paper proposes a meta-learning solution for problems involving optimizing multiple loss values.,pol,none,0.0,0,0
2042,"They use a simple (small mlp), discrete, stochastic controller to control applications of updates among a finite number of different update procedures.",pol,none,0.0,0,0
2043,"This controller is a function of heuristic features derived from the optimization problem, and is optimized using policy gradient either exactly in toy settings or in a online / truncated manor on larger problems.",pol,none,0.0,0,0
2044,"They present results on 4 settings: quadratic regression, MLP classification, GAN, and multi-task MNT.",pol,none,0.0,0,0
2045,They show promising performance on a number of tasks as well as show the controllers ability to generalize to novel tasks.,pol,none,0.0,3,0
2046,This is an interesting method and tackles a impactful problem.,pol,pol_positive,3.0,3,3
2047,"The setup and formulation (using PG to meta-optimize a hyper parameter controller) is not extremely novel (there have been similar work learning hyper parameter controllers), but the structure, the problem domain, and applications are.",pol,pol_neutral,2.0,3,2
2048,"The experimental results are through, and provide compelling proof that this method works as well as exploration as to why the method works (analyzing output softmax).",pol,pol_positive,3.0,3,3
2049,"Additionally the ""transfer to different models"" experiment is compelling.",pol,pol_positive,3.0,3,3
2050,Comments vaguely in order of importance:,pol,none,0.0,0,0
2051,1. I am a little surprised that this training strategy works.,pol,none,0.0,3,0
2052,"In the online setting for larger scale problems, your gradients are highly correlated and highly biased.",pol,pol_neutral,2.0,0,2
2053,"As far as I can tell, you are performing something akin to truncated back back prop through time with policy gradients.",pol,none,0.0,0,0
2054,The biased introduced via this truncation has been studied in great depth in [3] and shown to be harmful.,pol,pol_negative,1.0,0,1
2055,"As of now, the greedy nature of the algorithm is hidden across a number of sections (not introduced when presenting the main algorithm).",pol,pol_negative,1.0,1,1
2056,Some comment as to this bias -- or even suggesting that it might exist would be useful.,pol,pol_neutral,2.0,2,2
2057,"As of now, it is implied that the gradient estimator is unbiased.",pol,pol_neutral,2.0,0,2
2058,2.,pol,pol_neutral,2.0,0,2
2059,"Second, even ignoring this bias, the resulting gradients are heavily correlated.",pol,none,0.0,0,0
2060,Algorithm 1 shows no sign of performing batched updates on \phi or anything to remove these corrections.,pol,pol_neutral,2.0,1,2
2061,"Despite these concerns, your results seem solid.",pol,pol_positive,3.0,3,3
2062,"Nevertheless, further understanding as to this would be useful.",pol,pol_neutral,2.0,2,2
2063,3. The structure of the meta-training loop was unclear to me.,pol,pol_negative,1.0,1,1
2064,Algorithm 1 states S=1 for all tasks while the body -- the overhead section -- you suggest multiple trainings are required ( S>1?).,pol,none,0.0,1,0
2065,"4. If the appendix is correct and learning is done entirely online, I believe the initialization of the meta-parameters would matter greatly -- if the default task performed poorly with a uniform distribution for sampling losses, performance would be horrible.",pol,pol_neutral,2.0,1,2
2066,This seems like a limitation of the method if this is the case.,pol,pol_negative,1.0,1,1
2067,5. Clarity: The first half of this paper was easy to follow and clear. The experimental section had a couple of areas that left me confused. In particular:,pol,pol_negative,1.0,0,1
2068,5.1/Figure 1: I think there is an overloaded use of lambda?,pol,pol_neutral,2.0,2,2
2069,"My understanding as written that lambda is both used in the grid search (table 1) to find the best loss l_1 and then used a second location, as a modification of l_2 and completely separate from the grid search?",pol,none,0.0,2,0
2070,"6. Validation data / test sets: Throughout this work, it is unclear what / how validation is performed.",pol,pol_negative,1.0,1,1
2071,"It seems you performing controller optimization (optimizing phi), on the validation set loss, while also reporting scores on this validation set.",pol,pol_neutral,2.0,0,2
2072,This should most likely instead be a 3rd dataset.,pol,pol_neutral,2.0,1,2
2073,"You have 3 datasets worth of data for the regression task (it is still unclear, however, what is being used for evaluation), but it doesn't look like this is addressed in the larger scale experiments at all.",pol,pol_negative,1.0,1,1
2074,"Given the low meta-parameter count of the I don't think this represents a huge risk, and baselines also suffer from this issue (hyper parameter search on validation set) so I expect results to be similar.",pol,none,0.0,1,0
2075,"7. Page 4: ""When ever applicable, the final reward $$ is clipped to a given range to avoid exploding or vanishing gradients"".",pol,none,0.0,1,0
2076,It is unclear to me how this will avoid these.,pol,pol_negative,1.0,1,1
2077,"In particular, the ""exploding"" will come from the \nabla log p term, not from the reward (unless you have reason to believe the rewards will grow exponentially).",pol,pol_neutral,2.0,1,2
2078,"Additionally, it is unclear how you will have vanishing rewards given the structure of the learned controller.",pol,pol_negative,1.0,1,1
2079,"This clipping will also introduce bias, this is not discussed, and will probably lower variance.",pol,pol_negative,1.0,0,1
2080,"This is a trade off made in a number of RL papers so it seems reasonable, but not for this reason.",pol,pol_neutral,2.0,1,2
2081,"8. ""Beyond fixed schedules, automatically adjusting the training of G and D remains untacked"" -- this is not 100% true.",pol,pol_negative,1.0,1,1
2082,"While not a published paper, some early gan work [2] does contains a dynamic schedule but you are correct that this family of methods are not commonplace in modern gan research.",pol,pol_positive,3.0,1,3
2083,"9. Related work: While not exactly the same setting, I think [1] is worth looking at.",pol,pol_neutral,2.0,2,2
2084,"This is quite similar causing me pause at this comment: ""first framework that tries to learn the optimization schedule in a data-driven way"".",pol,pol_neutral,2.0,0,2
2085,"Like this work, they also lean a controller over hyper-parameters (in there case learning rate), with RL, using hand designed features.",pol,pol_neutral,2.0,0,2
2086,10. There seem to be a fair number of heuristic choices throughout.,pol,pol_neutral,2.0,0,2
2087,Why is IS squared in the reward for GAN training for example? Why is the scaling term required on all rewards?,pol,pol_neutral,2.0,2,2
2088,Having some guiding idea or theory for these choices or rational would be appreciated.,pol,pol_neutral,2.0,2,2
2089,11. Why is PPO introduced?,pol,pol_neutral,2.0,2,2
2090,"In algorithm 1, it is unclear how PPO would fit into this?",pol,pol_neutral,2.0,1,2
2091,More details or an alternative algorithm in the appendix would be useful.,pol,pol_neutral,2.0,2,2
2092,Why wasn't PPO used on all larger scale models? Does the training / performance of the meta-optimizer (policy gradient  vs ppo) matter?,pol,pol_neutral,2.0,2,2
2093,I would expect it would.,pol,none,0.0,0,0
2094,"This detail is not discussed in this paper, and some details -- such as the learning rate for the meta-optimizer I was unable to find.",pol,pol_negative,1.0,1,1
2095,"12. ""It is worth noting that all GAN K:1 baselines perform worse than the rest and are skipped in Figure 2, echoing statements (Arjovsky, Gulrajani, Deng) that more updates of G than D might be preferable in GAN training.",pol,none,0.0,1,0
2096,""" I disagree with this statement.",pol,pol_negative,1.0,1,1
2097,"The WGAN framework is built upon a loss that can be optimized, and should be optimized, until convergence (the discriminator loss is non-saturating) -- not the reverse (more G steps than D steps) as suggested here.",pol,pol_negative,1.0,1,1
2098,"Arjovsky does discuss issues with training D to convergence, but I don't believe there is any exploration into multiple G steps per D step as a solution.",pol,pol_neutral,2.0,1,2
2099,"13. Reproducibility seems like it would be hard. There are a few parameters (meta-learning rates, meta-optimizers) that I could not find for example and there is a lot of complexity.",pol,pol_negative,1.0,1,1
2100,14: Claims in paper seem a little bold / overstating.,pol,pol_neutral,2.0,1,2
2101,"The inception gain is marginal to previous methods, and trains slower than other baselines.",pol,pol_neutral,2.0,1,2
2102,"This is also true of MNT section -- there, the best baseline model is not even given equal training time!",pol,pol_neutral,2.0,1,2
2103,"There are highly positive points here, such as requiring less hyperparameter search / model evaluations to find performant models.",pol,pol_positive,3.0,0,3
2104,15. Figure 4a. Consider reformatting data (maybe histogram of differences? Or scatter plot).,pol,pol_neutral,2.0,1,2
2105,Current representation is difficult to read / parse.,pol,pol_negative,1.0,1,1
2106,Typos:,pol,none,0.0,0,0
2107,"page 2, ""objective term. on GANs, the AutoLoss: Capital o is needed.",pol,pol_neutral,2.0,1,2
2108,Page 3: Parameter Learning heading the period is not bolded.,pol,pol_neutral,2.0,1,2
2109,[1] Learning step size controllers for robust neural network training.,pol,pol_neutral,2.0,0,2
2110,Christian Daniel et. al.,pol,none,0.0,0,0
2111,[2]http://torch.ch/blog/2015/11/13/gan.html,pol,pol_neutral,2.0,0,2
2112,"[3] Understanding Short-Horizon Bias in Stochastic Meta-Optimization, Wu et.al.",pol,pol_neutral,2.0,0,2
2113,"Given the positives, and in-spite of the negatives, I would recommend to accept this paper as it discusses an interesting and novel approach when controlling multiple loss values.",pol,none,0.0,3,0
2114,This paper introduced a new architecture for input embeddings of neural language models: adaptive input representation (ADP).,pol,none,0.0,0,0
2115,ADP allowed a model builder to define a set of bands of input words with different frequency where frequent words have larger embedding size than the others.,pol,none,0.0,0,0
2116,The embeddings of each band are then projected into the same size.,pol,none,0.0,0,0
2117,This resulted in lowering the number of parameters.,pol,none,0.0,0,0
2118,Extensive experiments with the Transformer LM on WikiText-103 and Billion Word corpus showed that ADP achieved competitive perplexities.,pol,none,0.0,0,0
2119,"While tying weight with the output did not benefit the perplexity, it lowered the runtime significantly on Billion Word corpus.",pol,none,0.0,0,0
2120,Further analyses showed that ADP gained performance across all word frequency ranges.,pol,none,0.0,0,0
2121,"Overall, the paper was well-written and the experiments supported the claim.",pol,pol_positive,3.0,3,3
2122,The paper was very clear on its contribution.,pol,pol_positive,3.0,3,3
2123,The variable-size input of this paper was novel as far as I know.,pol,pol_positive,3.0,3,3
2124,"However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax.",pol,pol_negative,1.0,1,1
2125,The weight sharing was also needed further investigation and experimental data on sharing different parts.,pol,pol_negative,1.0,1,1
2126,"The experiments compared several models with different input levels (characters, BPE, and words).",pol,none,0.0,0,0
2127,The perplexities of the proposed approach were competitive with the character model with an advantage on the training time.,pol,pol_positive,3.0,3,3
2128,"However, the runtimes were a bit strange.",pol,pol_negative,1.0,1,1
2129,"For example, ADP and ADP-T runtimes were very close on WikiText-103 dataset but very different on Billion Word corpus (Table 3 and 4).",pol,pol_negative,1.0,0,1
2130,The runtime of ADP seemed to lose in term of scaling as well to BPE.,pol,pol_negative,1.0,1,1
2131,"Perhaps, the training time was an artifact of multi-GPU training.",pol,none,0.0,0,0
2132,Questions:,pol,none,0.0,0,0
2133,1. I am curious about what would you get if you use ADP on BPE vocab set?,pol,pol_neutral,2.0,2,2
2134,2. How much of the perplexity reduction of 8.7 actually come from ADP instead of the transformer and optimization?,pol,pol_neutral,2.0,2,2
2135,"This paper proposes a Frank-Wolfe based method, called DFW, for training Deep Network.",pol,none,0.0,0,0
2136,"The DFW method linearizes the loss function into a smooth one, and also adopts Nesterov Momentum to accelerate the training.",pol,none,0.0,0,0
2137,Both techniques have been widely used in the literature for similar settings.,pol,none,0.0,0,0
2138,"This paper mainly focuses on the algorithm part, but only empirically demonstrate the convergence results.",pol,none,0.0,1,0
2139,"After reading the authors’ feedback and the paper again, I think overall this is a good paper and should be of broader interest to the broader audience in machine learning community.",pol,pol_positive,3.0,3,3
2140,"In Section 6.1, the authors mention the good generalization is due to large number of steps at a high learning rate. Can we possibly get any theoretical justification on this?",pol,pol_neutral,2.0,2,2
2141,This paper uses multi class hinge loss as an example for illustration.,pol,none,0.0,0,0
2142,"Can this approach be applied for structure prediction, for example, various ranking loss?",pol,pol_neutral,2.0,2,2
2143,"This paper proposed to use the duality gap sup_f V(f, g*) – inf_g V(f*, g) as a metric for GAN training.",pol,none,0.0,0,0
2144,It proves that this metric is an upper bound of F-distance.,pol,none,0.0,0,0
2145,It also proves a generalization bound for this metric.,pol,none,0.0,0,0
2146,"Simulation resultson MNIST, CIFAR10, etc. are reported.",pol,none,0.0,0,0
2147,The contribution of this paper is incremental due to the following reasons.,pol,none,0.0,0,0
2148,1) The duality gap is only an upper bound of the F-distance.,pol,none,0.0,1,0
2149,This means that if the duality gap is zero then the learned distribution is the true distribution.,pol,none,0.0,0,0
2150,"However, the converse is not necessarily true: even if the algorithm starts with the true distribution, the duality gap may not be zero.",pol,none,0.0,1,0
2151,Thus the metric is not a proper metric.,pol,pol_negative,1.0,1,1
2152,The proof of the upper bound is straightforward.,pol,none,0.0,3,0
2153,2) Another issue is the gap between the min-max formulation and the real training algorithm.,pol,none,0.0,1,0
2154,"As for GAN, due to the inexact update, it is not really solving the min-max problem.",pol,pol_negative,1.0,1,1
2155,"For the proposed metric, it is also impossible to solve sup_f V(f, g*) and inf_g V(f*, g) to reasonable accuracy.",pol,pol_negative,1.0,1,1
2156,"Thus what the algorithm is really doing, perhaps, is to optimizing a new loss which is the sum of the original loss and and an extra term.",pol,none,0.0,1,0
2157,Viewing it as a “duality gap” seems to be far from the practical training.,pol,pol_negative,1.0,1,1
2158,"This discrepancy exists for GANs, but it is a bigger issue for the duality gap interpretation.",pol,pol_negative,1.0,0,1
2159,3) The simulation is not convincing.,pol,pol_negative,1.0,1,1
2160,"The reported FID for CIFAR10 using WGAN-GP is 54.4, which seems to be a bit high.",pol,pol_negative,1.0,1,1
2161,I’m not sure whether it is due to parameter choice or due to weak D/G networks used in the simulation.,pol,pol_negative,1.0,1,1
2162,"If the paper cannot compare various architecture, it is more convincing to at least use some standard architecture, like DCGAN. Or at least report the parameter tuning effort made for getting the results.",pol,pol_negative,1.0,1,1
2163,"This paper presents a pretty cool idea for enabling ""adaptive"" kernels for CNNs which allow dramatic reduction in the size of models with moderate to large performance drops.",pol,none,0.0,3,0
2164,"In at least one case, the training time is also significantly reduced (2x).",pol,none,0.0,0,0
2165,The best part about this paper is that the size of the models are much smaller; but the paper does offer any explanation of the value of this.,pol,pol_positive,3.0,3,3
2166,"For example, even a 1% drop in accuracy can be unacceptable; but in some applications (like cell phones and IOT devices) model size is critical.",pol,none,0.0,1,0
2167,The authors' should add some wording to explain this value.,pol,pol_negative,1.0,2,1
2168,"The ""adaptive""kernels the the authors talk about",pol,pol_positive,3.0,0,3
2169,are really a new class of nonlinear kernels.,pol,none,0.0,0,0
2170,It would be very interesting to see a discussion of the class of functions these nonlinear kernels represent.,pol,pol_neutral,2.0,2,2
2171,"This kind of discussion would give the reader  motivation for the choice of function, ideas for how to improve in this class of functions, and insight into why it works.",pol,pol_neutral,2.0,0,2
2172,The method presented is interesting; but it is not clear that it is present with enough detail for it's results to be replicated.,pol,pol_negative,1.0,3,1
2173,It would be nice if the authors pointed to a git repository with their code an experiments.,pol,pol_negative,1.0,2,1
2174,"More importantly, the results presented are quite meager.",pol,pol_negative,1.0,1,1
2175,"If this is a method for image recognition, it would be better to present results for a more substantial image recognition problem than MNIST and CIFAR-10.",pol,pol_negative,1.0,1,1
2176,"And the analysis of the ""dynamic range"" of the algorithim is missing.",pol,pol_negative,1.0,1,1
2177,How do performance and model size trade off?,pol,pol_negative,1.0,1,1
2178,How were the number of layers and kernels chosen?,pol,pol_negative,1.0,2,1
2179,Was the 5x10x20x10 topology used for MNIST the only topology tried?,pol,pol_negative,1.0,2,1
2180,That would be very surprising.,pol,pol_negative,1.0,1,1
2181,What is the performance on all of the other topologies tried for the proposed algorithm?,pol,none,0.0,2,0
2182,Was crossvalidation used to select the topology?,pol,pol_negative,1.0,2,1
2183,"If so, what was the methodology.",pol,pol_negative,1.0,1,1
2184,"Additionally, some readers may find this paper a little difficult to read due to (1) lack of clarity in the writing, e.g., the first three paragraphs in Section 3; (2) omitted details, e.g., how much overlap exists between kernels (Figs. 1, 2, and 4 suggests there is no overlap - this should be made clear); and (3) poor grammar and nonstandard terminology, e.g., the authors' use of the word ""energy"" and the phrase ""degradation problem"".",pol,pol_negative,1.0,1,1
2185,All of these issues should be addressed in a future version of the paper.,pol,pol_negative,1.0,2,1
2186,Not sure why Eqns. 2 and 9 need any parentheses,pol,pol_negative,1.0,1,1
2187,.  They should be removed.,pol,pol_negative,1.0,2,1
2188,The paper is really interesting.,pol,pol_positive,3.0,3,3
2189,Set prediction problem has lots of applications in AI applications and the problem has not been conquered by deep networks.,pol,pol_positive,3.0,0,3
2190,The paper proposes a formulation to learn the distribution over unobservable permutation variables based on deep networks and uses a MAP  estimator for inference.,pol,none,0.0,0,0
2191,It has object detection applications.,pol,pol_positive,3.0,0,3
2192,The results show that it can outperform YOLOv2 and Faster R-CNN in a small pedestrian detection dataset which contains heavy occlusions.,pol,none,0.0,0,0
2193,The limitation is clearly stated in the last part of the paper that the number of possible permutations exponentially grows with the maximum set size (cardinality).,pol,pol_positive,3.0,0,3
2194,"In the author response period, I would like the author give more details about the pedestrian detection experiments, such as how many dense layers are used after ResNet-101, what are the training and inference time, is it possible to report results on PASCAL VOC (only the person class).",pol,pol_neutral,2.0,2,2
2195,The method is exciting for object detection funs.,pol,pol_positive,3.0,3,3
2196,I would like to encourage the authors to release the code and let the whole object detection community overcome the limitation in the paper.,pol,pol_neutral,2.0,2,2
2197,The authors are proposing an end-to-end learning-based framework that can be incorporated into all classical frequency estimation algorithms in order to learn the underlying nature of the data in terms of the frequency in data streaming settings and which does not require labeling.,pol,none,0.0,0,0
2198,"According to my understanding, the other classical streaming algorithms also do not require labeling but the novelty here I guess lie in learning the oracle (HH) which feels like a logical thing to do as such learning using neural networks worked well for many other problems.",pol,pol_neutral,2.0,0,2
2199,The problem formulation and applications of this research are well explained and the paper is well written for readers to understand.,pol,pol_positive,3.0,3,3
2200,The experiments show that the learning based approach performs better than their all unlearned versions.,pol,pol_positive,3.0,0,3
2201,But the only negative aspect is the basis competitor algorithms are very simple in nature without any form of learning and that are very old.,pol,pol_negative,1.0,1,1
2202,"So, I am not sure if there are any new machine learning based frequency estimation algorithms.",pol,none,0.0,1,0
2203,This paper proposes PowerSGD for improving SGD to train deep neural networks.,pol,none,0.0,0,0
2204,The main idea is to raise the stochastic gradient to a certain power.,pol,none,0.0,0,0
2205,Convergence analysis and experimental results on CIFAR-10/CIFAR-100/Imagenet and classical CNN architectures are given.,pol,none,0.0,0,0
2206,"Overall, this is a clearly-written paper with comprehensive experiments.",pol,pol_positive,3.0,3,3
2207,My major concern is whether the results are significant enough to deserve acceptance.,pol,pol_negative,1.0,1,1
2208,The proposed method PowerSGD is an extension of the method in Yuan et al. (extended to handle stochastic gradient and momentum).,pol,pol_negative,1.0,0,1
2209,"I am not sure how novel the convergence analysis for PowerSGD is, and it would be nice if the authors could discuss technical challenges they overcome in the introduction.",pol,pol_negative,1.0,2,1
2210,"(As a disclamer I want to point out I'm not an expert in GANs and have only a basic understanding of the sub-field, but arguably this would make me target audience of this paper).",pol,none,0.0,1,0
2211,"The authors presents a large scale study comparing a large number of GAN experiments, in this study they compare various choices of architechtures, losses and hyperparameters.",pol,none,0.0,0,0
2212,"The first part of the paper describes the various losses, architectures, regularization and normalization schemes; and the second part describes the results of the comparison experiments.",pol,none,0.0,0,0
2213,"While I wish there were more such studies -- as I believe reproducing past results experimentally is important, and so is providing practical advice for practitioners -- this work in many parts hard to follow, and it is hard to get lot of new insight from the results, or a better understanding of GANs.",pol,pol_negative,1.0,1,1
2214,"As far I can see the most important take home message of the paper can be summarized in ""one should consider non-saturating GAN loss and spectral normalization as default choices [...] Given additional computational budget, we suggest adding",pol,none,0.0,1,0
2215,the,pol,none,0.0,0,0
2216,"gradient penalty [...] and train the model until convergence"".",pol,none,0.0,0,0
2217,Pros:,pol,none,0.0,0,0
2218,- available source code,pol,pol_positive,3.0,2,3
2219,- large number of experiments,pol,pol_positive,3.0,1,3
2220,Cons:,pol,none,0.0,0,0
2221,"- the exposition could be improved, in particular the description of the plots is not very clear, I'm still not sure exactly what they show",pol,pol_negative,1.0,1,1
2222,"- not clear what the target audience of the first part (section 2) is, it is too technical for a survey intended for outsiders, and discusses subtle points that are not easy to understand without more knowledge, but at the same time seems unlikely to give additional insight to an insider",pol,pol_negative,1.0,1,1
2223,"- limited amount of new insight, which is limiting as new and better understanding of GANs and practical guidelines are arguably the main contribution of a work of this type",pol,pol_negative,1.0,1,1
2224,Some suggestions that I think could make the paper stronger,pol,none,0.0,0,0
2225,- I believe that in particular section 2 goes into too many mathematical details and subtleties that do not really add a lot.,pol,pol_negative,1.0,1,1
2226,"I think that either the reader already understand those concepts well (which I admit, I don't really, I'm merely curious about GANs and have been following the action from a distance, hence my low confidence rating to this review), or if they does not, it will be very hard to get much out of it.",pol,pol_negative,1.0,1,1
2227,"I would leave out some of the details, shortening the whole sections, and focus more on making a few of the concepts more understandable, and potentially leaving more space for a clearer description of the results",pol,none,0.0,1,0
2228,"- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?",pol,pol_negative,1.0,2,1
2229,"- ""the variance of models obtained by Guassian Process regression is handled implicitely so we tran each model once""? I do not understand what this means, and I work with hyper-parameter tuning using gaussian processes daily.",pol,pol_negative,1.0,1,1
2230,It should probably be rephrased,pol,pol_neutral,2.0,1,2
2231,"- at the start of section 3: what is an ""experiment""?",pol,pol_negative,1.0,2,1
2232,"- in 3.1 towards the end of the first paragraph, what is a ""study"", is that the same as experiment or something different?",pol,pol_negative,1.0,2,1
2233,- (minor) stating that lower is better in the graphs might be useful,pol,pol_neutral,2.0,2,2
2234,"- (minor) typo in page 5 ""We use a fixed the number""",pol,pol_neutral,2.0,1,2
2235,TLDR: The function these deep set networks can approximate is too limited to call these networks universal equivariant set networks.,pol,none,0.0,1,0
2236,Authors should scope the paper to the specific function family these networks can approximate.,pol,pol_negative,1.0,2,1
2237,No baseline comparison with GraphNets.,pol,pol_negative,1.0,1,1
2238,The paper proposes theoretical analysis on a set of networks that process features independently through MLPs + global aggregation operations.,pol,none,0.0,0,0
2239,"However, the function of interest is limited to a small family of affine equivariant transformations.",pol,pol_negative,1.0,1,1
2240,A more general function is,pol,pol_negative,1.0,0,1
2241,\begin{equation},pol,none,0.0,0,0
2242,"P(X)_i = Ax_i + \sum_{j \in N(x_i, X)} B_{(x_j, x_i)} x_j + c",pol,none,0.0,0,0
2243,\end{equation},pol,none,0.0,0,0
2244,"where $N(x_i, X)$ is the set of index of neighbors within the set $X$. It is trivial to show that this function is permutation equivariant.",pol,none,0.0,1,0
2245,"Then, can the function family the authors used in the paper approximate this function?",pol,pol_negative,1.0,2,1
2246,No.,pol,pol_negative,1.0,0,1
2247,Can the proposed permutation equivariant function represent all function the authors used in the paper? Yes.,pol,pol_positive,3.0,2,3
2248,"1) If $B=0$, then the proposed function becomes MLP.",pol,none,0.0,0,0
2249,"2) If $A=0, N(x_i, X) = [n]$ and $B_{(x_j, x_i)} \leftarrow B$, then this is $\mathbf{1}\mathbf{1}^TXB$, the global aggregation function.",pol,none,0.0,0,0
2250,"Also, this is the actual function that a lot of people are interested in.",pol,none,0.0,0,0
2251,Let me go over few more examples.,pol,none,0.0,0,0
2252,"3) If $N(x_i, X) = $adjacency on a graph and $B_{(x_j, x_i)} \leftarrow B$, then this is a graph neural network ""convolution"" (it is not a convolution)",pol,none,0.0,0,0
2253,"Example adjacency $N(x_i, X) = \{j \;| \; \|x_i - x_j\|_p < \delta, x_j \in X\}$.",pol,none,0.0,0,0
2254,\begin{equation},pol,none,0.0,0,0
2255,"\text{GraphOp}(X)_i = Ax_i + \sum_{j \in \{j \;| \; \|x_i - x_j\|_p < \delta, x_j \in X\}} Bx_j + c",pol,none,0.0,0,0
2256,\end{equation},pol,none,0.0,0,0
2257,"4) If $x_i = [r,g,b,u,v]$ where $[r,g,b]$ is the color, $[u,v]$ is the pixel coordinate and $N(x_i, X) =$ pixel neighbors within some kernel size, $B(x_j, x_i)$ to be the block diagonal matrix only for the first three dimensions and 0 for the rest, then this is the 2D convolution.",pol,none,0.0,0,0
2258,"Again, the above function is a more general permutation equivariant function that can represent: a graph neural network layer, a convolution, MLP, global pooling and is one of the most widely used functions in the ML community, not MLP + global aggregation.",pol,none,0.0,0,0
2259,Regarding the experiment metrics and plots:,pol,none,0.0,0,0
2260,"On the Knapsack test, the metric of interest is not the accuracy of individual prediction.",pol,pol_negative,1.0,1,1
2261,"Rather, whether the network has successfully predicted the optimal solution, or how close the prediction is to the solution.",pol,none,0.0,0,0
2262,For example: success rate within the epsilon radius of the optimal solution while satisfying all the constraints.,pol,none,0.0,0,0
2263,Fail otherwise.,pol,none,0.0,0,0
2264,"If these networks can truly solve these problems, authors should report the success rate while varying the threshold, not individual accuracy of the items which can be arbitrarily high by violating constraints.",pol,pol_negative,1.0,1,1
2265,"Also, the authors should compare with few more graphnet + transmission layer (GraphNetST) baselines with the graph layers: $P(X)_i = Ax_i + \sum_{j \in N(x_i, X)} Bx_j + c$ and the same single transmission layer $\mathbf{1}\mathbf{1}^TXB$ in PointNetST.",pol,pol_negative,1.0,2,1
2266,PointNet is a specialization of graphnets and GraphNetST should be added as a baseline with reasonable adjacency.,pol,pol_negative,1.0,1,1
2267,Also experiment figures are extremely compact. Try using log scale or other lines to make the gaps wider.,pol,pol_negative,1.0,1,1
2268,Minor,pol,none,0.0,0,0
2269,I am quite confused with the name PointNetST.,pol,pol_negative,1.0,1,1
2270,"Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer.",pol,pol_negative,1.0,0,1
2271,"The convention is B -> B', not A + B -> A'.",pol,none,0.0,1,0
2272,"In this case, A: PointNet, B: DeepSet",pol,pol_negative,1.0,0,1
2273,Lemma 3 is too trivial.,pol,pol_negative,1.0,1,1
2274,The paper is not very self contained.,pol,pol_negative,1.0,1,1
2275,Spare few lines of equations to define what are DeepSets and PointNetSeg in the paper and point out the difference since these networks are used throughout the paper extensively without proper mathematical definition.,pol,pol_negative,1.0,1,1
2276,P.2 power sum multi-symmetric polynomials.,pol,none,0.0,0,0
2277,"""For a vector $x \in R^K$ and a multi-index ..."" I think it was moved out of the next paragraph",pol,pol_negative,1.0,1,1
2278,since  the same $x$ is defined again as $x \in R^n$ again in the next sentence.,pol,none,0.0,0,0
2279,"Also, try using the consistent dimension for x throughout the paper, it confuses the reader.",pol,pol_negative,1.0,1,1
2280,This manuscript applies transfer learning for protein surface prediction.,pol,none,0.0,0,0
2281,The problem is important and  the idea is novel and interesting.,pol,pol_positive,3.0,3,3
2282,"However, the  transfer learning model is unclear.",pol,pol_negative,1.0,1,1
2283,Pros:  interesting and novel idea,pol,pol_positive,3.0,3,3
2284,"Cons:  unclear transfer learning model, insufficient experiments.",pol,pol_negative,1.0,1,1
2285,"Detail: section 4 describes the transfer learning model used in the work, but the description is unclear.",pol,pol_negative,1.0,1,1
2286,It is unknown the used model is a new model or existing model.,pol,pol_negative,1.0,1,1
2287,"Besides, in the experiments, the proposed method is not compared to other transfer learning methods.",pol,pol_negative,1.0,1,1
2288,"Thus, the evidence of the experiments is not enough.",pol,pol_negative,1.0,1,1
2289,The paper suggests a new regularization technique which can be added on top of those used in AWD-LSTM of Merity et al. (2017) with little overhead.,pol,none,0.0,0,0
2290,This is a well-written paper with a clear structure.,pol,pol_positive,3.0,3,3
2291,"The experiments are presented in a clear and understandable fashion, and the evaluation seems thorough.",pol,pol_positive,3.0,3,3
2292,"The methodology seems sound, and the authors present the reader with all the information needed to replicate the experiments.",pol,pol_positive,3.0,3,3
2293,I would only suggest evaluating this technique on AWD-LSTM-MoS of Yang et al. (2017) to get a more complete picture.,pol,pol_positive,3.0,1,3
2294,References,pol,none,0.0,0,0
2295,"- Merity, S., Keskar, N.S. and Socher, R., 2017. Regularizing and optimizing LSTM language models. arXiv preprint arXiv:1708.02182.",pol,none,0.0,0,0
2296,"- Yang, Z., Dai, Z., Salakhutdinov, R. and Cohen, W.W., 2017. Breaking the softmax bottleneck: A high-rank RNN language model.",pol,none,0.0,0,0
2297,arXiv preprint arXiv:1711.03953.,pol,none,0.0,0,0
2298,The paper makes a significant attempt at solving one of the practical problems in machine learning -- learning from many noisy and limited number of clean labels.,pol,none,0.0,0,0
2299,This setting is presumably more practical than the setting of few-shot learning.,pol,pol_positive,3.0,0,3
2300,Noisy labels are often abundantly available and investing in methods that can take the noise into account for building a discriminative model is quite timely.,pol,pol_positive,3.0,3,3
2301,"To be honest, the theoretical contribution of the paper is limited.",pol,pol_negative,1.0,1,1
2302,The authors make use of the nearest neighbour graph obtained from a reduced-dimensional set of features to compute the weights of the noisy labels that must guide the predictive model.,pol,none,0.0,0,0
2303,"From this perspective, the paper seems like an application of existing tools (such as CNN, graph convolutional network and binary classification).",pol,pol_negative,1.0,3,1
2304,"However, that does not undermine the superior results the authors have received in the novel application they have targeted.",pol,pol_positive,3.0,1,3
2305,I appreciate the effort that went validating these ideas with real-world datasets.,pol,none,0.0,3,0
2306,"In future, I would like to see a joint approach to such training, where the function g(), the nearest neighbour graph loss and the classification loss are all tied in the same objective function and are optimized jointly.",pol,pol_neutral,2.0,2,2
2307,The paper has few really minor grammatical errors and typos. Please fix those before uploading the final draft.,pol,pol_negative,1.0,0,1
2308,"The paper proposed an interesting algorithm and direction, which tries fill the gap of NN in tabular data learning.",pol,pol_positive,3.0,0,3
2309,"My concern is, given this is an empirical work,  the number of datasets used in evolution is a bit small.",pol,pol_negative,1.0,1,1
2310,"Also, xgboost was the winning algorithm for many competitions for tabular data, would be good to compare the NN with properly optimised xgboost.",pol,pol_neutral,2.0,2,2
2311,"In chapter 2, related work.",pol,none,0.0,0,0
2312,"The authors state that ""tree-based models still yield two obvious shortages: (1) Hard to be integrated into complex end-to-end frameworks... (2) Hard to learn from streaming data.",pol,none,0.0,0,0
2313,To me these two reasoning statements are not particularly convincing. One could also say:,pol,pol_negative,1.0,1,1
2314,NN models yield two obvious shortages: (1) Hard to be integrated into complex end-to-end frameworks... (2) Hard to learn from streaming data...,pol,none,0.0,1,0
2315,"Actually, tree ensemble based algorithms, eg Hoeffding tree ensembles, are among the best performed algorithms for data streaming tasks.",pol,none,0.0,0,0
2316,Verifying the properties of neural networks can be very difficult.,pol,none,0.0,1,0
2317,Instead of,pol,none,0.0,0,0
2318,"finding a formal proof for a property that gives a True/False answer, this",pol,none,0.0,0,0
2319,paper proposes to take a sufficiently large number of samples around the input,pol,none,0.0,0,0
2320,point point and estimate the probability that a violation can be found.,pol,none,0.0,0,0
2321,Naive,pol,none,0.0,0,0
2322,Monte-Carlo (MC) sampling is not effective especially when the dimension is,pol,none,0.0,1,0
2323,"high, so the author proposes to use adaptive multi-level splitting (AMLS) as a",pol,none,0.0,0,0
2324,sampling scheme.,pol,none,0.0,0,0
2325,This is a good application of AMLS method.,pol,pol_positive,3.0,3,3
2326,Experiments show that AMLS can make a good estimate (similar quality as naive,pol,pol_positive,3.0,0,3
2327,"MC with a large number of samples) while using much less samples than MC, on",pol,pol_positive,3.0,0,3
2328,both small and relatively larger models,pol,pol_positive,3.0,0,3
2329,.,pol,none,0.0,0,0
2330,"Additionally, the authors conduct",pol,pol_positive,3.0,0,3
2331,sensitivity analysis and run the proposed algorithm with many different,pol,pol_positive,3.0,0,3
2332,"parameters (M, N, pho, etc), which is good to see.",pol,pol_positive,3.0,0,3
2333,I have some concerns on this paper:,pol,pol_negative,1.0,0,1
2334,I have doubts on applying the proposed method to higher dimensional inputs.,pol,pol_negative,1.0,1,1
2335,In,pol,pol_negative,1.0,0,1
2336,"section 6.3, the authors show an experiments in this case, but only on a dense",pol,pol_negative,1.0,1,1
2337,"ReLU network with 2 hidden layers, and it is unknown if it works in general.",pol,pol_negative,1.0,1,1
2338,How does the number of required samples increases when the dimension of input,pol,pol_neutral,2.0,1,2
2339,(x) increases?,pol,pol_neutral,2.0,2,2
2340,"Formally, if there exists a violation (counter-example) for a certain property,",pol,pol_neutral,2.0,0,2
2341,"and given a failure probability p, what is the upper bound of number of samples",pol,pol_neutral,2.0,1,2
2342,"(in terms of input dimension, and other factors) required so that the",pol,pol_neutral,2.0,0,2
2343,probability we cannot detect this violation with probability less than p?,pol,pol_neutral,2.0,0,2
2344,"Without such a guarantee, the proposed method is not very useful because we",pol,pol_negative,1.0,1,1
2345,have no idea how confident the sampling based result is.,pol,pol_negative,1.0,1,1
2346,Verification needs,pol,pol_negative,1.0,0,1
2347,"something that is either deterministic, or a probabilistic result with a small",pol,pol_negative,1.0,0,1
2348,"and bounded failure rate, otherwise it is not really a verification method.",pol,pol_negative,1.0,1,1
2349,The experiments of this paper lack comparisons to certified verification,pol,pol_negative,1.0,1,1
2350,methods. There are some scalable property verification methods that can give a,pol,pol_negative,1.0,0,1
2351,lower bound on the input perturbation (see [1][2][3]),pol,pol_negative,1.0,0,1
2352,.,pol,none,0.0,0,0
2353,These methods can,pol,pol_positive,3.0,0,3
2354,"guarantee that when epsilon is smaller than a threshold, no violations can be",pol,pol_positive,3.0,0,3
2355,found.,pol,pol_positive,3.0,0,3
2356,"On the other hand, adversarial attacks give an upper bound of input",pol,none,0.0,0,0
2357,perturbation by providing a counter-example (violation).,pol,none,0.0,0,0
2358,The authors should,pol,pol_neutral,2.0,2,2
2359,compare the sampling based method with these lower and upper bounds.,pol,pol_neutral,2.0,0,2
2360,For,pol,pol_neutral,2.0,0,2
2361,"example, what is log(I) for epsilon larger than upper bound?",pol,pol_neutral,2.0,1,2
2362,"Additionally, in section 6.4, the results in Figure 2 also does not look very",pol,pol_negative,1.0,1,1
2363,positive - it unlikely to be true that an undefended network is predominantly,pol,pol_negative,1.0,1,1
2364,"robust to perturbation of size epsilon = 0.1. Without any adversarial training,",pol,pol_negative,1.0,0,1
2365,adversarial examples (or counter-examples for property verification) with L_inf,pol,pol_negative,1.0,0,1
2366,distortion less than 0.1 (at least on some images) should be able to find.,pol,none,0.0,1,0
2367,It,pol,pol_neutral,2.0,0,2
2368,is better to conduct strong adversarial attacks after each epoch and see what,pol,pol_neutral,2.0,1,2
2369,are the epsilons of adversarial examples.,pol,pol_neutral,2.0,0,2
2370,Ideas on further improvement:,pol,pol_neutral,2.0,0,2
2371,The proposed method can become more useful if it is not a point-wise method.,pol,pol_neutral,2.0,1,2
2372,"If given a point, current formal verification method can tell if a property is",pol,pol_neutral,2.0,0,2
2373,hold or not.,pol,pol_neutral,2.0,0,2
2374,"However, most formal verification method cannot deal with a input",pol,none,0.0,0,0
2375,"drawn from a distribution randomly (for example, an unseen test example).",pol,none,0.0,0,0
2376,This,pol,pol_neutral,2.0,0,2
2377,is the place where we really need a probabilistic verification method.,pol,pol_neutral,2.0,1,2
2378,The,pol,pol_negative,1.0,0,1
2379,setting in the current paper is not ideal because a probabilistic estimate of,pol,pol_negative,1.0,1,1
2380,"violation of a single point is not very useful, especially without a guarantee",pol,pol_negative,1.0,0,1
2381,of failure rates.,pol,pol_negative,1.0,0,1
2382,"For finding counter-examples for a property, using gradient based methods might",pol,pol_neutral,2.0,0,2
2383,be a better way.,pol,pol_neutral,2.0,1,2
2384,The authors can consider adding Hamiltonian Monte Carlo,pol,pol_neutral,2.0,1,2
2385,to,pol,pol_neutral,2.0,0,2
2386,this framework,pol,pol_neutral,2.0,0,2
2387,(See [4]).,pol,none,0.0,0,0
2388,References:,pol,none,0.0,0,0
2389,"There are some papers from the same group of authors, and I merged them to one.",pol,none,0.0,0,0
2390,"Some of these papers are very recent, and should be helpful for the authors",pol,none,0.0,2,0
2391,to further improve their work.,pol,none,0.0,0,0
2392,"[1] ""AI2: Safety and Robustness Certification of Neural Networks with Abstract",pol,none,0.0,0,0
2393,"Interpretation"", IEEE S&P 2018 by Timon Gehr, Matthew Mirman, Dana",pol,none,0.0,0,0
2394,"Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, Martin Vechev",pol,none,0.0,0,0
2395,"(see also ""Differentiable Abstract Interpretation for Provably Robust Neural",pol,none,0.0,0,0
2396,"Networks"", ICML 2018.",pol,none,0.0,0,0
2397,"by Matthew Mirman, Timon Gehr, Martin Vechev.  They also",pol,none,0.0,0,0
2398,"have a new NIPS 2018 paper ""Fast and Effective Robustness Certification"" but is",pol,none,0.0,1,0
2399,not on arxiv yet),pol,none,0.0,1,0
2400,"[2] ""Efficient Neural Network Robustness Certification with General Activation",pol,none,0.0,0,0
2401,"Functions""",pol,none,0.0,0,0
2402,", NIPS 2018. by Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui",pol,none,0.0,0,0
2403,"Hsieh, Luca Daniel.",pol,none,0.0,0,0
2404,"(see also ""Towards Fast Computation of Certified Robustness for ReLU Networks"",",pol,none,0.0,0,0
2405,"ICML 2018 by Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh,",pol,none,0.0,0,0
2406,"Duane Boning, Inderjit S. Dhillon, Luca Danie.)",pol,none,0.0,0,0
2407,[3] Provable defenses against adversarial examples via the convex outer,pol,none,0.0,0,0
2408,"adversarial polytope, NIPS 2018. by Eric Wong, J. Zico Kolter.",pol,none,0.0,0,0
2409,"(see also ""Scaling provable adversarial defenses"", NIPS 2018 by the same authors)",pol,none,0.0,0,0
2410,"[4] ""Stochastic gradient hamiltonian monte carlo."" ICML 2014. by Tianqi Chen,",pol,none,0.0,0,0
2411,"Emily Fox, and Carlos Guestrin.",pol,none,0.0,0,0
2412,============================================,pol,none,0.0,0,0
2413,"After discussions with the authors, they agree to revise the paper according to our discussions and my primary concerns of this paper have been resolved. Thus I increased my rating.",pol,none,0.0,0,0
2414,This paper proposes a hybrid machine learning algorithm using Gradient Boosted Decision Trees (GBDT) and Deep Neural Networks (DNN).,pol,none,0.0,0,0
2415,The intended research direction on tabular data is essential and promising.,pol,pol_positive,3.0,3,3
2416,"However, the proposed technique does not seem to be handling the problem foundationally well.",pol,pol_negative,1.0,1,1
2417,It seems heavily dependent on GBDT.,pol,pol_negative,1.0,0,1
2418,It also shows itself in the results that final algorithm is almost indistinguishable from GBDT regarding results.,pol,pol_negative,1.0,0,1
2419,"Moreover, I  don't think that the data sets in experiments are good enough to cover the importance and the nature of the problem.",pol,pol_negative,1.0,1,1
2420,Pros:,pol,none,0.0,0,0
2421,"-This is a crucial line of research direction that aims to make DNNs applicable to many real-world problems (beyond speech and vision) in which discrete data and heterogeneous features exist such as engagement prediction, recommendation, and search.",pol,pol_positive,3.0,0,3
2422,-The starting point of using GBDT seems like a good choice.,pol,pol_positive,3.0,3,3
2423,-The Paper is mostly well written except occasional repetitions and missing acronym definitions.,pol,pol_positive,3.0,3,3
2424,Cons:,pol,none,0.0,0,0
2425,"-The proposed technique does not seem to be original enough, and it does not handle the problem foundationally well.",pol,pol_negative,1.0,1,1
2426,I do not think that there is enough justification/demonstration for the fact that a general NN solution for Tabular Data invented.,pol,pol_negative,1.0,1,1
2427,The proposed technique is heavily dependent on GBDT (Indeed the algorithm and the learned trees are used at least three times).,pol,pol_negative,1.0,1,1
2428,"This shows itself in the results; i.e., the proposed algorithm is either negligibly performing better than GBDT or when  GBDT dependence removed, it performs worse. It seems to me",pol,pol_negative,1.0,0,1
2429,"that (except the minor small section of streaming data), the paper is more like a proper verification of how tree-based learning algorithms work very well in tabular data--which is far from the basis of the paper and does not make the paper novel enough for ICLR.",pol,pol_negative,1.0,1,1
2430,-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data.,pol,pol_negative,1.0,1,1
2431,-In the provided benchmark data sets the depth of the analysis seems to be enough.,pol,pol_positive,3.0,3,3
2432,"However, in the proposed domain of tabular data, often data sets are significantly more high dimensional in reality and include at least one set of sparse large dimensional features",pol,pol_negative,1.0,0,1
2433,"(e.g., unstructured raw text for the search queries.) In such scenarios, it had been showed that wide-and-deep NNs perform decently.",pol,none,0.0,0,0
2434,However such problems are entirely missing in the results section.,pol,pol_negative,1.0,1,1
2435,I also think that this is a lost opportunity for the authors as they could be showing that it is the NN part contributing.,pol,pol_negative,1.0,1,1
2436,This paper studies the problem of generating contracts by a principal to incentive agents to optimally accomplish multiagent tasks.,pol,none,0.0,0,0
2437,"The setup of the environment is that the agents have certain skills and preferences for activities, which the principal must learn to act optimally.",pol,none,0.0,0,0
2438,"The paper takes a combined approach of agent modeling to infer agent skills and preferences, and a deep reinforcement learning approach to generate contracts.",pol,none,0.0,0,0
2439,The evaluation of the approach is fairly thorough.,pol,none,0.0,1,0
2440,The main novel contribution of the paper is to introduce the principal-agent problem to the deep multiagent reinforcement learning literature.,pol,pol_positive,3.0,0,3
2441,My concerns are:,pol,none,0.0,0,0
2442,"- The paper should perform a literature search on related work from operations research, including especially principal-agent problems, which are not currently surveyed, and perhaps also optimal scheduling problems.",pol,pol_neutral,2.0,2,2
2443,- How do the problems introduced either map onto real applications or map onto environments studied in existing literature (such as in operations research)?,pol,pol_neutral,2.0,2,2
2444,- More details should be given on the mind tracker module.,pol,pol_neutral,2.0,2,2
2445,- Is it necessary to use deep reinforcement learning for contract generation?,pol,pol_neutral,2.0,2,2
2446,"If the agent modeling is good, the optimal contracts look like they are probably simple to compute directly in the environments studied.",pol,none,0.0,0,0
2447,"Overall, the paper is somewhat interesting and relatively technically sound, but the contribution seems marginal. The problems studied seem pulled out a hat, when they could be situated in specific existing literature.",pol,none,0.0,1,0
2448,"Dual Block-Coordinate Frank-Wolfe (Dual-BCFW) has been widely used in the literature of non-smooth and strongly-convex stochastic optimization problems, such as (structural) Support Vector Machine.",pol,none,0.0,0,0
2449,"To my knowledge, the submission is the first sound attempt to adapt this type of Dual-based algorithm for optimization of Deep Neural Network, which employs a proximal-point method that linearizes not the whole loss function but only the DNN (up to the logits) to form a convex subproblem and then deal with the loss part in the dual.",pol,pol_positive,3.0,0,3
2450,"The attempt is not perfect (actually with a couple of issues detailed below), but the proposed approach is inspiring and I personally would love it published to encourage more development along this thread.",pol,pol_positive,3.0,3,3
2451,The following points out a couple of items that could probably help further improve the paper.,pol,none,0.0,0,0
2452,*FW vs BCFW*,pol,none,0.0,0,0
2453,"The algorithm employed in the paper is actually not Frank-Wolfe (FW) but Block-Coordinate Frank-Wolfe (BCFW), as it minimizes w.r.t. a block of dual variables belonging to the min-batch of samples.",pol,pol_neutral,2.0,1,2
2454,*Batch Size*,pol,none,0.0,0,0
2455,"Though the algorithm can be easily extended to the min-batch case, the author should discuss more how the batch size is interpreted in this case (i.e. minimizing w.r.t. a larger block of dual variables belonging to the batch of samples) and the algorithmic block (Algorithm 1) should be presented in a way reflecting the batch size since this is the way people use an algorithm in practice (to improve the utilization rate of a GPU).",pol,pol_neutral,2.0,1,2
2456,*Convex-Conjugate Loss*,pol,none,0.0,0,0
2457,The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss).,pol,pol_negative,1.0,0,1
2458,All convex loss function can derive a dual formulation based on its convex-conjugate.,pol,none,0.0,0,0
2459,"See [1,2] for examples.",pol,none,0.0,0,0
2460,It would be more insightful to compare SGD vs dual-BCFW when both of them are optimizing the same loss functions (either hinge loss or cross-entropy loss) in the experimental comparison.,pol,pol_neutral,2.0,1,2
2461,"[1] Shalev-Shwartz, Shai, and Tong Zhang. ""Stochastic dual coordinate ascent methods for regularized loss minimization."" JMLR (2013)",pol,none,0.0,0,0
2462,"[2] Tomioka, Ryota, Taiji Suzuki, and Masashi Sugiyama. ""Super-linear convergence of dual augmented Lagrangian algorithm for sparsity regularized estimation."" JMLR (2011).",pol,none,0.0,0,0
2463,*BCFW vs BCD*,pol,none,0.0,0,0
2464,"Actually, (Lacoste-Julien, S. et al., 2013) proposes Dual-BCFW to optimize structural SVM because the problem contains exponentially many number of dual variables.",pol,none,0.0,0,0
2465,For typical multiclass hinge loss problem the Dual Block-Coordinate Descent that minimizes w.r.t. all dual variables of a sample in a closed-form update converges faster without extra computational cost.,pol,none,0.0,0,0
2466,"See the details in, for example, [3, appendix for the multiclass hinge loss case].",pol,none,0.0,0,0
2467,"[3] Fan, Rong-En, et al. ""LIBLINEAR: A library for large linear classification."" JMLR (2008).",pol,none,0.0,0,0
2468,*Hyper-Parameter*,pol,none,0.0,0,0
2469,"The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD.",pol,pol_negative,1.0,1,1
2470,The authors propose to augment NMT with a grounded inventory of images.,pol,none,0.0,0,0
2471,The intuition is clear and the premise is very tempting.,pol,none,0.0,3,0
2472,The key architectural choice is to allow the transformer to use language embeddings to attend into a topic-image lookup table.,pol,none,0.0,0,0
2473,The proportion is learned to balance how much signal comes from each source.,pol,none,0.0,0,0
2474,"Figure 4, attempts to investigate the importance of this sharing and its effects on performance.",pol,none,0.0,0,0
2475,While reviewing this paper I went back and read the EN-DE evaluation data for the last few years trying to see how often I could reason that images would help and I came up severely lacking.,pol,pol_negative,1.0,1,1
2476,"For example, ""The old system of private arbitration courts is off the table"" from DE-EN 2016 Dev doesn't seem like it should benefit from this architecture.",pol,pol_negative,1.0,1,1
2477,It's then hard for me to square that with the +VR gains seen throughout this work on non-grounded datasets.,pol,pol_negative,1.0,1,1
2478,I trust that the authors did in fact achieve these results but I cannot figure out how or why.,pol,pol_negative,1.0,1,1
2479,This is all further confused by the semantic topics used for clustering the images which ignores stop words and therefore spatial relations or any grammatical nuances.,pol,pol_negative,1.0,1,1
2480,"In contrast, it does make sense that Multi30K would benefit from this architecture.",pol,pol_positive,3.0,0,3
2481,"As a minor note, were different feature extractors compared?",pol,pol_negative,1.0,2,1
2482,The recent flurry of papers on multimodal transformers indicate that deeper resnet stacks correspond to improved downstream performance.,pol,none,0.0,0,0
2483,Is that also true in this domain?,pol,pol_negative,1.0,2,1
2484,"In this paper the authors proposed a new policy gradient method, which is known as the angular policy gradient (APG), that aims to provide provably lower variance in the gradient estimate.",pol,none,0.0,0,0
2485,Here they presented a stochastic policy gradient method for directional control.,pol,none,0.0,0,0
2486,"Under the set of parameterized Gaussian policies, they presented a unified analysis of the variance of APG and showed how it theoretically outperform (in terms of having lower variance) than other state-of-the art methods.",pol,none,0.0,0,0
2487,"They further evaluated the APG algorithms on a grid-world navigation domain as well as the King of Glory task, and showed that the APG estimator significantly out-performs the standard policy gradient.",pol,none,0.0,0,0
2488,In general I think this paper addressed an important issue in policy gradient in terms of deriving a lower variance gradient estimate.,pol,pol_positive,3.0,3,3
2489,"In particular the authors showed that under the parameterized marginal distribution, such as the angular Gaussian distribution, the corresponding APG estimate has a lower variance estimate than that of CAPG.",pol,none,0.0,0,0
2490,"Furthermore, I also appreciate that they evaluated these results in realistic experiments such as the RTS game domains.",pol,pol_positive,3.0,3,3
2491,My only question is on the possibility of deriving realistic APG algorithms beyond the class of angular Gaussian policy.,pol,pol_neutral,2.0,1,2
2492,"In terms of the layout of the paper, I would also recommend including the exact algorithm pseudo-code used in the main paper.",pol,pol_neutral,2.0,2,2
2493,[EDIT]: I have updated my score after the author response and paper revision.,pol,none,0.0,0,0
2494,=============================,pol,none,0.0,0,0
2495,[I was asked to step in as a reviewer last minute. I did not look at the other reviews].,pol,none,0.0,0,0
2496,-------------------------------,pol,none,0.0,0,0
2497,Summary,pol,none,0.0,0,0
2498,-------------------------------,pol,none,0.0,0,0
2499,This paper proposes to learn disentangled latent states under the GAN framework.,pol,none,0.0,0,0
2500,"The core idea is to partition the latent states into N partitions, and correspondly have N Siamese networks that pull the generated images with the same latent partition towards each other, along with a contrastive loss which ensures generated images with different latent partitions to be different.",pol,none,0.0,0,0
2501,"The authors experiment with two setups: in the ""unguided setup"" training is completely unsupervised, while in the ""guided"" setup, there is some weak supervision to encourage different partitions to learn different factors.",pol,none,0.0,0,0
2502,-------------------------------,pol,none,0.0,0,0
2503,Evaluation,pol,none,0.0,0,0
2504,-------------------------------,pol,none,0.0,0,0
2505,"While the motivation is nice, I find the results (especially in the unguided setup) underwhelming.",pol,pol_negative,1.0,1,1
2506,"This does not seem surprising to me, as in the unguided case, the constrative loss seems not strong enough to encourage the latent partitions to be different.",pol,pol_negative,1.0,1,1
2507,Results with weak supervision (their method for injecting weak supervision was very nice),pol,pol_positive,3.0,3,3
2508,are more impressive.,pol,none,0.0,0,0
2509,"However, there is no comparison against existing work.",pol,pol_negative,1.0,1,1
2510,Learning disentangled representations with deep generative models is very much an active area.,pol,pol_negative,1.0,0,1
2511,Here are some recent papers:,pol,none,0.0,0,0
2512,https://openreview.net/references/pdf?id=Sy2fzU9gl,pol,pol_negative,1.0,0,1
2513,https://arxiv.org/abs/1802.05822,pol,pol_negative,1.0,0,1
2514,https://arxiv.org/abs/1802.05983,pol,pol_negative,1.0,0,1
2515,https://arxiv.org/abs/1802.04942,pol,pol_negative,1.0,0,1
2516,"Importantly, there are no quantitative metrics.",pol,pol_negative,1.0,1,1
2517,I do not think this work is ready for publication.,pol,pol_negative,1.0,1,1
2518,This paper proposes two modifications for the MixMatch method [1] and achieves improved accuracy on a range of semi-supervised benchmarks.,pol,none,0.0,0,0
2519,The first modification enforces the distribution of predicted labels to match the distribution of labeled data.,pol,none,0.0,0,0
2520,"The second modification is adding a learned data augmentation strategy, and adapting the method to work with strong data augmentation.",pol,none,0.0,0,0
2521,"The final method is titled ReMixMatch, and improves significantly over MixMatch, especially in low-data regime.",pol,none,0.0,0,0
2522,The main contribution of the paper is really strong empirical results.,pol,pol_positive,3.0,3,3
2523,"The method achieves state of the art results or close to that on multiple benchmarks, with especially large gains in settings with very scarce labeled data, like 40 labels on CIFAR-10.",pol,pol_positive,3.0,0,3
2524,"Another important contribution is the learned data augmentation strategy, which as far as I understand is novel and overcomes some of the limitations of  existing learned data augmentation techniques.",pol,pol_positive,3.0,3,3
2525,"However, the explanation of the strategy wasn’t very clear for me, and the authors didn’t frame it as a major contribution.",pol,pol_negative,1.0,1,1
2526,"The main drawback of the paper is that it seems to be more engineering-focused, and doesn’t provide much insight into semi-supervised learning.",pol,pol_negative,1.0,1,1
2527,"The paper can be summarized as adding two modifications to mix-match, and getting better results.",pol,none,0.0,0,0
2528,The final method becomes fairly involved.,pol,none,0.0,1,0
2529,"Mix-Match is already an elaborate method, and ReMixMatch additionally introduces learned data augmentation, an additional loss term for matching label distributions between labeled and unlabeled data, consistency-loss, and a self-supervised loss (section 3.3).",pol,none,0.0,0,0
2530,"For the reasons above, I think the paper is borderline, but I am currently voting for acceptance based on the strong empirical performance.",pol,none,0.0,0,0
2531,"At the same time, I think the paper can be made stronger and more interesting to read, if the authors added some experiments aimed at understanding the proposed modifications.",pol,pol_neutral,2.0,2,2
2532,One set of experiments that I think would be interesting is aimed at understanding the distribution-matching part.,pol,pol_negative,1.0,2,1
2533,"For example, it would be great if the author could demonstrate that without this loss term the distribution of the predicted classes is wrong in the experiments from Section 4.",pol,pol_neutral,2.0,2,2
2534,"It would also be interesting to see an experiment where the labeled data has a skewed distribution of classes, but we provide the method with information about the true class distribution, and demonstrating that this information helps predictive performance.",pol,pol_negative,1.0,2,1
2535,"For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives.",pol,pol_negative,1.0,2,1
2536,Just analyzing the learned data augmentation in different settings and adding more intuition for what happens would make the paper more insightful and interesting to read.,pol,pol_neutral,2.0,2,2
2537,"On a more minor note, the paper [1] seems to report 4.95 accuracy for MixMatch on CIFAR-10 with 4k labels, while in this paper it’s being reported as 6.24.",pol,pol_negative,1.0,1,1
2538,What is the reason for the difference?,pol,pol_neutral,2.0,2,2
2539,"Another paper, [2], reports very competitive results on CIFAR-10 for 4k labels.",pol,none,0.0,0,0
2540,I would recommend discussing these results briefly in the paper.,pol,pol_neutral,2.0,2,2
2541,"At the same time the empirical performance of ReMixMatch is really impressive, and I don’t think the results in [1] and [2] affect their significance.",pol,none,0.0,3,0
2542,[1] MixMatch: A Holistic Approach to Semi-Supervised Learning,pol,none,0.0,0,0
2543,"David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, Colin Raffel",pol,none,0.0,0,0
2544,[2] There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average,pol,none,0.0,1,0
2545,"Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, Andrew Gordon Wilson",pol,none,0.0,0,0
2546,This paper presents some experiments using random projections instead of embeddings from a 1-of-V encoding.,pol,none,0.0,0,0
2547,"Experiments on the Penn TreeBank benchmark data set show that in a feed-forward language modeling architecture similar to that of (Bengio, 2003), the random projections substantially reduce the number of parameters of the model while not harming perplexity too much.",pol,none,0.0,0,0
2548,The paper would need to be improved substantially in order to appear at a conference like ICLR.,pol,pol_negative,1.0,1,1
2549,"First, the novelty of the approach is limited -- the approach amounts to using a sparse integer layer instead of a floating-point layer within a feed-forward architecture.",pol,pol_negative,1.0,1,1
2550,"Second, and more importantly, the experiments need to be re-done to better measure the practical impact of the techniques.",pol,pol_negative,1.0,1,1
2551,"First, larger data sets such as Wikitext-2 and Wikitext-103, and/or the billion-word benchmark, are needed to understand how well the approach works in practical LM settings.",pol,pol_negative,1.0,1,1
2552,"Second, the paper needs to use more state-of-the-art architectures.",pol,pol_negative,1.0,1,1
2553,"Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.",pol,pol_negative,1.0,1,1
2554,"Finally, the paper needs to compare its parameter-reduction approaches against other compression and hyperparameter optimization techniques.",pol,pol_negative,1.0,1,1
2555,Changing the number/sizes of the network layers or using sparse weight matrices (perhaps with sparsity-inducing regularization) would be natural ways to reduce the parameter space.,pol,pol_neutral,2.0,0,2
2556,"In my opinion, due to how many researchers are and have been looking into improvements of language modeling, the authors may find it hard to break new ground in this direction.",pol,pol_negative,1.0,1,1
2557,Minor,pol,none,0.0,0,0
2558,"In the start of Section 3, it is not clear why having the projection be sparse is desired.",pol,pol_negative,1.0,1,1
2559,"Later, space (and time) efficiency is revealed as the motivation for the sparsity, but it would be helpful if the paper said this earlier.",pol,pol_neutral,2.0,2,2
2560,"Equation 6 seems to have an error, the probability should be P(w_t | w_t-1...) instead of P(w_t , w_t-1...) if this is to represent the standard LM objective (the probability of the corpus).",pol,pol_neutral,2.0,1,2
2561,"Sec 3.3: ""all models sare""",pol,pol_neutral,2.0,1,2
2562,The submission proposes a method for hierarchical RL in multiagent settings.,pol,none,0.0,0,0
2563,In particular it proposes to explicitly decouple training of a high-level and low-level controller with grounded the controller interface as goals in the environment to reach for the low-level controller.,pol,none,0.0,0,0
2564,The model is trained via PPO with GAE and evaluated on a small set of multi agent locomotion tasks.,pol,none,0.0,0,0
2565,"The paper is overall well written and intuitive but limited in evaluation and novelty (see e.g. [1,2] ) with only limited modifications (sharing low-level controller) for the multi agent case.",pol,pol_negative,1.0,1,1
2566,"Furthermore, the experimental section does not compare to other forms of hierarchical approaches for MARL, and generally only provides a single comparison to PPO & MADDPG.",pol,pol_negative,1.0,1,1
2567,"To evaluate the impact of the proposed changes in this paper, one would have to perform extended evaluations and ablations for the submission.",pol,pol_negative,1.0,1,1
2568,A large part of making the MA system work well is based on reward shaping which nearly fills all of page 5.,pol,none,0.0,0,0
2569,This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms.,pol,pol_negative,1.0,1,1
2570,"The experimental section includes various mistakes (see under minor) and misses to describe figures, leading to the assumption that additional time is required for a more detailed evaluation of the algorithm (including more domains and in particular baselines).",pol,pol_negative,1.0,1,1
2571,"Regarding the challenges (and focus on learning simple tasks), reference [3] might be of interest to the authors.",pol,none,0.0,2,0
2572,Minor,pol,none,0.0,0,0
2573,- Direct duplication of text between parts of section 5.3 and 8.3 leading to the duplication of the error of describing the value function learning rate as 0.000.,pol,pol_negative,1.0,1,1
2574,- Self-referential sentences in the supplementary materials (i.e. referral to itself),pol,pol_negative,1.0,1,1
2575,- Missing references on page 3,pol,pol_negative,1.0,1,1
2576,- The egocentric velocity field is not described (section 5),pol,pol_negative,1.0,1,1
2577,- Section 3.1: maximize,pol,none,0.0,1,0
2578,- The wording new paradigm in MARL might be unsuited given existing work on complex domains.,pol,pol_negative,1.0,1,1
2579,‘Our proposed approach represents the first physics-based simulation of its kind that supports MARL.’ This sentence remains unclear as the authors do not propose a simulation engine.,pol,pol_negative,1.0,1,1
2580,- Text on experiment figures is much too small.,pol,pol_negative,1.0,1,1
2581,"[1] Andrew Levy, Robert Platt, and Kate Saenko. Learning Multi-Level Hierarchies with Hindsight. In International Conference on Learning Representations, 2019.",pol,none,0.0,0,0
2582,"[2] Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient Hierarchical Reinforcement Learning. In Advances in Neural Information Processing Systems, pp. 3303–3313, 2018.",pol,none,0.0,0,0
2583,"[3] Ray Interference: a Source of Plateaus in Deep Reinforcement Learning Tom Schaul, Diana Borsa, Joseph Modayil and Razvan Pascanu",pol,none,0.0,0,0
2584,This paper addresses to compress the network weights by quantizing their values to some fixed codeword vectors.,pol,none,0.0,0,0
2585,The authors aim to reduce the distortion of each layer rather than the weight distortion.,pol,none,0.0,0,0
2586,The proposed algorithm first selects the candidate codeword vectors using k-means clustering and fine-tune them via knowledge distillation.,pol,none,0.0,0,0
2587,The authors verify the proposed algorithm by comparing it with existing algorithms for ResNet-18 and ResNet-50.,pol,none,0.0,0,0
2588,"Overall, I think that the proposed algorithm is easy to apply and the draft is relatively well written.",pol,pol_positive,3.0,1,3
2589,Some questions and doubts are listed below.,pol,none,0.0,0,0
2590,"-In k-means clustering (E-step and M-step), is it correct to multiply \tilde x to (c-v)?",pol,pol_negative,1.0,2,1
2591,"I think that the error arising from quantizing v into c is only affected by a subset of rows of \tilde x. For example, if v is the first subvector of w_j, then I think that only 1-st, m+1-th, 2m+1-th, … rows of \tilde x affect to the error.",pol,pol_negative,1.0,0,1
2592,"-Does minimizing reconstruction error minimizes the training loss (before any further fine-tuning) compared to naïve PQ? If not,",pol,pol_neutral,2.0,2,2
2593,-Is there any guideline for choosing the optimal number of centroids and the optimal block size given a target compression rate?,pol,pol_neutral,2.0,2,2
2594,"-Is there any reason not comparing the proposed algorithm with other compression schemes? (e.g., network pruning and low-rank approximation)",pol,pol_negative,1.0,2,1
2595,The paper presents a combination of evolutionary search methods (CEM) and deep reinforcement learning methods (TD3).,pol,none,0.0,0,0
2596,The CEM algorithm is used to learn a Diagional Gaussian distribution over the parametes of the policy.,pol,none,0.0,0,0
2597,The population is sampled from the distribution.,pol,none,0.0,0,0
2598,Half of the population is updated by the TD3 gradient before evaluating the samples.,pol,none,0.0,0,0
2599,"For filling the replay buffer of TD3, all state action samples from all members of the population are used.",pol,none,0.0,0,0
2600,The algorithm is compared against the plane variants of CEM and TD3 as well as against the evoluationary RL (ERL) algorithm.,pol,none,0.0,0,0
2601,Results are promising with a negative result on the swimmer_v2 task.,pol,pol_positive,3.0,3,3
2602,The paper is well written and easy to understand.,pol,pol_positive,3.0,3,3
2603,"While the presented ideas are well motivated and it is certainly a good idea to combine deep RL and evoluationary search, novelty of the approach is limited as the setup is quite similar to the ERL algorithm (which is still on archive and not published, but still...).",pol,pol_positive,3.0,3,3
2604,See below for more comments:,pol,none,0.0,0,0
2605,"- While there seems to be a consistent improvement over TD3, this improvement is in some cases small (e,g. ants).",pol,pol_negative,1.0,1,1
2606,- We are learning a value function for each of the first half of the population.,pol,none,0.0,0,0
2607,"However, the value function from the previous individual is used to initialize the learning of the current value function.",pol,none,0.0,0,0
2608,"Does this cause some issues, e.g., do we need to set the number of steps so high that the initialization does not matter so much any more? Or would it make more sense to reset the value function to some ""mean value function"" after every individual?",pol,pol_neutral,2.0,2,2
2609,- The importance mixing does not seem to provide a better performance and could therefore be shortened in the paper,pol,pol_negative,1.0,1,1
2610,"The paper studies several different techniques for training GANs: the architecture chosen, the loss function of the discriminator and generator,",pol,none,0.0,0,0
2611,"and training techniques: normalization methods, ratio between updates of discriminator and generator, and regularization.",pol,none,0.0,0,0
2612,"The method is performing an empirical training study on three image datasets, modifying the training procedure (e.g. changing one of the parameters) and using different metrics to evaluate the performance of the trained network.",pol,none,0.0,0,0
2613,"Since the space of possible hyper-parameters , training algorithms, loss functions and network architecture is huge , the authors set a default training procedure, and in each numerical experiment freeze all techniques and parameters",pol,none,0.0,0,0
2614,except for one or two which they modify and evaluate.,pol,none,0.0,0,0
2615,"The results of the paper do not give major insights into what are the preferred techniques for training GANs, and certainly not why and under what circumstances they'll work.",pol,pol_negative,1.0,1,1
2616,"The authors recommend using non-saturated GANs loss and spectral normalization when training on new datasets, because these techniques achieved good performance metrics in most experiments.",pol,none,0.0,0,0
2617,"But there is no attempt to generalize the findings (e.g. new datasets not from original study, changing other parameters and then evaluating again if these techniques help etc.), not clear",pol,pol_negative,1.0,1,1
2618,if the,pol,none,0.0,0,0
2619,"improvement in performance is statistically significant, how robust it is to changes in other parameters etc.",pol,none,0.0,3,0
2620,"The authors also rely mostly on the FID metric, but do not show if and how there is improvement upon visual inspection of the generated images (i.e. is resolution improved, is fraction of images that look clearly 'unnatural' reduced etc.)",pol,pol_negative,1.0,1,1
2621,"The writing is understandable for the most part, but the paper seems to lack focus - there is no clear take home message.",pol,pol_negative,1.0,1,1
2622,"The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them,",pol,pol_negative,1.0,1,1
2623,"give mathematical formulations, or insights into their advantages/disadvantages, making it hard to the non-expert reader to understand what are these techniques and why are they introduced.",pol,none,0.0,1,0
2624,"With lack of clear novel insights, or at least more systematic study on additional datasets of the 'winning' techniques and a sensitivity analysis, the paper does not give a valuable enough contribution to the field to merit publication.",pol,pol_negative,1.0,1,1
2625,"I really enjoyed reading the paper! The exposition is clear with interesting observations, and most importantly, the authors walk the extra mile in doing a theoretical analysis of the observed phenomena.",pol,pol_positive,3.0,3,3
2626,Questions for the authors:,pol,none,0.0,0,0
2627,1. (Also AREA CHAIR NOTE): Another parallel submission to ICLR titled “Generative Ensembles for Robust Anomaly Detection” makes similar observations and seemed to suggest that ensembling can help counter the observed CIFAR/SVHN phenomena unlike what we see in Figure 10.,pol,pol_negative,1.0,0,1
2628,Their criteria also accounts for the variance in model log-likelihoods and is hence slightly different.,pol,pol_neutral,2.0,1,2
2629,"2. Even though Figure 2b shows that SVHN test likelihoods are higher than CIFAR test likelihoods, the overlap in the histograms of CIFAR-train and CIFAR-test is much higher than the overlap in CIFAR-train and SVHN-test.",pol,pol_negative,1.0,1,1
2630,"If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?",pol,pol_negative,1.0,2,1
2631,3. Why does the constant image (all zeros) in Figure 9 (appendix) have such a high likelihood? It’s mean (=0 trivially) is clearly different from the means of the CIFAR-10 images (Figure 6a) so the second order analysis of Section 5 doesn’t seem applicable.,pol,pol_neutral,2.0,1,2
2632,4. How much of this phenomena do you think is characteristic for images specifically? Would be interesting to test anomaly detection using deep generative models trained on modalities other than images.,pol,pol_neutral,2.0,2,2
2633,5. One of the anonymous comments on OpenReview is very interesting: samples from a CIFAR model look nothing like SVHN.,pol,pol_negative,1.0,3,1
2634,This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.,pol,pol_negative,1.0,2,1
2635,Minor nitpick: There seems to be some space crunching going on via Latex margin and spacing hacks that the authors should ideally avoid :),pol,pol_negative,1.0,1,1
2636,"This paper introduces a structured drop-in replacement for linear layers in a neural network, referred to as Kaleidoscope matrices.",pol,none,0.0,0,0
2637,"The class of such matrices are proven to be highly expressive and includes a very general class of sparse matrices, including convolution, Fastfood, and permutation matrices.",pol,pol_positive,3.0,0,3
2638,"Experiments are carried in a variety of settings: (i) can nearly replace a series of hand-designed feature extractor, (ii) can perform better than fixed permutation matrices (though parameter count also increased by 10%), (iii) can learn permutations, and (iv) can help reduce parameter count and increase inference speed with a small performance degradation of 1.0 BLEU on machine translation.",pol,pol_positive,3.0,0,3
2639,This appears to be a solid contribution in terms of both theory and practical use.,pol,pol_positive,3.0,3,3
2640,"As I have not thought much about expressiveness in terms of arithmetic circuits (though I was unable to fully follow or appreciate the derivations, the explanations all seem reasonable)",pol,none,0.0,3,0
2641,", my main comments are regarding experiments.",pol,none,0.0,0,0
2642,"Though there are experiments in different domains, each could benefit from some additional ablations, especially to existing parameterizations of structured matrices such as Fastfood, ACDC, and any of the multiple works on permutation matrices and/or orthogonal matrices.",pol,pol_positive,3.0,0,3
2643,"Though Kaleidoscope include these as special cases, it is not clear whether when given the same resources (either memory or computational cost), Kaleidoscope would outperform them.",pol,pol_negative,1.0,1,1
2644,"There is also a matter of ease of training compared to existing approximations or relaxations, e.g. Gumbel-Sinkhorn.",pol,pol_negative,1.0,0,1
2645,Pros:,pol,none,0.0,0,0
2646,"- The writing is easy to follow and concise, with contributions and place in the literature clearly stated.",pol,pol_positive,3.0,3,3
2647,"- The Kaleidoscope matrix seem generally applicable, both proven theoretically and shown empirically (experiments are spread across a wide range of domains).",pol,pol_positive,3.0,3,3
2648,"- The code includes specific C++ and CUDA kernels for computing K matrices, which will be very useful for adaptation.",pol,pol_positive,3.0,1,3
2649,"- The reasoning using arithmetic circuits seems interesting, and the Appendix includes a primer.",pol,pol_positive,3.0,3,3
2650,Cons:,pol,none,0.0,0,0
2651,"- For the squeezenet and latent permutation experiments, would be nice if there is a comparison to other parameterizations of permutation matrices, e.g. gumbel-sinkhorn.",pol,pol_negative,1.0,2,1
2652,"- For the speed processing experiment, did you test what the performance would be if K matrix is replaced by a fully connected layer?",pol,pol_negative,1.0,2,1
2653,"This comparison appears in other experiments, but seems to be missing here for some reason.",pol,none,0.0,1,0
2654,It would lead to better understanding than only comparing to SincNet.,pol,none,0.0,1,0
2655,- The setup for the learning to permute experiment is not as general as it would imply in the main text.,pol,pol_negative,1.0,1,1
2656,"The matrices are constrained so that an actual permutation matrix is always sampled, and the permutation is (had to be?) pretrained to reduce total variation for 100 epochs before jointly trained with the classifier.",pol,none,0.0,1,0
2657,"Though this is stated very clearly in the Appendix, I hope the authors can also communicate this clearly in the main text as it appears to be a crucial component of the experimental setup.",pol,pol_neutral,2.0,0,2
2658,Comments:,pol,none,0.0,0,0
2659,- How easy is it to train with K matrices? Did you have to change optimizer hyperparameter compared to existing baselines?,pol,pol_negative,1.0,2,1
2660,- There seems to be some blurring between the meaning of structure (used to motivate K matrices in the introduction) and sparsity (used to analyze K matrices).,pol,pol_neutral,2.0,1,2
2661,"Structure might also include parameter sharing, orthogonality, and maybe other concepts.",pol,none,0.0,0,0
2662,"For instance, while Kaleidoscope matrices might include the subclass of circulant matrices, can they also capture the same properties or ""inductive bias"" (for lack of better word) as convolutional layers when trained?",pol,pol_neutral,2.0,2,2
2663,"Given a network and input model for generating adversarial examples, this paper presents an idea to quantitatively evaluate the robustness of the network to these adversarial perturbations.",pol,none,0.0,0,0
2664,"Although the idea is interesting, I would like to see more experimental results showing the scalability of the proposed method and for evaluating defense strategies against different types of adversarial attacks.",pol,pol_negative,1.0,2,1
2665,Detailed review below:,pol,none,0.0,0,0
2666,"- How does the performance of the proposed method scale wrt scalability? It will be useful to do an ablation study, i.e. keep the input model fixed and slowly increase the dimension.",pol,pol_neutral,2.0,2,2
2667,- Did you experiment with other MH proposal beyond a random walk proposal? Is it possible to measure the diversity of the samples using techniques such as the effective sample size (ESS) from the SMC literature?,pol,pol_neutral,2.0,2,2
2668,"- What is the performance of the proposed method against ""universal adversarial examples""?",pol,pol_neutral,2.0,2,2
2669,- The most interesting question is whether this method gives reasonable robustness estimates even for large networks such as AlexNet?,pol,pol_neutral,2.0,2,2
2670,"- Please provide some intuition for this line in Figure 3: ""while the robustness to perturbations of size  = 0:3 actually starts to decrease after around 20 epochs.""",pol,pol_neutral,2.0,1,2
2671,- A number of attack and defense strategies have been proposed in the literature.,pol,none,0.0,0,0
2672,"Isn't it possible to use the proposed method to quantify the increase in the robustness towards an attack model using a particular defense strategy? If it is possible to show that the results of the proposed method match the conclusions from these papers, then this will be an important contribution.",pol,pol_neutral,2.0,2,2
2673,Summary:,pol,none,0.0,0,0
2674,This paper uses siamese networks to define a discriminative function for predicting protein-protein interaction interfaces.,pol,none,0.0,0,0
2675,They show improvements in predictive performance over some other recent deep learning methods.,pol,none,0.0,0,0
2676,"The work is more suitable for a bioinformatics audience though, as the bigger contribution is on the particular application, rather than the model / method itself.",pol,none,0.0,0,0
2677,Novelty:,pol,none,0.0,0,0
2678,The main contribution of this paper is the representation of the protein interaction data in the input layer of the CNN,pol,none,0.0,0,0
2679,Clarity:,pol,none,0.0,0,0
2680,"- The paper is well written, with ample background into the problem.",pol,pol_positive,3.0,3,3
2681,Significance:,pol,none,0.0,0,0
2682,- Their method improves over prior deep learning approaches to this problem.,pol,pol_positive,3.0,3,3
2683,"However, the results are a bit misleading in their reporting of the std error.",pol,pol_negative,1.0,1,1
2684,They should try different train/test splits and report the performance.,pol,pol_negative,1.0,1,1
2685,- This is an interesting application paper and would be of interest to computational biologists and potentially some other members of the ICLR community,pol,pol_positive,3.0,3,3
2686,- Protein conformation information is not required by their method,pol,none,0.0,1,0
2687,Comments:,pol,none,0.0,0,0
2688,"- The authors should include citations and motivation for some of their choices (what sequence identity is used, what cut-offs are used etc)",pol,pol_negative,1.0,2,1
2689,-  The authors should compare to at least some popular previous approaches that use a feature engineering based methodology such as - IntPred,pol,pol_negative,1.0,2,1
2690,- The authors use a balanced ratio of positive and negative examples.,pol,none,0.0,0,0
2691,The true distribution of interacting residues is not balanced -- there are several orders of magnitude more non-interacting residues than interacting ones.,pol,pol_negative,1.0,1,1
2692,Can they show performance at various ratios of positive:negative examples?,pol,pol_negative,1.0,2,1
2693,"In case there is a consistent improvement over prior methods, then this would be a clear winner",pol,pol_neutral,2.0,0,2
2694,This paper studies how the FiLM visual question answering (VQA) model answer questions involving the quantifier ‘most’.,pol,none,0.0,0,0
2695,"This quantifier is chosen for study because it cannot be expressed in first order logic (i.e., high-order logic is required), and secondly because there are two different algorithmic approaches to answering questions involving ‘most’ (cardinality-based strategy and pairing-based strategy).",pol,none,0.0,0,0
2696,"Experiments are performed by designing abstract visual scenes with controlled numerosity and spatial layouts, and applying methodologies from pyscholinguistics.",pol,none,0.0,0,0
2697,"The paper concludes that the model learns an approximate number system (ANS), consistent with the cardinality-based strategy, with implications for understanding the conditions under which existing VQA models should perform well or badly (and possibly for improving VQA models).",pol,none,0.0,0,0
2698,Strengths:,pol,none,0.0,0,0
2699,"- The research question is clear and well-conceived. In general, it seems there are significant opportunities for better collaboration between the experimental psychology and machine learning communities, and this is a good example of the benefits.",pol,pol_positive,3.0,3,3
2700,"- The paper is clear, highly-focused, and well-written.",pol,pol_positive,3.0,3,3
2701,Weaknesses:,pol,none,0.0,0,0
2702,- The arguments for why the experimental evidence actually supports the existance of an approximate number system (ANS) could be made more clear.,pol,pol_negative,1.0,2,1
2703,"For example, the section on “Ratios andWeber fraction” argues that “these curves align well with the trend predicted by Weber’s law”, but does not explain how the experimental data would present if the alternative hypothesis (pairing-based strategy) was being used.",pol,pol_negative,1.0,1,1
2704,What would the pairing-based strategy look like in Figure 6 right? Are there not significance tests that could be used to more carefully quantify the level of support for the two alternative strategies?,pol,pol_negative,1.0,2,1
2705,"- The experiments seem very similar to Wu et al. 2018, which is considered to be prior work under the ICLR guidelines.",pol,pol_negative,1.0,3,1
2706,"While this paper is acknowledged in the related work, it would be helpful to expand further on the relationship between these works, so the originality and contribution of this paper can be better evaluated.",pol,pol_negative,1.0,2,1
2707,"- In some ways it is not that surprising that the CNN more easily learns an approximate number system rather than a pairing-based algorithm, as the later would presumably need to learn a different convolutional filter for every possible spatial arrangement of the pairs (which would be very sample inefficient).",pol,none,0.0,1,0
2708,"Therefore, it might be interesting to consider, are there any circumstances under which the CNN would learn a pairing based algorithm?",pol,pol_neutral,2.0,2,2
2709,"For example, what if the spatial configuration of the pairs was simplified, so they were always side-by-side at a fixed distance? If pairing-based algorithms emerged under simplified scenarios, this might have implications for the design of CNN filters (if we want models that are capable of learning these types of functions).",pol,pol_neutral,2.0,1,2
2710,Summary:,pol,none,0.0,0,0
2711,"I regard this as a good paper, with a couple of weakness that could be addressed as indicated.",pol,pol_positive,3.0,3,3
2712,This paper studies the problem of coordinating many strategic agents with private valuation to perform a series of common goals.,pol,none,0.0,0,0
2713,The algorithm designer is a manager who can assign goals to various agents but cannot see their valuation or control them explicitly.,pol,none,0.0,0,0
2714,The manager has a utility function for various goals and wants to maximize the total revenue.,pol,none,0.0,0,0
2715,The abstract problem is well-motivated and significant and is an entire branch of study called algorithmic mechanism design.,pol,none,0.0,3,0
2716,However often many assumptions have to be made to make the problem mathematically tractable.,pol,none,0.0,0,0
2717,"In this paper, the authors take an empirical approach by designing an RL framework that efficiently maximizes rewards across many episodes.",pol,none,0.0,0,0
2718,"Overall I find the problem interesting, well-motivated.",pol,pol_positive,3.0,3,3
2719,The paper is well-written and contains significant experiments to support its point.,pol,pol_positive,3.0,3,3
2720,"However, I do not have the necessary background in the related literature to assess the significance of the methods proposed compared to prior work and thus would refrain from making a judgment on the novelty of this paper in terms of methodology.",pol,none,0.0,1,0
2721,Here are some of my comments/questions to the author on this paper.,pol,none,0.0,0,0
2722,(1) I want to clarify how the skills of the agents play a role in the problem setup. Does it show up in the expression for the manager's reward?,pol,pol_neutral,2.0,2,2
2723,"In particular, does it affect the Indicator for whether a goal is completed Eq. (2) via a process that need not be explicitly modeled but can be observed via a feedback of whether or not the goal is completed?",pol,pol_neutral,2.0,2,2
2724,"So in the case of resource collection example, the skill set is a binary value for each resource, whether it can be collected or not?",pol,pol_neutral,2.0,2,2
2725,"(2) Related to the first point, the motivation for modeling the agents as maximizing their utility is the assumption that agents do not know their skills. I am wondering, is this really justified? Over the course of episodes, can the agents learn their skills based on the relationship between their intention and the goals they achieve? In the resource collection example, when they reach a resource and are not able to collect it, they understand that they do not have the corresponding skill.",pol,pol_neutral,2.0,2,2
2726,Is there a way to extrapolate the results from this paper to such a setting?,pol,pol_neutral,2.0,2,2
2727,(3) I am slightly concerned about the sample complexity of keeping track of the probability of worker i finishing goal g within t steps with a bonus b. This scales linearly in parameters which usually would be large (such as the number of time-steps).,pol,pol_negative,1.0,1,1
2728,"Are there alternate ways to overcome maintaining the UCB explicitly, especially for the number of time-steps?",pol,pol_neutral,2.0,2,2
2729,Some minor comments on the presentation.,pol,none,0.0,0,0
2730,(1) What are the units for rewards in the plots? Is it the average per episode reward? It would be good to mention this in the caption.,pol,pol_neutral,2.0,2,2
2731,(2) There are a few typos in the paper.,pol,none,0.0,1,0
2732,"Some I could catch was,",pol,none,0.0,0,0
2733,"- Last line in Page 5: ""quantitative"" -> ""quantity""",pol,pol_neutral,2.0,2,2
2734,- Page 8: skills nad preferences -> skills and preferences,pol,pol_neutral,2.0,2,2
2735,- Page 8: For which we combining -> for which we combine,pol,pol_neutral,2.0,2,2
2736,Summary,pol,none,0.0,0,0
2737,The authors make three major contributions that improve MixMatch and achieve state-of-the-art in a semi-supervised image classification task.,pol,pol_positive,3.0,0,3
2738,The major contributions include: (1) distribution alignment to calibrate the predicted distribution of unlabeled data; (2) augmentation anchoring to allow more aggressive data augmentation; and (3) CTAugment to train the augmentation policy alongside the semi-supervised model.,pol,none,0.0,0,0
2739,"The authors conduct experiments on SVHN, CIFAR-10 and STL, and show significant improvements over the MixMatch baseline.",pol,none,0.0,0,0
2740,"They also show good results (15.08% error rate) of training with 40 labeled data, in spite of very high variation.",pol,pol_positive,3.0,3,3
2741,"In the ablation study, they show the error rate drops as K (number of augmentation) increases.",pol,none,0.0,0,0
2742,They also conduct ablation studies on the design choices of their method.,pol,none,0.0,0,0
2743,Decision,pol,none,0.0,0,0
2744,"The decision for this paper is borderline, tending towards a weak accept.",pol,none,0.0,1,0
2745,"Overall, the paper proposes some simple but interesting ideas, e.g. distribution environments.",pol,pol_positive,3.0,3,3
2746,"However, although the proposed method achieves good performance over various (smaller) benchmarks, the method seems ad-hoc and complicated.",pol,pol_negative,1.0,1,1
2747,"As pointed out in the weakness section, many design choices are not well motivated, and the effects of those designs are not well studied.",pol,pol_negative,1.0,1,1
2748,The tendency to accept is due to the overall strong results.,pol,pol_positive,3.0,0,3
2749,Strength,pol,none,0.0,0,0
2750,1. Significant improvement over MixMatch baseline.,pol,pol_positive,3.0,3,3
2751,2. The proposed augmentation anchoring and distribution alignment can be easily integrated into existing work.,pol,pol_positive,3.0,3,3
2752,3. The proposed CTAugment method lifts the burden of training an RL data augmentation policy.,pol,pol_positive,3.0,3,3
2753,Weakness,pol,none,0.0,0,0
2754,1. The objective of the update equation of CTAugment’s learned weights seems contradicted with the purpose of how data augmentation is used in the consistency-based SSL method.,pol,pol_negative,1.0,1,1
2755,"In other words, the objective of the update equation encourages higher weights for the distortion parameter that leads to lower variation in the predicted distribution.",pol,none,0.0,0,0
2756,"However, the idea of aggressive data augmentation is to generate data that has high variation in the model prediction, and then penalize the variation in the form of consistency loss.",pol,none,0.0,0,0
2757,The variation induced by aggressive augmentation is the root of the consistency loss that helps regularize the model.,pol,none,0.0,0,0
2758,2. The authors should provide ablation study and analysis of their CTAugment.,pol,pol_negative,1.0,2,1
2759,"For example, they should compare with simple random augmentation policy.",pol,pol_neutral,2.0,1,2
2760,It is also recommended to show the learned weights of the distortion parameter.,pol,pol_neutral,2.0,0,2
2761,Also does larger K value when applied for vanilla MixMatch approach the results in ReMixMatch?,pol,pol_neutral,2.0,2,2
2762,3. The authors should provide more detail of the setting in the ablation study.,pol,pol_negative,1.0,2,1
2763,"For example, the setting of “No strong aug.” and “No weak aug.” are not clear.",pol,pol_negative,1.0,1,1
2764,"4. The authors hypothesize that “stronger augmentation can result in disparate predictions, so their average may not be a meaningful target.” However, they do not show any analysis to support this hypothesis.",pol,pol_negative,1.0,1,1
2765,5. It is recommended to evaluate the method on larger datasets such as CIFAR-100.,pol,pol_neutral,2.0,1,2
2766,"It is not clear how well these methods scale, and for example using k=8 adds computation which may hinder training scalability.",pol,pol_negative,1.0,1,1
2767,Minor Comments,pol,none,0.0,0,0
2768,"1. For Table 2 and Table 3, it should be “error rate” rather than “accuracy”.",pol,pol_neutral,2.0,1,2
2769,2. How is the loss weight λr tuned in the 40 labeled setting? How are the hyper-parameters tuned in general?,pol,pol_neutral,2.0,2,2
2770,This paper provides some insights on influence of data distribution on robustness of adversarial training.,pol,none,0.0,0,0
2771,The paper demonstrates through a number of analysis that the distance between the training an test data sets plays an important role on the effectiveness of adversarial training.,pol,none,0.0,0,0
2772,"To show the latter, the paper proposes an approach to measure the distance between the two data sets using combination of nonlinear projection (e.g. t-SNE), KDE, and K-L divergence.",pol,none,0.0,0,0
2773,"The paper also shows that under simple transformation to the test dataset (e.g. scaling), performance of adversarial training reduces significantly due to the large gap between training and test data set.",pol,none,0.0,0,0
2774,This tends to impact high dimensional data sets more than low dimensional data sets since it is much harder to cover the whole ground truth data distribution in the training dataset.,pol,none,0.0,0,0
2775,Pros:,pol,none,0.0,0,0
2776,- Provides insights on why adversarial training is less effective on some datasets.,pol,pol_positive,3.0,0,3
2777,- Proposes a metric that seems to strongly correlate with the effectiveness of adversarial training.,pol,pol_positive,3.0,0,3
2778,Cons:,pol,none,0.0,0,0
2779,- Lack of theoretical analysis. It could have been nice if the authors could show the observed phenomenon analytically on some simple distribution.,pol,pol_negative,1.0,1,1
2780,"- The marketing phrase ""the blind-spot attach"" falls short in delivering what one may expect from the paper after reading it.",pol,pol_negative,1.0,1,1
2781,The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot.,pol,pol_negative,1.0,2,1
2782,"For some dataset, this is beyond a spot, it could actually be huge portion of the input space!",pol,none,0.0,1,0
2783,Minor comments:,pol,none,0.0,0,0
2784,- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models.,pol,pol_negative,1.0,1,1
2785,"Though the paper is not suggesting that, it would help to clarify it in the paper.",pol,pol_negative,1.0,2,1
2786,"Furthermore, it would help if the paper elaborates why the distance between the test and training dataset is smaller in an adversarially trained network compared to a naturally trained network.",pol,pol_neutral,2.0,2,2
2787,- Are the results in Table 1 for an adversarially trained network or a naturally trained network?,pol,pol_neutral,2.0,2,2
2788,"Either way, it could be also interesting to see the average K-L divergence between an adversarially and a naturally trained network on the same dataset.",pol,pol_neutral,2.0,2,2
2789,- Please provide more visualization similarly to those shown in Fig 4.,pol,pol_neutral,2.0,1,2
2790,This paper used the concept based on channel deficiency to derive a variational bound similar to variational information bottleneck.,pol,none,0.0,0,0
2791,Theoretical analysis shows that this bound is an lower bound on the VIB objective.,pol,none,0.0,0,0
2792,The empirical analysis shows it outperforms VIB in some sense.,pol,none,0.0,0,0
2793,I think this paper's contribution is rather theoretical than practical.,pol,pol_negative,1.0,1,1
2794,The experiments section can be improved in the following aspect:,pol,none,0.0,1,0
2795,-  Figure 2 are hard to read for different M's. It would be better if the authors can show the exact accuracy numbers rather than the overlapped lines,pol,pol_negative,1.0,1,1
2796,- I(Z;Y) vs I(Z;X) graph is typically used in a VIB setting.,pol,none,0.0,0,0
2797,"In the paper's variational deficiency setting, although plotting I(Z;Y) vs I(Z;X) is necessary, it would be also helpful for the authors' to plot Deficiency vs I(Z;X), because this is what new objective is trading-off.",pol,pol_positive,3.0,2,3
2798,"- Again, Figure 3, it is hard to see the benefits for increasing M from the visualizations for different clusterings.",pol,pol_negative,1.0,1,1
2799,- How do the paper estimate I(Z;Y) and I(Z;X) for plotting these figures? Does the paper use lower bound or some estimators? It should be made clear in the paper since these are non-trivial estimations.,pol,pol_negative,1.0,2,1
2800,"Last comment is that, although the concept of `deficiency` in a bottleneck setting is novel, the similar idea for tighter bound of log likelihood has already been pursed in the following paper:",pol,pol_negative,1.0,0,1
2801,"- Yuri Burda, Roger Grosse, Ruslan Salakhutdinov. Importance Weighted Autoencoders. ICLR 2016",pol,pol_negative,1.0,0,1
2802,It was kind of surprising that the authors did not cite this paper given the results are pretty much the same.,pol,pol_negative,1.0,3,1
2803,It would also be helpful for the authors to do a comparison or connection section with this paper.,pol,pol_neutral,2.0,2,2
2804,"I like the paper in general, but given it still has some space for improvement, I would keep my decision as boarder line for now.",pol,none,0.0,0,0
2805,"This paper addresses the problem of adversarial transferability, i.e. the ability that an adversarial example generated by one model can successfully fool another model.",pol,none,0.0,0,0
2806,"There are numerous papers on this topic recently, such as Fawzi'15, Liu'17, Dong'18, Athalye'18...",pol,none,0.0,0,0
2807,The authors propose tot study two types of factors that might influence transferability: model-specific parameters and smoothness of loss surface for constructing adversarial examples.,pol,none,0.0,0,0
2808,Two experimental studies are made for each influence factor from existing architectures.,pol,none,0.0,0,0
2809,"Another attack strategy aiming at smoothing the loss surface is proposed, an experimental evaluation shows the effectiveness of the proposed method.",pol,none,0.0,0,0
2810,Pros,pol,none,0.0,0,0
2811,-the proposed experimental studies can be interesting to the community,pol,pol_positive,3.0,3,3
2812,-many interesting illustrations are provided.,pol,pol_positive,3.0,3,3
2813,Cons,pol,none,0.0,0,0
2814,"-The conclusions of the study were suggested by previous papers or are rather expected: adversarial transfer is not symmetric: Deep models less transferable than shallow ones, averaging gradient is better",pol,pol_negative,1.0,1,1
2815,-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.,pol,pol_negative,1.0,1,1
2816,"-Only two influence factors are studied, again the paper would be more interesting with a more general study",pol,pol_negative,1.0,2,1
2817,The paper has an interesting potential but seems a bit limited in its present form.,pol,pol_negative,1.0,1,1
2818,This paper displays an occurrence of density models assigning higher likelihood to out-of-distribution inputs compared to the training distribution.,pol,none,0.0,0,0
2819,"Specifically, density models trained on CIFAR10 have higher likelihood on SVHN than CIFAR10.",pol,none,0.0,0,0
2820,This is an interesting observation because the prevailing assumption is that density models can distinguish inliers from outliers.,pol,pol_positive,3.0,0,3
2821,"However, this phenomenon is not encountered when comparing MNIST and NotMNIST.",pol,none,0.0,0,0
2822,The SVHN/CIFAR10 phenomenon has also been shown in concurrent work [1].,pol,none,0.0,0,0
2823,"Given that you observed that SVHN has higher likelihood on all three model types (PixelCNN, VAE, Glow), why investigate a component specific to just flow-based models (the volume term)?",pol,pol_negative,1.0,1,1
2824,It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.,pol,pol_negative,1.0,0,1
2825,"For instance, the experiments seem to indicate that generalizing density estimation from CIFAR training set to CIFAR test set is likely challenging and thus the models underfit the true data distribution, resulting in the simpler dataset (SVHN) having higher likelihood.",pol,none,0.0,0,0
2826,"Given the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.",pol,pol_negative,1.0,2,1
2827,"For instance, a scenario where the data statistics (pixel means and variances) are nearly equivalent for both datasets would be interesting.",pol,pol_neutral,2.0,2,2
2828,The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.,pol,pol_negative,1.0,1,1
2829,This paper is well written.,pol,pol_positive,3.0,3,3
2830,I think the presentation of this density modelling shortcoming is a good contribution but leaves a bit to be desired.,pol,pol_neutral,2.0,1,2
2831,"[1] Choi, H. and Jang, E. Generative Ensembles for Robust Anomaly Detection.",pol,none,0.0,0,0
2832,https://arxiv.org/abs/1810.01392,pol,none,0.0,0,0
2833,Pros:,pol,none,0.0,0,0
2834,- Interesting observation of density modelling shortcoming,pol,pol_positive,3.0,3,3
2835,- Clear presentation,pol,pol_positive,3.0,3,3
2836,Cons:,pol,none,0.0,0,0
2837,- Lack of a strong explanation for the results or a solution to the problem,pol,pol_negative,1.0,1,1
2838,- Lack of an extensive exploration of datasets,pol,pol_negative,1.0,1,1
2839,"The paper analyzes the strategy that a visual question answering model (FiLM) uses to verify statements containing the quantifier ""most"" (""most of the dots are red"").",pol,none,0.0,0,0
2840,"It finds that the model is sensitive to the ratio of objects that satisfy the predicate (that are red) to objects that do not; as the ratio decreases (e.g. 10 red dots compared to 9 blue dots), the model's performance decreases too.",pol,none,0.0,0,0
2841,This is consistent with human behavior.,pol,none,0.0,0,0
2842,Strengths:,pol,none,0.0,0,0
2843,* The introduction lays out an ambitious program of comparing humans to deep neural networks.,pol,pol_positive,3.0,3,3
2844,* The experimental results are interesting (although of modest scope) and support the hypothesis that the network is not counting the objects but rather is using an approximation that is sensitive to the ratio between the red and non-red items.,pol,pol_neutral,2.0,3,2
2845,Weaknesses:,pol,none,0.0,0,0
2846,"* The architecture of the particular model is described very briefly, and at multiple points there’s an implication that this is an investigation of “deep learning models” more generally, even though those models may vary widely.",pol,pol_negative,1.0,0,1
2847,"While the authors are using an existing model, they shouldn't assume that the reader has read the paper describing that model.",pol,pol_negative,1.0,1,1
2848,"I would like to see more discussion of whether it is at all plausible for this model to acquire the pairing strategy, compared to alternative VQA models (e.g., using relation networks).",pol,pol_neutral,2.0,2,2
2849,* I found it difficult to follow the theoretical motivation for performing the work.,pol,pol_negative,1.0,1,1
2850,"The goal seems to be to test whether the network is performing the task in way that ""if not human-like, at least is cognitively plausible"".",pol,pol_neutral,2.0,0,2
2851,I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue.,pol,pol_negative,1.0,1,1
2852,"Later in the same paragraph, the authors argue that ""in the case of a human-centered domain like natural language, ultimately, some degree of comparability to human performance is indispensable"".",pol,none,0.0,0,0
2853,"This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there).",pol,pol_negative,1.0,1,1
2854,"In general, I don't understand why we would want a visual question answering system that returns approximate answers --",pol,pol_negative,1.0,1,1
2855,isn't it better to have it count exactly how many red dots there are compared to non-red dots?,pol,pol_neutral,2.0,2,2
2856,"* The authors assume that explicit counting is not ""likely to be learned by the 'one-glance' feed-forward-style neural network"" evaluated in the paper. What is this statement based on? Why would a ""one-glance"" network have trouble counting objects?",pol,pol_neutral,2.0,1,2
2857,(What is a “one-glance network”?),pol,pol_neutral,2.0,1,2
2858,"* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can ""learn and utilize higher-level concepts than mere pattern matching"".",pol,pol_negative,1.0,1,1
2859,"What is ""pattern matching"" and how does it differ from ""higher-level concepts""?",pol,pol_neutral,2.0,1,2
2860,* Why would the pairing strategy in a neural network be affected by the clustering of the objects?,pol,pol_neutral,2.0,2,2
2861,"I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.",pol,pol_negative,1.0,1,1
2862,Minor comments:,pol,none,0.0,0,0
2863,"* Is the definition of ""most"" really a central piece of evidence for ""the apparent importance of a cardinality concept to human cognition""? Our ability to count seems sufficient to me. Perhaps I'm not understanding what the authors have in mind here.",pol,pol_neutral,2.0,1,2
2864,"* Please use the terms ""interpretation"" and ""verification"" consistently.",pol,pol_neutral,2.0,1,2
2865,"* ""One over the other strategy"" -> ""one strategy over the other"".",pol,pol_neutral,2.0,2,2
2866,"* The paper is almost 9 pages long, but the contribution does not appear more substantial than a standard 8-page submission.",pol,pol_negative,1.0,1,1
2867,"This paper presents a method for interactive agent modeling that involves learning to model a demonstrator agent not only through passively viewing the demonstrator agent, but also through interactions from a learner agent that learns to probe the environment of the demonstrator agent so as to maximally change the behavior of the demonstrator agent.",pol,none,0.0,0,0
2868,The approximated demonstrator agent is trained through standard imitation learning techniques and the learning or probing agent is trained using reinforcement learning.,pol,none,0.0,0,0
2869,The mind of the demonstrating agent is modeled as a latent space representation from a neural net.,pol,none,0.0,0,0
2870,This latent space representation is used as the reinforcement learning signal for the learner (probing) agent similar to the curiosity driven techniques where larger changes in the representation of mind are sought out since they should lead to larger differences in demonstrator agent behavior.,pol,none,0.0,0,0
2871,The authors test this in several gridworld environments as well as a sorting task and show that their method achieves superior performance and generalizes better to unseen states and task variations compared to several baseline methods.,pol,none,0.0,0,0
2872,"General comments, in no particular order:",pol,none,0.0,0,0
2873,1. The authors should provide more details on how the hand-crafted demonstrator agents were made.,pol,pol_negative,1.0,2,1
2874,"I assume something similar to an a* algorithm was probably used for the passing task, but what about the maze navigation task?",pol,pol_negative,1.0,2,1
2875,2. The demonstrated tasks are (gridworld and algorithmic) which are very simple RL taks with low-dimensional (non-visual) state-spaces,pol,none,0.0,1,0
2876,.,pol,none,0.0,0,0
2877,"It's unclear how this would scale to more complex tasks with higher-dimensional state spaces such as Atari, Starcraft II or if this would work with tasks with continuous state and action spaces such as mujoco.",pol,pol_negative,1.0,1,1
2878,3. The core premise behind training the learner agent with RL is using a curiosity driven approach to train a probing policy to incite new demonstrator behaviors by maximizing the differences between the latent vectors of the behavior trackers at different time steps.,pol,none,0.0,0,0
2879,"Because the latent vector is modeled as a non-linear function, distances between latent vector representations do not necessarily correspond to similar distances between behavior policies (for example, KL distances between two policy distributions).",pol,none,0.0,1,0
2880,"Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.",pol,pol_negative,1.0,1,1
2881,4. The biggest flaw that I see in this method is the practicality of it's use.,pol,pol_negative,1.0,1,1
2882,This method relies on the ability to obtain or gain access to a demonstration agent to learn from.,pol,none,0.0,0,0
2883,"In very simple tasks, such as the one presented here, the authors were able to hard-code their own demonstration agent.",pol,pol_negative,1.0,0,1
2884,"However, in harder tasks, this will not be feasible.",pol,pol_negative,1.0,1,1
2885,"If you are able to obtain or code your own agent, then you've already solved the task and there is no need to do any sort of imitation learning in the first place.",pol,pol_negative,1.0,0,1
2886,"In reality, for sufficiently difficult tasks, a human would be the demonstration agent (as is done in most robotics tasks).",pol,pol_negative,1.0,0,1
2887,"In practice, imitation learning from a human works well since the learning can be done offline (i.e., post-hoc after a set of demonstrations are collected from the human).",pol,none,0.0,0,0
2888,"However, this task requires the learning to be interactive and thus the demonstrator needs to be present during the learning.",pol,pol_negative,1.0,0,1
2889,Interactively learning from a human becomes a problem if the learning takes tens of thousands of episodes of training since a human cannot reasonably be expected to be present for that amount of time.,pol,none,0.0,0,0
2890,"Thus, the question is 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?",pol,pol_negative,1.0,1,1
2891,5. My previous comment relates mainly to the application of improved imitation learning.,pol,none,0.0,0,0
2892,"However, I do think this is still very useful in the context of multi-agent reinforcement learning for collaborative and competitive tasks (sections 4.6 and 4.7).",pol,pol_positive,3.0,0,3
2893,I think this method demonstrates a method for improved collaborative and/or competitive performances given the fact that you already have a single agent with a learned policy.,pol,none,0.0,0,0
2894,"Overall, I think the paper presents a really nice idea of how to improve modeling of agents.",pol,pol_positive,3.0,3,3
2895,"essentially, a learner agent learns how to probe a demonstrator agent to provide more information about what's being demonstrated and prevent over-fitting to a set of fixed demonstrations.",pol,none,0.0,0,0
2896,"This work sounds novel to me from a reinforcement learning perspective, however, I'm not well versed on theory of mind research.",pol,pol_positive,3.0,1,3
2897,This paper investigates the impact of stale weights on the statistical efficiency and performance in a pipelined backpropagation scheme that maximizes accelerator utilization while keeping the memory overhead modest.,pol,none,0.0,0,0
2898,The paper proposes to combine pipelined and non-pipelined training in a hybrid scheme to address the issue of significant drop in accuracy when pipelining is deeper in the network.,pol,none,0.0,0,0
2899,The performance of the proposed pipelined backpropagation is demonstrated on 2 GPUs using ResNet with speedups of up to 1.8X over a 1-GPU baseline and a small drop in inference accuracy.,pol,none,0.0,0,0
2900,The paper is well written and easy to follow.,pol,pol_positive,3.0,3,3
2901,The proposed idea is interesting and its effectiveness is well demonstrated with a promising speed and a small drop in accuracy.,pol,pol_positive,3.0,3,3
2902,The proposed approach is compared to two existing works:  PipeDream [1] and GPipe [2].,pol,none,0.0,0,0
2903,"Though promising results have been demonstrated, a drawback of the proposed method is that it introduces more memory overhead compared to GPipe.",pol,pol_negative,1.0,1,1
2904,"Although a detailed discussion is provided related to the memory consumption between the proposed method and PipeDream, no detailed discussion is provided with respect to GPipe.",pol,pol_negative,1.0,1,1
2905,"Further, no proper convergence analysis of the proposed approach is provided and is desired due to the likely divergence in the optimization.",pol,pol_negative,1.0,1,1
2906,Minor comment: An interesting line of work is that of [3] which could be included in the discussion.,pol,pol_negative,1.0,2,1
2907,"Overall, the proposed approach is interesting and is shown to achieve promising results.",pol,pol_positive,3.0,3,3
2908,"However, memory overhead is still an issue compared to existing method.",pol,pol_negative,1.0,1,1
2909,"[1] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg Ganger, and Phil Gibbons. Pipedream: Fast and efficient pipeline parallel DNN training, 2018. URL http://arXiv:1806.03377.",pol,none,0.0,0,0
2910,"[2] Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, and Zhifeng Chen. Gpipe: Efficient training of giant neural networks using pipeline parallelism, 2018. URL http://arXiv:1811.06965.",pol,none,0.0,0,0
2911,"[3] Guanhua Wang, Shivaram Venkataraman, Amar Phanishayee, Jorgen Thelin, Nikhil Devanur, Ion Stoica: Blink: Fast and Generic Collectives for Distributed ML. arXiv:1910.04940, 2019.",pol,none,0.0,0,0
2912,This paper proposes to enable GAN based TTS in the time domain with the careful designs of the (non-autoregressive) generator and discriminator.,pol,none,0.0,0,0
2913,There have been various trials of GAN-TTS but not so many success and I'm glad to hear that the proposed method seems to enable GAN-TTS with fast inference thanks to the non-autoregressive property.,pol,pol_positive,3.0,3,3
2914,"The method also proposes new objective measures inspired by the image recognition network based on the high-level features generated by end-to-end ASR, which is also another important contribution of this paper.",pol,pol_positive,3.0,0,3
2915,My concern for this paper is reproducibility.,pol,pol_negative,1.0,1,1
2916,"Although I really appreciate the authors' efforts on providing implementational details in the appendix, the code and data do not seem to be publicly available, and I'm expecting that the implementation of this technique is relatively hard due to their complex designs of the generator and discriminator.",pol,pol_negative,1.0,1,1
2917,"Apart from that, the paper is well written overall by well describing the trend of GAN studies in the image processing and the application of such image processing oriented GAN techniques to TTS.",pol,pol_positive,3.0,3,3
2918,"The authors propose learnable ""kaleidoscope matrices"" (K-matrices) in place of manually engineered structured and sparse matrices.",pol,none,0.0,0,0
2919,"By capturing ""all"" structured matrices in a way that can be learned, and without imposing a specific structure or sparsity pattern, these K-matrices can improve on existing systems by",pol,pol_positive,3.0,0,3
2920,"* capturing more structure (that was not handled by the existing manually engineered architecture),",pol,none,0.0,0,0
2921,* running faster than dense implementations.,pol,none,0.0,0,0
2922,"The claim that ""all"" structured matrices can be represented efficiently is a strong one, and in section 2.3 the authors make it clear what they mean by this.",pol,pol_positive,3.0,0,3
2923,"Although the proof is long and beyond the expertise of this reviewer, the basic explanation given in section 2.3 makes their point clear for the non-expert reader.",pol,pol_positive,3.0,0,3
2924,The balance of the paper empirically tests the claims of learnable structure and efficiency.,pol,pol_positive,3.0,0,3
2925,"On the basis that these experiments essentially bear out the claims of the paper, I selected to accept the paper.",pol,none,0.0,0,0
2926,Weaknesses:,pol,none,0.0,0,0
2927,1. Regarding the ISWLT translation task result:,pol,none,0.0,0,0
2928,"With this dataset, it's a bit of a stretch to say there was ""only a 1 point drop in BLEU score"". That's a significant drop, and in fact the DynamicConv paper goes to significant lengths to make a smaller 0.8 point improvement.",pol,pol_negative,1.0,1,1
2929,"There are probably many other ways to trade BLEU score for efficiency, and without showing those other methods (and the point drops they have), it's not clear that K-matrices are a good way to speed up decoding a bit.",pol,pol_negative,1.0,1,1
2930,The paper proposes a framework for learning interpretable latent representations for GANs.,pol,none,0.0,0,0
2931,The key idea is to use siamese networks with contrastive loss.,pol,none,0.0,0,0
2932,"Specifically, it decomposes the latent code to a set of knobs (sub part of the latent code).",pol,none,0.0,0,0
2933,Each time it renders different images with different configurations of the knobs.,pol,none,0.0,0,0
2934,"For example, 1) as changing one knob while keeping the others, it expects it would only result in change of one attribute in the image, and 2) as keeping one knob while changing all the others, it expects it would result in large change of image appearances.",pol,none,0.0,0,0
2935,The relative magnitude of change for 1) and 2) justifies the use of a Siamese network in addition to the image discriminator in the standard GAN framework.,pol,none,0.0,0,0
2936,The paper further talks about how to use inductive bias to design the Siamese network so that it can control the semantic meaning of a particular knob.,pol,none,0.0,0,0
2937,"While I do like the idea, I think the paper is still in the early stage.",pol,pol_positive,3.0,1,3
2938,"First of all, the paper does not include any numerical evaluation.",pol,pol_negative,1.0,1,1
2939,It only shows a couple of examples.,pol,pol_negative,1.0,1,1
2940,It is unclear how well the proposed method works in general.,pol,pol_negative,1.0,1,1
2941,"In addition, the InfoGAN work is designed  for the same functionality.",pol,none,0.0,0,0
2942,The paper should compare the proposed work to the InfoGAN work both quantitatively and qualitatively to justify its novelty.,pol,pol_negative,1.0,1,1
2943,Summary,pol,none,0.0,0,0
2944,-------,pol,none,0.0,0,0
2945,This paper describes a model for musical timbre transfer which builds on recent developments in domain- and style transfer.,pol,none,0.0,0,0
2946,"The proposed method is designed to be many-to-many, and uses a single pair of encoders and decoders with additional conditioning inputs to select the source and target domains (timbres).",pol,none,0.0,0,0
2947,"The method is evaluated on a collection of individual note-level recordings from 12 instruments, grouped into four families which are used as domains.",pol,none,0.0,0,0
2948,"The method is compared against the UNIT model under a variety of training conditions, and evaluated for within-domain reconstruction and transfer accuracy as measured by maximum mean discrepancy.",pol,none,0.0,0,0
2949,"The proposed model seems to improve on the transfer accuracy, with a slight hit to reconstruction accuracy.",pol,none,0.0,3,0
2950,Qualitative investigation demonstrates that the learned representation can approximate several coarse spectral descriptors of the target domains.,pol,none,0.0,0,0
2951,High-level comments,pol,none,0.0,0,0
2952,-------------------,pol,none,0.0,0,0
2953,"Overall, this paper is well written, and the various design choices seem well-motivated.",pol,pol_positive,3.0,3,3
2954,"The empirical comparisons to UNIT are reasonably thorough, though I would have preferred more in-depth evaluation of the MoVE model as well.",pol,pol_negative,1.0,3,1
2955,"Specifically, the authors introduced an extra input (control) to encode the pitch class and octave information during encoding.",pol,none,0.0,0,0
2956,"I infer that this was necessary to achieve good performance, but it would be instructive to see the results without this additional input, since it does in a sense constitute a form of supervision, and therefore limits the types of training data which can be used.",pol,pol_positive,3.0,1,3
2957,"While I understand that quantifying performance in this application is difficult, I do find the results difficult to interpret.",pol,pol_negative,1.0,1,1
2958,Some of this comes down to incomplete definition of the metrics (see detailed comments below).,pol,none,0.0,1,0
2959,"However, the more pressing issue is that evaluation is done either sample-wise within-domain (reconstruction), or distribution-wise across domains (transfer).",pol,pol_negative,1.0,1,1
2960,"The transfer metrics (MMD and kNN) are opaque to the reader: for instance, in table 1, is a knn score of 43173 qualitatively different than 43180?",pol,pol_negative,1.0,1,1
2961,What is the criteria for bolding here?,pol,pol_neutral,2.0,2,2
2962,"It would be helpful if these scores could be calibrated in some way, e.g., with reference to",pol,pol_neutral,2.0,2,2
2963,MMD/KNN scores of random partitions of the target domain samples.,pol,none,0.0,0,0
2964,"Since the authors do additional information here for each sample (notes), it would be possible to pair generated and real examples by instrument and note, rather than (in addition to) unsupervised, feature-space pairing by MMD.",pol,pol_positive,3.0,1,3
2965,"This could provide a slightly stronger version of the comparison in Figure 3, which shows that the overall distribution of spectral centroids is approximated by transfer, but does not demonstrate per-sample correspondence.",pol,pol_positive,3.0,1,3
2966,Detailed comments,pol,none,0.0,0,0
2967,-----------------,pol,none,0.0,0,0
2968,"At several points in the manuscript, the authors refer to ""invertible"" representations (e.g., page 4, just after eq. 1), but it seems like what they mean is approximately invertible or decodable.",pol,pol_neutral,2.0,1,2
2969,It would be better if the authors were a little more careful in their use of terminology here.,pol,pol_negative,1.0,1,1
2970,"In the definition of the RBF kernel (page 4), why is there a summation?",pol,pol_neutral,2.0,1,2
2971,What does this index? How are the kernel bandwidths defined?,pol,pol_neutral,2.0,2,2
2972,"How exactly are reconstruction errors calculated: using the NSGT magnitude representation, or after resynthesis in the time domain?",pol,pol_neutral,2.0,2,2
2973,PROS:,pol,none,0.0,0,0
2974,"- The text is very well written, with a good balance between mathematical details and intuitions.",pol,pol_positive,3.0,3,3
2975,- I really like the high-level description of the algorithms and proof techniques,pol,pol_positive,3.0,3,3
2976,CONS:,pol,none,0.0,0,0
2977,"to be completely honest, I am not sure I have learnt anything new from the paper.",pol,pol_negative,1.0,1,1
2978,1) the proof techniques are very standard,pol,pol_negative,1.0,3,1
2979,"2) although there must be some small innovations, I thought that all the results had more or less been proven by Dupuis and co-authors:",pol,pol_negative,1.0,0,1
2980,a. large deviation principles,pol,none,0.0,0,0
2981,"b. the larger the swapping rate, the better (which motivated Dupuis & al to consider the infinite swapping limit.)",pol,none,0.0,0,0
2982,and,pol,none,0.0,0,0
2983,c. Bakri & al methodology to prove convergence relying on the carre du champ is by now very standard and the proofs of the paper are only minor adaptations.,pol,none,0.0,1,0
2984,"I must probably be missing something, and I encourage the authors to clarify what the main novelties are when compared to the several papers by Dupuis & al.",pol,pol_negative,1.0,2,1
2985,REMARKS:,pol,none,0.0,0,0
2986,1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions.,pol,pol_negative,1.0,1,1
2987,"For the task of predicting interaction contact among atoms of protein complex consisting of two interacting proteins, the authors propose to train a Siamese convolutional neural network, noted as SASNet, and to use the contact map of two binding proteins’ native structure.",pol,none,0.0,0,0
2988,The authors claim that the proposed method outperforms methods that use hand crafted features; also the authors claim that the proposed method has better transferability.,pol,none,0.0,0,0
2989,"My overall concern is that the experiment result doesn’t really fully support the claim in the two aspects: 1) the SASNet takes the enriched dataset as input to the neural net but it also uses the complex (validation set) to train the optimal parameters, so strictly it doesn’t really fit in the “transfer” learning scenario.",pol,pol_negative,1.0,1,1
2990,"Also, the compared methods don’t really use the validation set from the complex data for training at all.",pol,pol_negative,1.0,1,1
2991,Thus the experiment comparison is not really fair.,pol,pol_negative,1.0,1,1
2992,"2) The experiment results include standard errors for different replicates where such replicates correspond to different training random seeds (or different samples from the enriched set?), however, it doesn’t include any significance of the sampling.",pol,pol_negative,1.0,1,1
2993,"Specifically, the testing dataset is fixed.",pol,pol_neutral,2.0,1,2
2994,"A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.",pol,pol_negative,1.0,0,1
2995,"Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.",pol,pol_negative,1.0,1,1
2996,"Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can.",pol,pol_negative,1.0,1,1
2997,"Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.",pol,pol_negative,1.0,1,1
2998,"Results and discussion about how the previous methods with full features perform compared to SASNet, and also how we can include those features into SASNet should complete the paper.",pol,pol_negative,1.0,1,1
2999,"Overall the paper is well written, and I do think the paper could be much stronger the issues above are addressed.",pol,pol_positive,3.0,3,3
3000,Some minor issues:,pol,none,0.0,0,0
3001,"1)	on page 4, Section 3, the first paragraph, shouldn’t “C_p^{val} of 55” be “C_p^{test} of 55”?",pol,pol_negative,1.0,2,1
3002,2)	It is not clear what the “replicates” refer to in the experiments.,pol,pol_negative,1.0,1,1
3003,3)	Some discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?,pol,pol_negative,1.0,2,1
3004,Problem and contribution:,pol,none,0.0,0,0
3005,The paper studies if the Visual Question answering model “FILM” from Perez et al (2018) is able to decide if “most” of the objects have a certain attribute or color.,pol,none,0.0,0,0
3006,For this it tries to mimic the setup used to test human abilities in the study by Pietroski et al. (2009).,pol,none,0.0,0,0
3007,The main contribution of this is work is a discussion of how a model could solve the problem of deciding “most” and the study which shows that the studied model has some ability to do this.,pol,none,0.0,0,0
3008,From this the paper concludes that the model is likely to have some approximate number system.,pol,none,0.0,0,0
3009,Strengths:,pol,none,0.0,0,0
3010,"1.	The paper looks at a new angle to study and characterize CNN models in general, and VQA models in particular by looking into the psycholinguistic literature experimental setup studied with human subjects.",pol,pol_positive,3.0,3,3
3011,"2.	The paper studies different variants of controlling for different factors (e.g. pairing data points, area used, different training data and pre-trained vs. trained from scratch CNN models)",pol,pol_positive,3.0,0,3
3012,3.	It is interesting to see that the models performance reasonably aligns with the curve predicted by “Weber’s law”.,pol,pol_positive,3.0,3,3
3013,Weaknesses:,pol,none,0.0,0,0
3014,"4.	Number of objects vs. ratios is not disentangled: While the paper clarifies that not only a smaller number of objects are used, it would be interesting to understand if similar conclusions hold if only the same number or about the same number of total objects are used but the ratios change (at least for more extreme ratios, 1:2, this seems to be the case as they achieve 100% accuracy).",pol,pol_neutral,2.0,2,2
3015,5.,pol,none,0.0,0,0
3016,"The paper only focusses on a single VQA model (FILM) which limits the understanding if this observation is specific to this model; what about other models such as the one from Hudson & Manning (2018), or Relation Networks (Santoro et al) or even simpler baselines: A system which two attention mechanisms (without normalizations) which are sum pooled and then compared would sort of explicitly encode the idea of the APN system.",pol,none,0.0,1,0
3017,It would be valuable to compare them to see how different systems (can) solve this task.,pol,pol_neutral,2.0,2,2
3018,I would expect that the architecture favors certain capabilities; e.g. Relation Networks might lead more to a paring-based strategy. Or Zhang et al. (2018) might be able to exploit explicit counting to solve the task.,pol,none,0.0,1,0
3019,6.	The “most” ability or APN ability seems to be highly related to accumulation in neural networks.,pol,none,0.0,0,0
3020,The paper FiLM uses global max-pooling and I am wondering if this affect this ability.,pol,pol_neutral,2.0,1,2
3021,7.	The study is only performed on symbols which a very large training set (given the difficulty of the problem) and it not clear how well this generalizes to real images or scenarios with less training data.,pol,pol_negative,1.0,1,1
3022,7.1.,pol,none,0.0,0,0
3023,"Maybe beyond the scope of this work, but it would be interesting to understand how much training data different models need to obtain this capability.",pol,pol_neutral,2.0,2,2
3024,"8.	For evaluation: Are there distractors, i.e. elements which don’t belong to set A or B? If not, how would distractors affect it.",pol,pol_neutral,2.0,2,2
3025,9.	Clarity:,pol,none,0.0,0,0
3026,9.1.,pol,pol_neutral,2.0,0,2
3027,The equation between equation (1) and (2) misses a number [I will call it 1.5 for now],pol,none,0.0,1,0
3028,9.2.	In formula (1.5) “<=>” seems to be used at different levels (?) it would be good to use brackets to make clear which level “<=>” refers to.,pol,pol_negative,1.0,1,1
3029,Minor:,pol,none,0.0,0,0
3030,10.	The title suggests that the paper studies multiple VQA models but only a single model is studied.,pol,pol_negative,1.0,0,1
3031,Conclusion:,pol,none,0.0,0,0
3032,"The paper looks into an interesting direction to study CNN models but has some limitations including studying only a single VQA model type, limited to artificially generated images.",pol,pol_negative,1.0,0,1
3033,This paper proposes an additional loss term to use when training an LSTM LM.,pol,none,0.0,0,0
3034,"The authors argue that, intuitively, we want the output distribution to retain some information about the context, or ""past"".",pol,none,0.0,0,0
3035,"Given this, they use the output distribution as input to a one layer network that must predict the current token.",pol,none,0.0,0,0
3036,The loss for this network is incorporated as an additional term used when training the LM.,pol,none,0.0,0,0
3037,The authors show that by adding this loss term they can achieve SOTA (for single softmax model) perplexity on a number of LM benchmarks.,pol,none,0.0,0,0
3038,The technical contribution is proposing a new loss term to use when training a language model.,pol,none,0.0,0,0
3039,"The idea is clear, simple, and well explained, and it seems to be effective in practice.",pol,pol_positive,3.0,3,3
3040,One drawback is that it is highly specific to language models.,pol,pol_negative,1.0,1,1
3041,"Other recent works which have demonstrated effective regularization of LSTM LMs have proposed methods that can be used in any LSTM model, but that is not the case here.",pol,pol_negative,1.0,0,1
3042,"In addition, there is not much theoretical justification for it, it seems like a one-off trick.",pol,pol_negative,1.0,1,1
3043,"The loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?",pol,pol_negative,1.0,1,1
3044,"Although it is specific to language models, there are a few reasons it might be of broader significance:",pol,pol_positive,3.0,0,3
3045,- It falls in the recent line of work in incorporating auxiliary losses for various tasks.,pol,none,0.0,0,0
3046,This idea has touched many problems and seen success in practice.,pol,pol_positive,3.0,3,3
3047,- Perhaps it can be applied to other sequence models.,pol,pol_positive,3.0,2,3
3048,"For example in encoder-decoder models, the decoder can be thought of as a conditional LM.",pol,none,0.0,0,0
3049,Experiments are comprehensive and rigorous.,pol,pol_positive,3.0,3,3
3050,They might be more convincing if there were results on a very large corpus such as 1 billion word corpus.,pol,pol_negative,1.0,1,1
3051,Pros:,pol,none,0.0,0,0
3052,- New SOTA for single softmax model on LM benchmarks.,pol,pol_positive,3.0,3,3
3053,"- Simple, clearly explained idea.",pol,pol_positive,3.0,3,3
3054,- Demonstrates effectiveness of auxiliary losses.,pol,pol_positive,3.0,3,3
3055,- Rigorous experiments.,pol,pol_positive,3.0,3,3
3056,Cons,pol,none,0.0,0,0
3057,- Trick is specific to LM.,pol,pol_negative,1.0,1,1
3058,- No large corpus results.,pol,pol_negative,1.0,1,1
3059,"The main idea behind the paper is to use random projections as the initial word representations, rather than the vocab-size 1-hot representations, as is usually done in language modeling.",pol,none,0.0,0,0
3060,"The benefit is that the matrix which projects words into embedding space can then be much smaller, since the space of random projections can be much smaller than the vocab size.",pol,none,0.0,0,0
3061,"The idea is an interesting one, but",pol,pol_negative,1.0,3,1
3062,this work is at too much of a preliminary stage for a top-tier conference such as ICLR. In its present state it would make for a potentially interesting paper at a targeted workshop.,pol,none,0.0,1,0
3063,More specific comments,pol,none,0.0,0,0
3064,--,pol,none,0.0,0,0
3065,"The initial description of the language modeling problem assumes a particular decomposition of the joint probability, according to a particular application of the chain rule, but of course this is a modeling choice and not the only option (albeit the standard one).",pol,none,0.0,0,0
3066,The main problem with the paper is the use of simple baseline setups as the only experimental configuration:,pol,none,0.0,1,0
3067,o feedforward rather than recurrent network;,pol,pol_negative,1.0,0,1
3068,o use of the Penn Treebank dataset only;,pol,pol_negative,1.0,1,1
3069,o use of a small n for the n-grams.,pol,pol_negative,1.0,0,1
3070,All or at least some of these decisions would need to be relaxed to make a convincing paper.,pol,pol_negative,1.0,1,1
3071,The reasons for the use of the energy-based formulation are not clear to me.,pol,pol_negative,1.0,1,1
3072,"Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?",pol,pol_negative,1.0,2,1
3073,Just before equation 6 it says that the resulting vector representation is the *sum* of all the non-zero entries.,pol,none,0.0,0,0
3074,But there are some minus ones in the random projection?,pol,pol_negative,1.0,2,1
3075,The PPL expression at the bottom of p.5 doesn't look right.,pol,pol_negative,1.0,1,1
3076,"The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.",pol,pol_negative,1.0,1,1
3077,It looks like all the results are given on the test set. Did you not do any tuning on the validation data?,pol,pol_negative,1.0,2,1
3078,The plots in figure 4 are too small.,pol,pol_negative,1.0,1,1
3079,"It would be useful to have a table, like the one on the last page, which clearly shows the baseline vs. the random-projection model, with some description of the results in the main body of the text.",pol,pol_negative,1.0,2,1
3080,"The overall presentation could be better, and I would encourage the authors to tidy the paper up in any subsequent submission.",pol,pol_negative,1.0,2,1
3081,"For example, there are lots of typos such as ""instead of trying to probability of a target word"".",pol,pol_negative,1.0,1,1
3082,This work proposes a hybrid VAE-based model (combined with an adversarial or maximum mean discrepancy (MMD) based loss) to perform timbre transfer on recordings of musical instruments.,pol,none,0.0,0,0
3083,"Contrary to previous work, a single (conditioned) decoder is used for all instrument domains, which means a single model can be used to convert any source domain to any target domain.",pol,none,0.0,0,0
3084,"Unfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts.",pol,pol_negative,1.0,1,1
3085,"The instruments are often unrecognisable, although with knowledge of the target domain, some of its characteristics can be identified.",pol,pol_negative,1.0,1,1
3086,"The many-to-many results are clearly better than the pairwise results in this regard, but in the context of musical timbre transfer, I don't feel that this model successfully achieves its goal -- the results of Mor et al. (2018), although not perfect either, were better in this regard.",pol,pol_negative,1.0,1,1
3087,I have several further concerns about this work:,pol,none,0.0,0,0
3088,"* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present.",pol,pol_negative,1.0,1,1
3089,I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.,pol,pol_negative,1.0,0,1
3090,"* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?",pol,pol_negative,1.0,1,1
3091,* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE?,pol,pol_negative,1.0,1,1
3092,"This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.",pol,pol_negative,1.0,1,1
3093,"I appreciated that the one-to-one transfer experiments are incremental comparisons, which provides valuable information about how much each idea contributes to the final performance.",pol,pol_positive,3.0,3,3
3094,"Overall, I feel that this paper falls short of what it promises, so I cannot recommend acceptance at this time.",pol,pol_negative,1.0,1,1
3095,Other comments:,pol,none,0.0,0,0
3096,"* In the introduction, an adversarial criterion is referred to as a ""discriminative objective"", but ""adversarial"" (i.e. featuring a discriminator) and ""discriminative"" mean different things. I don't think it is correct to refer to an adversarial criterion as discriminative.",pol,none,0.0,1,0
3097,"* Also in the introduction, it is implied that style transfer constitutes an advance in generative models, but style transfer does not make use of / does not equate to any generative model.",pol,pol_negative,1.0,1,1
3098,"* Some turns of phrase like ""recently gained a flourishing interest"", ""there is still a wide gap in quality of results"", ""which implies a variety of underlying factors"", ... are vague / do not make much sense and should probably be reformulated to enhance readability.",pol,pol_negative,1.0,1,1
3099,"* Introduction, top of page 2: should read ""does not learn"" instead of ""do not learns"".",pol,pol_negative,1.0,1,1
3100,"* Mor et al. (2018) do actually make use of an adversarial training criterion (referred to as a ""domain confusion loss""), contrary to what is claimed in the introduction.",pol,pol_negative,1.0,1,1
3101,* The claim that training a separate decoder for each domain necessarily leads to prohibitive training times is dubious -- a single conditional decoder would arguably need more capacity than each individual separate decoder model.,pol,pol_negative,1.0,1,1
3102,I think all claims about running time should be corroborated by controlled experiments.,pol,pol_negative,1.0,1,1
3103,* I think Figure 1 is great and helps a lot to distinguish the different domain translation paradigms.,pol,pol_neutral,2.0,3,2
3104,"* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. ""matching samples"").",pol,pol_negative,1.0,1,1
3105,"* Section 3.1, ""amounts to optimizing"" instead of ""amounts to optimize""",pol,pol_negative,1.0,1,1
3106,* Higgins et al. (2016) specifically discuss the case where beta in formula (1) is larger than one.,pol,none,0.0,0,0
3107,"As far as I can tell, beta is annealed from 0 to 1 here, which is an idea that goes back to ""Generating Sentences from a Continuous Space"" by Bowman et al. (2016).",pol,none,0.0,0,0
3108,This should probably be cited instead.,pol,pol_negative,1.0,1,1
3109,"* ""circle-consistency"" should read ""cycle-consistency"" everywhere.",pol,pol_negative,1.0,2,1
3110,* MMD losses in the context of GANs have also been studied in the following papers:,pol,none,0.0,0,0
3111,"- ""Training generative neural networks via Maximum Mean Discrepancy optimization"", Dziugaite et al. (2015)",pol,none,0.0,0,0
3112,"- ""Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy"", Sutherland et al. (2016)",pol,none,0.0,0,0
3113,"- ""MMD GAN: Towards Deeper Understanding of Moment Matching Network"", Li et al. (2017)",pol,none,0.0,0,0
3114,"* The model name ""FILM-poi"" is only used in the ""implementation details"" section, it doesn't seem to be referred to anywhere else. Is this a typo?",pol,pol_negative,1.0,2,1
3115,* The differences between UNIT (GAN; C-po) and UNIT (MMD; C-po) in Table 1 seem very small and I'm not convinced that they are significant. Why does the MMD version constitute an improvement? Or is it simply more stable to train?,pol,pol_negative,1.0,1,1
3116,"* The descriptor distributions in Figure 3 don't look like an ""almost exact match"" to me (as claimed in the text).",pol,pol_negative,1.0,1,1
3117,There are some clearly visible differences.,pol,pol_negative,1.0,0,1
3118,I think the wording is a bit too strong here.,pol,pol_negative,1.0,1,1
3119,I believe that the authors have a solid contribution that can be interesting for the ICLR community.,pol,pol_positive,3.0,3,3
3120,"Therefore, I recommend to accept the paper but after revision because the presentation and explanation of the ideas contain multiple typos and lacking some details (see bellow).",pol,pol_negative,1.0,1,1
3121,Summary:,pol,none,0.0,0,0
3122,The authors propose a new method called BA-Net to solve the SfM problem by explicitly incorporating geometry priors into a machine learning task.,pol,none,0.0,0,0
3123,The authors focus on the Bundle Adjustment process.,pol,none,0.0,0,0
3124,"Given several successive frames of a video sequence (2 frames but can be extended up to 5), BA-Net jointly estimates the depth of the first frame and the relative camera motion (between the first frame and the next one).",pol,none,0.0,0,0
3125,The method is based on a convolutional neural network which extracts the features of the different pyramid levels of the two images and in parallel computes the depth map of the first frame.,pol,none,0.0,0,0
3126,"The proposed network is based on the DRN-54 (Yu et al., 2017) as a feature extractor.",pol,none,0.0,0,0
3127,This is complemented by the linear combination of depth bases obtained from the first image.,pol,none,0.0,0,0
3128,The features and the initial depth then passed to the optimization layer called BA-layer where the feature re-projection error is minimized by the modified LM algorithm.,pol,none,0.0,0,0
3129,The authors adapt the standard multi-view geometry constraints by a new concept of feature re-projection error in the BA framework (BA-layer) which they made differentiable.,pol,none,0.0,0,0
3130,Differentiable optimization of camera motion and image depth via LM algorithm is now possible and can be used in various other DL architectures (ex. MVS-Net can probably benefit from BA-layer).,pol,none,0.0,0,0
3131,"The authors also propose a novel depth parametrization in the form of linear combination of depth bases which reduces the number of parameters for the learning task,",pol,none,0.0,0,0
3132,enables integration into the same backbone net as used or feature pyramids and makes it possible to jointly train the depth generator and the BA-layer.,pol,none,0.0,0,0
3133,Originally the proposed approach depicts the network operating in the two-view settings.,pol,none,0.0,0,0
3134,"The extensibility to more views is also possible and, as shown by authors, proved to improve performance.",pol,none,0.0,3,0
3135,"It is, however, limited by the GPU capacity.",pol,none,0.0,1,0
3136,"Overall, the authors came up with an interesting approach to the standard BA problem.",pol,none,0.0,3,0
3137,They have managed to inject the multi-view geometry priors and BA into the DL architecture.,pol,none,0.0,0,0
3138,Major comments regarding the paper:,pol,none,0.0,0,0
3139,It would be interesting to know the evaluation times for the BA-net and more importantly to have some implementation details to ensure reproducibility.,pol,pol_negative,1.0,2,1
3140,Minor comments regarding the paper:,pol,none,0.0,0,0
3141,-	The spacing between sections is not consistent.,pol,pol_negative,1.0,1,1
3142,-	Figures 1 is way too abstract given the complicated set-up of the proposed architecture.,pol,pol_negative,1.0,1,1
3143,It would be nice to see more details on the subnet for depth estimator and output of the net.,pol,pol_negative,1.0,2,1
3144,Overall it would be helpful for reproducibility if authors can visualize all the layers of all the different parts of the network as it is commonly done in the DL papers.,pol,pol_negative,1.0,2,1
3145,-,pol,pol_negative,1.0,0,1
3146,Talking about proposed formulation of BA use either of the following and be consistent across the paper:,pol,none,0.0,0,0
3147,Featuremetric BA / Feature-metric BA / Featuremetric BA / ‘Feature-metric BA’,pol,pol_negative,1.0,0,1
3148,-	Talking about depth parametrization use ‘basis’ or ‘bases’ not both and clearly defined the meaning of this important notion.,pol,pol_negative,1.0,1,1
3149,-	Attention should be given to the notation in formulas (3) and (4).,pol,pol_negative,1.0,1,1
3150,The projection function there is no longer accepts a 3D point parametrized by 3 variables.,pol,pol_negative,1.0,1,1
3151,Instead only depth is provided.,pol,pol_negative,1.0,0,1
3152,"In addition, the subindex ‘1’ of the point ‘q’ is not explained.",pol,pol_negative,1.0,1,1
3153,-	More attention should be given to the evaluation section.,pol,pol_negative,1.0,2,1
3154,Specifically to the tables (1 and 2) with quantitative results showing the comparison to other methods.,pol,pol_neutral,2.0,0,2
3155,It is not clear how the depth error is measured and it would be nicer to have the other errors explained exactly as they referred in the tables (e.g. ATE?).,pol,pol_negative,1.0,1,1
3156,-	How the first camera pose is initialized?,pol,pol_negative,1.0,2,1
3157,-	In Figure 2.b I’m surprised by the difference obtained in the feature maps for images which seems very similar (only the lighting seems to be different). Is it three consecutive frames?,pol,pol_negative,1.0,2,1
3158,"-	Attention should be given to the grammar, formatting in particular the bibliography.",pol,pol_negative,1.0,2,1
3159,Summary.,pol,none,0.0,0,0
3160,The authors empirically investigate the influence of the architecture and the capacity of an NN-model on the transferability of adversarial examples.,pol,none,0.0,0,0
3161,They also study the influence of the smoothness.,pol,none,0.0,0,0
3162,"From the obtained results, they propose the smoothed gradient attack showing improvements on the transferability of adversarial examples.",pol,none,0.0,0,0
3163,Pros.,pol,none,0.0,0,0
3164,* Robustness of neural nets is a challenging problem of interest for ICLR,pol,pol_positive,3.0,0,3
3165,* The paper is well written,pol,pol_positive,3.0,3,3
3166,* The experimental study is convincing,pol,pol_positive,3.0,3,3
3167,* The experimental results for the smoothed gradient attacks are promising,pol,pol_positive,3.0,3,3
3168,Cons.,pol,none,0.0,0,0
3169,* The results of the experimental study are somehow expected,pol,pol_neutral,2.0,0,2
3170,* the idea of smoothing gradients is not new,pol,pol_negative,1.0,1,1
3171,Evaluation.,pol,none,0.0,0,0
3172,The experimental study of the transferability of adversarial examples is well designed.,pol,pol_positive,3.0,3,3
3173,Experimental protocol is convincing.,pol,pol_positive,3.0,3,3
3174,The smoothed gradient attacks improve many previously proposed attacks.,pol,pol_positive,3.0,0,3
3175,"Therefore, my opinion is rather positive. But, as a non expert in the field, I am not completely convinced by the novelty of the approach.",pol,pol_negative,1.0,1,1
3176,Some details.,pol,none,0.0,0,0
3177,"Typos: That l8 abstract; systems l9 intro; and l2 related work; directly evaluation l2 Section4, must has l-10 p4;",pol,pol_neutral,2.0,0,2
3178,* the choice \sigma = 15 in Section 6.2 should be justified by the following study,pol,pol_negative,1.0,1,1
3179,* \sigma is not given in Figure 3(a),pol,pol_negative,1.0,1,1
3180,This paper introduces a new generative poisoning attack method against machine learning classifiers.,pol,none,0.0,0,0
3181,The authors propose pGAN with three components to maximum the error of classification and guarantee undistinguished poisoning data for the discriminator.,pol,none,0.0,0,0
3182,The experimental results show that the hyperparameter \alpha significantly affects the poisoning data distribution and pGAN leads to specific error in a classification task.,pol,none,0.0,0,0
3183,"This paper should be weekly accepted, considering the following aspects.",pol,none,0.0,1,0
3184,Positive points: (1) The experiments seem solid.,pol,pol_positive,3.0,3,3
3185,The overall performance with different parameters and the corresponding error type have been evaluated.,pol,pol_neutral,2.0,0,2
3186,(2) The error-specific and performance-control characteristics of pGAN seem to be interesting.,pol,pol_positive,3.0,3,3
3187,(3) The paper is well organized.,pol,pol_positive,3.0,3,3
3188,Negative points: (1) The authors should provide more justification on equation-3.,pol,pol_negative,1.0,1,1
3189,Why do the authors directly average different loss for the discriminator and the classifer?,pol,pol_negative,1.0,1,1
3190,"(2) The function of the discriminator is not very clear, especially for the classification error test.",pol,pol_negative,1.0,1,1
3191,Does the discriminator exclude the poisoning data according to certain rule?,pol,pol_negative,1.0,2,1
3192,It would make more sense if the classification error measured from the data the discriminator selects.,pol,pol_negative,1.0,1,1
3193,(3) pGAN can produce error-specific attack without sufficient justifications.,pol,none,0.0,0,0
3194,Why can pGAN lead to the inclination? Is it possible for pGAN to control the specific error tendency?,pol,pol_negative,1.0,2,1
3195,"(4) For the error-specific attack task, it would be better to provide an ablation experiment.",pol,pol_negative,1.0,1,1
3196,"For example, authors could implement pGAN by ignoring the detectability of the discriminator (i.e. \alpha=0) or typical pGAN when they compare with the label-flip operation.",pol,pol_negative,1.0,1,1
3197,Please explain which component contribute to the error-specific inclination.,pol,none,0.0,0,0
3198,The paper presents proof that the DeepSets and a variant of PointNet are universal approximators for permutation equivariant functions.,pol,none,0.0,0,0
3199,The proof uses an expression for equivariant polynomials and the universality of MLP.,pol,none,0.0,0,0
3200,"It then shows that the proposed expression in terms of power-sum polynomials can be constructed in PointNet using a minimal modification to the architecture, or using DeepSets, therefore proving the universality of such deep models.",pol,none,0.0,0,0
3201,The results of this paper are important.,pol,pol_positive,3.0,3,3
3202,"In terms of presentation, the notation and statement of theorems are precise, however, the presentation is rather dry, and I think the paper can be significantly more accessible.",pol,pol_negative,1.0,1,1
3203,"For example, here is an alternative and clearer route presenting the same result: one may study the simple case of having single input channel, for which the output at index ""i"" of an equivariant polynomial is written as the sum of all powers of input multiplied by a polynomial function of the corresponding power-sum.",pol,none,0.0,0,0
3204,"This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible.",pol,pol_negative,1.0,0,1
3205,Generalizing this to the multi-channel input as the next step could make the proof more accessible.,pol,pol_negative,1.0,1,1
3206,The second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model.,pol,pol_negative,1.0,1,1
3207,"Given the fact that it applies the same MLP independently to individual set members, it is obvious that it is not universal equivariant (for example, consider a function that performs a fixed permutation to its input), and I fail to see why the paper goes into the trouble of having theorems and experiments just to demonstrate this point. If there were any other objectives beyond this in the experiments could you please clarify?",pol,pol_negative,1.0,1,1
3208,"Finally, could you give a more accurate citation (chapter-page number) for the single-channel version of Theorem 2.?",pol,pol_negative,1.0,2,1
3209,The paper focuses on the stability prediction task on the ShapeStacks dataset.,pol,none,0.0,0,0
3210,"Specifically, the paper creates a new extension to the dataset, and it proposes the use of ""Neural Stethoscopes"" framework to analyze deep neural nets' physical reasoning of local stability v.s. global stability.",pol,none,0.0,0,0
3211,It is shown in the paper neural nets tend to be misled by local stability when the task is to predict global stability.,pol,none,0.0,0,0
3212,Then the paper utilizes the proposed framework to de-bias the misleading correlation to achieve a state-of-the-art on the dataset.,pol,none,0.0,0,0
3213,The paper is very well-written and easy to follow.,pol,pol_positive,3.0,3,3
3214,The main idea is simple and the experiments are detailed.,pol,pol_positive,3.0,3,3
3215,"Specifically on the task of stability prediction, it is quite interesting to know that neural nets can be misled by visual cues (local stability).",pol,pol_positive,3.0,0,3
3216,"However, my concern is that the paper focuses only on a very specific application domain,  and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all. In the mean time, the notion of ""Neural Stethoscopes"" could be much more  generally applied. Without applications in other domains, it is not immediately clear what the paper's implication is.",pol,pol_negative,1.0,1,1
3217,Summary:,pol,none,0.0,0,0
3218,The authors look at the problem of exploration in deep RL.,pol,none,0.0,0,0
3219,They propose a “curiosity grid” which is a virtual grid laid out on top of the current level/area that an Atari agent is in.,pol,none,0.0,0,0
3220,"Once an agent enters a new cell of the grid, it obtains a small reward, encouraging the agent to explore all parts of the game.",pol,none,0.0,0,0
3221,The grid is reset (meaning new rewards can be obtained) after every roll out (meaning the Atari agent has used up all its lives and the game restarts).,pol,none,0.0,0,0
3222,The authors argue that this method enables better exploration and they obtain an impressive score on Montezuma’s Revenge (MR).,pol,none,0.0,0,0
3223,Review:,pol,none,0.0,0,0
3224,"The paper contains an extensive introduction with many references to prior work, and a sensible lead up to the introduced algorithm.",pol,pol_positive,3.0,3,3
3225,The algorithm itself seems to work well and some of the results are convincing.,pol,pol_positive,3.0,3,3
3226,I am a bit worried about the fact that the agents have access to their history of locations (“the grid”).,pol,pol_negative,1.0,1,1
3227,The authors mention that none of the methods they compare against has this advantage and it seems that in a game that rewards exploration directly (MR) this is a large advantage.,pol,none,0.0,0,0
3228,The authors comment on this advantage in section 3 and found that removing intrinsic rewards hurt performance significantly.,pol,none,0.0,0,0
3229,Only removing the grid access made results on MR very unstable.,pol,none,0.0,1,0
3230,"However in order to compute the intrinsic rewards, it still seems necessary to access the location of the agent, meaning that implicitly the advantage of the method is still there.",pol,none,0.0,1,0
3231,"I was wondering if the authors find that the agents are forcibly exploring the entire environment during each rollout? Even if the agent knows what/where the actual goal is. There is a hint to this behaviour in section 4, on exploration in sparse domains.",pol,pol_neutral,2.0,1,2
3232,"The future work section mentions some interesting improvements, where the agent position is learned from data. That seems like a promising direction that would generalise beyond Atari games and avoids the advantage.",pol,pol_positive,3.0,3,3
3233,Nits/writing feedback:,pol,none,0.0,0,0
3234,- There is no need for such repetitive citing (esp paragraph 2 on page 2).,pol,pol_negative,1.0,0,1
3235,Sometimes the same paper is cited 4 times within a few lines.,pol,pol_negative,1.0,0,1
3236,"While it’s great that so much prior work was acknowledged, mentioning a paper once per paragraph is (usually) sufficient and increases readability.",pol,pol_negative,1.0,0,1
3237,- I think the comparison between prior lifetimes and humans mastering a language doesn’t hold up and is distracting,pol,pol_negative,1.0,1,1
3238,##,pol,none,0.0,0,0
3239,##,pol,none,0.0,0,0
3240,Revision:,pol,none,0.0,0,0
3241,The rebuttal does little to clarify open questions:,pol,pol_negative,1.0,1,1
3242,1. Both reviewer 2 and I commented on the ablation study regarding the grid but received no reply.,pol,pol_negative,1.0,1,1
3243,"2. I am not convinced this method is sufficiently new, given that there are other methods that try to directly reward visiting new states.",pol,pol_negative,1.0,1,1
3244,"3. The authors argue in their rebuttal that ""the grid"" is a novel idea that warrants investigation, but remark in figure 5 that likely it isn't the key aspect of their algorithm.",pol,pol_negative,1.0,1,1
3245,This seems contradictory.,pol,pol_negative,1.0,1,1
